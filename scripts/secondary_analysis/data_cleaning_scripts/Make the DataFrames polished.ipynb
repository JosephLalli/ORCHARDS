{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import json\n",
    "import ete3\n",
    "import subprocess\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "import pysam\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "\n",
    "sys.path.append('/mnt/d/orchards/H1N1/supporting_code')\n",
    "from vcfClass import VCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings to load pre-calculated data to skip the parts of this code that take a long time\n",
    "usePreCalcClades = True\n",
    "alreadyFilteredSNPs = True\n",
    "useSNPGenie2 = True\n",
    "recalc_bottlenecks=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatafile = '/mnt/d/orchards/h1n1/metadata.csv'\n",
    "completemetadatakey= '/mnt/d/orchards/h1n1/completemetadata_key.csv'\n",
    "expandedMetadata = '/mnt/d/orchards/h1n1/completemetadata.csv'\n",
    "figures = '/mnt/d/orchards/h1n1/figures'\n",
    "\n",
    "\n",
    "SNP_frequency_cutoff = 0.01\n",
    "#for a global mutation to be considered 'variant', it must hit this value at least one month.\n",
    "global_freq_cutoff=0.05\n",
    "\n",
    "\n",
    "commonGlobalfreqsFilelocation = '/mnt/d/orchards'\n",
    "GlobalH3N2AAfreqsFile = commonGlobalfreqsFilelocation + '/H3N2_AA.json'\n",
    "GlobalH1N1AAfreqsFile = commonGlobalfreqsFilelocation + '/H1N1_AA.json'\n",
    "GlobalFluBAAfreqsFile = commonGlobalfreqsFilelocation + '/FluB_AA.json'\n",
    "GlobalH3N2DNAfreqsFile = commonGlobalfreqsFilelocation + '/H3N2_DNA.json'\n",
    "GlobalH1N1DNAfreqsFile = commonGlobalfreqsFilelocation + '/H1N1_DNA.json'\n",
    "GlobalFluBDNAfreqsFile = commonGlobalfreqsFilelocation + '/FluB_DNA.json'\n",
    "\n",
    "mainSampleFolders = ['/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016', \n",
    "                     '/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Hong_Kong_4801_2014_EPI834581', \n",
    "                     '/mnt/d/orchards/h1n1/orchards_run19/A_Michigan_45_2015_H1N1_18', \n",
    "                     '/mnt/d/orchards/h1n1/orchards_run19/A_Michigan_45_2015_H1N1_19',\n",
    "                     '/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_3073_2013_17',\n",
    "                     '/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_3073_2013_18']\n",
    "\n",
    "vcfdirs = [x+'/snp_calls/filtered_snpcalls' for x in mainSampleFolders]\n",
    "vcffiles = [x+'/all_snps_lenient_filter.vcf' for x in vcfdirs]\n",
    "allSNPsVCFfiles = [x+'/all_snps_with_depths.vcf' for x in vcfdirs]\n",
    "\n",
    "references = [x.split('/')[-1] for x in mainSampleFolders]\n",
    "\n",
    "consensusReferences = [mainSampleFolder + '/consensus/' + reference + '_consensus_noambig.fasta' for mainSampleFolder, reference in zip(mainSampleFolders, references)]\n",
    "distancefiles = [x + '/snp_calls/sequenceDistances.tsv' for x in mainSampleFolders]\n",
    "gtfFiles = ['/mnt/d/orchards/h1n1/' + reference + '_antigenic.gtf' for reference in references]\n",
    "# allnucVCFfiles = [x+'/rerun/all_snps.vcf' for x in mainSampleFolders]\n",
    "snpGenieFolderName = 'SNPGenie_bootstrapped_filter/SNPGenie_output'\n",
    "SnpGenieSegFolders = []\n",
    "for x in mainSampleFolders:\n",
    "    SnpGenieSegFolders.extend(glob.glob(x+'/'+snpGenieFolderName+'/*'))\n",
    "\n",
    "treebase = '/mnt/d/orchards/usacladework/augurWD/'\n",
    "treefiles = ['FluB/all/FluBYamaDNA_HA_all_refined.tree','/allH3N2/H3N2DNA_HA_refined.tree','/iqtreeTake2/H1N1DNA_HA_all_aligned.fasta.treefile']\n",
    "clade_references = '/mnt/d/orchards/recombination/clade_references.txt'\n",
    "\n",
    "hongkongContigs = {'NP':'A_Hong_Kong_4801_2014_834574_NP', 'NS':'A_Hong_Kong_4801_2014_834575_NS', \n",
    "                   'MP':'A_Hong_Kong_4801_2014_834576_MP', 'PA':'A_Hong_Kong_4801_2014_834577_PA',\n",
    "                   'PB2':'A_Hong_Kong_4801_2014_834578_PB2', 'PB1':'A_Hong_Kong_4801_2014_834579_PB1',\n",
    "                  'NA':'A_Hong_Kong_4801_2014_834580_NA','HA':'A_Hong_Kong_4801_2014_834581_HA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = VCF('/mnt/d/orchards/H1N1/ORCHARDS_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/SNP_Genie_061220_loose_Filter/A_Hong_Kong_4801_2014_834578_PB2/A_Hong_Kong_4801_2014_834578_PB2.vcf')\n",
    "# x = VCF('/mnt/d/orchards/H1N1/ORCHARDS_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf')\n",
    "x = VCF('/mnt/d/orchards/H1N1/ORCHARDS_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/snp_calls/filtered_snpcalls/18VR001031.vcf')\n",
    "x.add_bamfile_locations({'18VR001031': '/mnt/d/orchards/H1N1/ORCHARDS_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/map_to_consensus/18VR001031.bam'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = VCF('/mnt/d/orchards/H1N1/ORCHARDS_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/snp_calls/filtered_snpcalls/18VR001031.vcf')\n",
    "# x.add_bamfile_locations({'18VR001031': '/mnt/d/orchards/H1N1/ORCHARDS_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/map_to_consensus/18VR001031.bam'})\n",
    "# x.apply_position_filter()\n",
    "# df = x.to_dataframe()\n",
    "# df.loc[(df.sampleID=='18VR001031')&(df.FREQ > 0)&(df.chrom=='A_Hong_Kong_4801_2014_834578_PB2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/d/orchards/h1n1/A_Singapore_INFIMH-16-0019_2016_antigenic.gtf',\n",
       " '/mnt/d/orchards/h1n1/A_Hong_Kong_4801_2014_EPI834581_antigenic.gtf',\n",
       " '/mnt/d/orchards/h1n1/A_Michigan_45_2015_H1N1_18_antigenic.gtf',\n",
       " '/mnt/d/orchards/h1n1/A_Michigan_45_2015_H1N1_19_antigenic.gtf',\n",
       " '/mnt/d/orchards/h1n1/B_Phuket_3073_2013_17_antigenic.gtf',\n",
       " '/mnt/d/orchards/h1n1/B_Phuket_3073_2013_18_antigenic.gtf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consensusReferences\n",
    "gtfFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Constants\n",
    "potentialmixed = ['18VR001531', '19VR004455', '19VR003675', '19VR003920','19VR004452']\n",
    "#Samples which, based off visual inspection of mutations, seem to be mixed infections.\n",
    "#This is determined by looking at 10 samples with the most mutations and seeing if those mutations,\n",
    "#when close together, tend to be linked on the same read.\n",
    "\n",
    "subtypesToAnalyze = ['H1N1pdm', 'H3N2', 'Influenza B'] #options: 'H1N1pdm', 'H3N2', 'Influenza B', 'Mixed'\n",
    "geneOrder = [\"PB2\", \"PB1\", 'PB1-F2', \"NP\", \"HA\", \"NA\", \"PA\", 'PA-X', \"M1\", \"M2\", \"NS1\", \"NEP\"]\n",
    "naValues = ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A','N/A', '#NA', 'NULL', 'NaN', '-NaN', 'nan', '-nan','','*']\n",
    "read_tsv_args = {'sep':'\\t', 'keep_default_na':False, 'na_values':naValues}\n",
    "read_csv_args = {'keep_default_na':False, 'na_values':naValues}\n",
    "referenceDict = {\"A_Singapore_INFIMH-16-0019_2016\":\"H3N2\",\"A_Hong_Kong_4801_2014_EPI834581\":\"H3N2\", \"A_Michigan_45_2015_H1N1\":\"H1N1\",\"A_Michigan_45_2015_H1N1_18\":\"H1N1\",\"A_Michigan_45_2015_H1N1_19\":\"H1N1\", \"B_Brisbane_60_2008\":\"Influenza B\", \"B_Phuket_3073_2013_17\":\"Influenza B\",\"B_Phuket_3073_2013_18\":\"Influenza B\",\"Influenza A H3N2, Influenza B (Yamagata)\":\"Mixed\"}\n",
    "basefolder = '/mnt/d/orchards/h1n1/'\n",
    "sampleFolderDict = {'H3N2':{'17-18':basefolder+'/ORCHARDS_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/', '18-19':basefolder+'/ORCHARDS_run19H3N2/A_Singapore_INFIMH-16-0019_2016/'},'H1N1':{'17-18':basefolder+'Orchards_run19/A_Michigan_45_2015_H1N1_18/','18-19':basefolder+'Orchards_run19/A_Michigan_45_2015_H1N1_19/'},'H1N1pdm':{'17-18':basefolder+'Orchards_run19/A_Michigan_45_2015_H1N1_18/','18-19':basefolder+'Orchards_run19/A_Michigan_45_2015_H1N1_19/'},'Influenza B':{'16-17':basefolder+'Orchards_runB/B_Phuket_3073_2013_17/','17-18':basefolder+'Orchards_runB/B_Phuket_3073_2013_18/'}}\n",
    "refFileDict={'H3N2':{'17-18':basefolder+'A_Hong_Kong_4801_2014_EPI834581.fasta', '18-19':basefolder+'A_Singapore_INFIMH-16-0019_2016.fasta'},'H1N1':{'17-18':basefolder+'A_Michigan_45_2015_H1N1_18.fasta','18-19':basefolder+'A_Michigan_45_2015_H1N1_19.fasta'},'Influenza B':{'16-17':basefolder+'B_Phuket_3073_2013_17.fasta','17-18':basefolder+'B_Phuket_3073_2013_18.fasta'}}\n",
    "gtfFileDict={'H3N2':{'17-18':basefolder+'A_Hong_Kong_4801_2014_EPI834581_antigenic.gtf', '18-19':basefolder+'A_Singapore_INFIMH-16-0019_2016_antigenic.gtf'},'H1N1':{'17-18':basefolder+'A_Michigan_45_2015_H1N1_18_antigenic.gtf','18-19':basefolder+'A_Michigan_45_2015_H1N1_19_antigenic.gtf'},'H1N1pdm':{'17-18':basefolder+'A_Michigan_45_2015_H1N1_18_antigenic.gtf','18-19':basefolder+'A_Michigan_45_2015_H1N1_19_antigenic.gtf'},'Influenza B':{'16-17':basefolder+'B_Phuket_3073_2013_17_antigenic.gtf','17-18':basefolder+'B_Phuket_3073_2013_18_antigenic.gtf'}}\n",
    "subtypeDict = {'Influenza A H3N2':'H3N2','Flu A (H3)':'H3N2', \n",
    "\t'Flu A (Unable to Subtype)':'H3N2', 'Flu B (Yamagata)':'Influenza B', 'Flu A 09H1':'H1N1pdm', \n",
    "\t'Influenza A H1N1':'H1N1pdm', 'Influenza A, Influenza B': 'Mixed', 'Influenza B':'Influenza B',\n",
    "\t'Influenza A H3N2, Influenza B (Yamagata)':'Mixed', 'Influenza A H3N2, Influenza A H1N1':'Mixed',\n",
    "\t'Influenza A, Influenza B (Yamagata)':'Mixed', 'Influenza B (Yamagata)':'Influenza B', 'Influenza A':'H3N2',\n",
    "\t'Influenza B (Victoria)':'Influenza B', 'H3N2':'H3N2','H1N1pdm':'H1N1pdm','Influenza B':'Influenza B'}\n",
    "\n",
    "snpGenieDict = {'H3N2':{'18-19':'/mnt/d/orchards/h1n1/ORCHARDS_run19H3N2/A_Singapore_INFIMH-16-0019_2016/'+snpGenieFolderName+'/A_Singapore_INFIMH-16-0019_2016_',\n",
    "                        '17-18':'/mnt/d/orchards/h1n1/ORCHARDS_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/'+snpGenieFolderName+'/A_Hong_Kong_4801_2014_834574_'},\n",
    "                'H1N1pdm':{'18-19':'/mnt/d/orchards/h1n1/ORCHARDS_run19/A_Michigan_45_2015_H1N1_19/'+snpGenieFolderName+'/A_Michigan_45_2015_',\n",
    "                          '17-18':'/mnt/d/orchards/h1n1/ORCHARDS_run19/A_Michigan_45_2015_H1N1_19/'+snpGenieFolderName+'/A_Michigan_45_2015_'},\n",
    "               'Influenza B':{'17-18':'/mnt/d/orchards/h1n1/ORCHARDS_runB/B_Phuket_3073_2013_18/'+snpGenieFolderName+'/B_Phuket_3073_2013_',\n",
    "                              '16-17':'/mnt/d/orchards/h1n1/ORCHARDS_runB/B_Phuket_3073_2013_18/'+snpGenieFolderName+'/B_Phuket_3073_2013_'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define antigenic sites\n",
    "def convertListofClassicH3N2SitestoZeroIndexedMStart(listOfSites):\n",
    "    return [site+15 for site in listOfSites]\n",
    "\n",
    "H1N1_antigenic_sites = [87,88,90,91,92, 132,\n",
    "          141,142,143,171,172,174,177,180,\n",
    "          170,173,202,206,210,211,212,\n",
    "          151,154,156,157,158,159,200,238,\n",
    "          147]\n",
    "H1N1_antigenic_sites = [site-1 for site in H1N1_antigenic_sites] #convert to zero-index\n",
    "\n",
    "antigenic_sites = {59, 60, 61, 62, 63, 65, 66, 68, 69, 72, 74, 77, 78, 82, 90, 93, 95, 96, 97, 98, 101, 102, 103, 106, 107, 109, 111, 117, 118, 124, 132, 136, 137, 139, 141, 143, 144, 145, 146, 147, 148, 150, 152, 153, 155, 157, 158, 159, 160, 161, 165, 167, 170, 171, 172, 173, 174, 178, 180, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 194, 197, 201, 202, 203, 204, 205, 207, 208, 209, 211, 212, 213, 216, 218, 222, 223, 224, 227, 228, 229, 230, 231, 232, 233, 234, 241, 242, 243, 244, 245, 253, 255, 257, 259, 261, 262, 263, 275, 276, 277, 280, 288, 290, 291, 293, 294, 295, 309, 312, 314, 315, 319, 320, 322, 323, 324, 325, 326, 327}\n",
    "glycosylation_sites = set(convertListofClassicH3N2SitestoZeroIndexedMStart([8,22,38,45,63,81,133,126,159,160,165,246,285]))\n",
    "\n",
    "antigenic_sites = antigenic_sites.union(glycosylation_sites)\n",
    "#antigenic sites are 0-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert from 1-index to 0-index\n",
    "# antigenic_sites = set(np.array(list(antigenic_sites))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load subject metadata keys, convert into useable format from what RedHat provides\n",
    "metadataMetadata = pd.read_csv(completemetadatakey, **read_csv_args)\n",
    "mm = metadataMetadata\n",
    "\n",
    "mm = mm.loc[mm['Choices, Calculations, OR Slider Labels'].notna()]\n",
    "mm = mm.drop(['Description', 'Field Type'], axis=1).set_index('Variable / Field Name')\n",
    "mmdict = mm['Choices, Calculations, OR Slider Labels'].to_dict()\n",
    "metadataDict = {key:{value.split(',')[0].strip():value.split(',')[1].strip() for value in valuelist.split('|')} for key, valuelist in mmdict.items() if '|' in valuelist}\n",
    "metadataDict = {key:{**value,**{'':''}} for key, value in metadataDict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load subject metadata, separate out household members into their own listing\n",
    "subjects = pd.read_csv(expandedMetadata, **read_csv_args)\n",
    "relatives = pd.DataFrame()\n",
    "priorColumns = []\n",
    "for idnum in range(7, 0, -1):\n",
    "    startid = list(subjects.columns).index(f\"substudy_id_{idnum}\")\n",
    "    columnstomove = list(subjects.columns)[startid:]\n",
    "    newcolumnnames = [column.replace(f'_{idnum}','') for column in columnstomove]\n",
    "    repsamples = subjects[columnstomove]\n",
    "    subjects = subjects.drop(columnstomove, axis=1)\n",
    "    repsamples.columns = newcolumnnames\n",
    "    priorColumns=newcolumnnames\n",
    "    \n",
    "    relatives = relatives.append(repsamples, sort=True)\n",
    "    \n",
    "relatives = relatives.loc[relatives.substudy_id != ''].rename(columns={'substudy_id':'record_id'})\n",
    "relatives['record_id'] = relatives.record_id.str.replace('-','.')\n",
    "subjects = subjects.append(relatives, sort=True)\n",
    "\n",
    "subjects = subjects.replace(r'^\\s*$', np.nan, regex=True).reset_index(drop=True).dropna(how='all')\n",
    "subjects = subjects.loc[~pd.isna(subjects.record_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects['recieved_flu_vaccine'] = subjects.vaccine_status.combine_first(subjects.flu_vaccine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects.record_id = subjects.record_id.astype(str)\n",
    "subjects['age_category'] = '18 or Under'\n",
    "subjects.loc[subjects.age>18, 'age_category'] = 'Over 18'\n",
    "seq = pd.read_csv('/mnt/d/orchards/h1n1/figures/allSamplesSequenced_evenIfFailed.csv', **read_csv_args)\n",
    "seq['Patient ID'] = seq['Patient ID'].astype(str)\n",
    "x = seq.merge(subjects, left_on='Patient ID', right_on='record_id', how='left', suffixes=('_seq', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine household id from ptID information\n",
    "\n",
    "subjects = subjects.rename(columns={'record_id':'participant_ID'}).reset_index(drop=True)\n",
    "subjects['household'] = subjects.participant_ID.astype(str).str.split('.').str[0]\n",
    "subjects['day0_sample'] = ''\n",
    "subjects['day7_sample'] = ''\n",
    "\n",
    "subjects['household'] = subjects[['household', 'paired_with']].apply(lambda x: str(int(min(float(x.household), float(x.paired_with)))),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate time since symptom onset for each sample collection time\n",
    "\n",
    "subjects['day0_sample_date'] = pd.to_datetime(pd.concat([subjects.date_day0, subjects.home_visit_appointment]).dropna().sort_index())\n",
    "# subjects['symptom_onset_day0'] = subjects.symptom_onset_day0.replace(r'^\\s*$', np.nan, regex=True).astype(float)\n",
    "subjects['days_since_symptom_onset'] = pd.to_timedelta(subjects.symptom_onset_day0, unit='D')\n",
    "subjects['time_of_symptom_onset'] = pd.to_datetime(subjects.time_symptoms_started)\n",
    "\n",
    "subjects.loc[subjects.time_of_symptom_onset.isna() & subjects.days_since_symptom_onset.notna(), 'time_of_symptom_onset'] = subjects.day0_sample_date - subjects.days_since_symptom_onset\n",
    "\n",
    "subjects = subjects.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At this point, so many columns that were once mixed dtypes are now numeric. To streamline things, \n",
    "#convert all columns to numeric dtypes where possible.\n",
    "#convert subject columns to numeric where possible\n",
    "for column in subjects.select_dtypes(exclude=[np.datetime64, np.timedelta64, np.number]).columns:\n",
    "    try:\n",
    "        subjects[column] = pd.to_numeric(subjects[column])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORCHARDS participants and household members often had variables called different things.\n",
    "#Here we unify the name of those variables to allow for direct comparisons.\n",
    "subjects['recieved_flu_vaccine'] = subjects.vaccine_status.combine_first(subjects.flu_vaccine)\n",
    "\n",
    "subjects['day0_symptom_severity'] = subjects.symptom_severity_day0.combine_first(subjects.illness_severity)\n",
    "subjects['day7_symptom_severity'] = subjects.symptom_severity_day7.combine_first(subjects.symptm_severity_day7_index)\n",
    "\n",
    "subjects['saw_a_doctor_day0'] = subjects.healthcarevisit_prior.combine_first(subjects.healthcare_provider_day0)\n",
    "subjects['saw_a_doctor_day7'] = subjects.healthcare_provider_day7.combine_first(subjects.hlthcre_providr_day7_index)\n",
    "\n",
    "subjects['day0_pcr_ct'] = subjects.student_ct_value.combine_first(subjects.pcrday0_1_ctvalue).combine_first(subjects.pcrday0_ctvalue)\n",
    "subjects['day7_pcr_ct'] = subjects.pcrday72_c68_ctvalue.combine_first(subjects.pcrday7_1_ctvalue).combine_first(subjects.pcrday7_index_ctvalue)\n",
    "\n",
    "subjects['day0_sample_date'] = pd.to_datetime(subjects.date_day0.combine_first(subjects.home_visit_appointment))\n",
    "subjects['day7_sample_date'] = pd.to_datetime(subjects.date_day7.combine_first(subjects.date_day7_index))\n",
    "\n",
    "subjects['day7_time_of_symptom_onset'] = subjects.symptom_onset_day7.combine_first(subjects.symptom_onset_day7_index)\n",
    "\n",
    "#ORCHARDS participants' sample date is more precise than the household members' sample dates.\n",
    "#Specifically, household members have only the date as their sample date, while orchards participants\n",
    "#have the date and time of collection. Here we make sure that, where possible, household members also\n",
    "#have the date *and time* of sample collection as recorded by the date and time of the home visit appointment.\n",
    "for hh_id, df in subjects.groupby('household'):\n",
    "    initialvisittime = pd.to_datetime(df.iloc[0].home_visit_appointment)\n",
    "    subjects.loc[(subjects.household == hh_id) & (subjects.day0_sample_date.dt.date == initialvisittime.date()), 'day0_sample_date'] = initialvisittime\n",
    "\n",
    "    subjects = subjects.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCR results are encoded, here I decode those results from a number to the virus\n",
    "subtypeColumns = [column for column in subjects.columns if ('pcr' in column) and ('ct' not in column) and ('c68' not in column)]\n",
    "subjects['pcr_subtype'] = subjects[subtypeColumns].apply(np.max, axis=1)\n",
    "subjects['pcr_subtype'] = subjects['pcr_subtype'].map({24.0:'H1N1pdm',25.0:'H3N2',3.0:'Influenza B',26.0:'Influenza B',27.0:'Influenza B'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepcolumns = ['household', 'participant_ID', 'age', 'gender', 'time_of_symptom_onset', \n",
    "               'day7_time_of_symptom_onset','recieved_flu_vaccine','school','jackson_score', 'pcr_subtype', \n",
    "               'day0_sample', 'day7_sample','day0_sample_date', 'day7_sample_date', \n",
    "               'day0_symptom_severity', 'day7_symptom_severity',\n",
    "               'day0_pcr_ct', 'day7_pcr_ct', \n",
    "               'saw_a_doctor_day0', 'saw_a_doctor_day7', \n",
    "               'antipyretic_use','flu_antiviral_treatment','school_absence', \n",
    "               'number_in_household','recent_travel']\n",
    "\n",
    "subjects = subjects[keepcolumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going back and fixing some edge cases related to time of symptom onset estimate\n",
    "subjects['day0startestimate'] = subjects.time_of_symptom_onset\n",
    "\n",
    "subjects['day7startestimate'] = subjects.day7_sample_date-pd.to_timedelta(subjects.day7_time_of_symptom_onset, unit='d')\n",
    "\n",
    "#Setting time between start of symptoms and sample collection\n",
    "\n",
    "#First, if someone was not sick on day 0 and got sick over the course of the week, I want to assign time_of_symptom_onset to their day 7 estimate.\n",
    "nullday0ToSO = pd.isnull(subjects.day0startestimate)\n",
    "hasday7symptomOnset = pd.notnull(subjects.day7_time_of_symptom_onset)\n",
    "\n",
    "subjects.loc[nullday0ToSO & hasday7symptomOnset, 'time_of_symptom_onset'] = subjects.day7startestimate\n",
    "\n",
    "#Second, there are over 200 instances where people's estimate of when their illness started changes between day 0 and day 7\n",
    "#Above, I just go with their estimate on day 0, since memories are more accurate then\n",
    "#However if they are PCR negative on day0 and pcr positive on day 7, then perhaps they had a cold on day 0 and the flu on day 7 (poor people)\n",
    "#44 people fit this description. Below I look for these people and change their start of illness to their day7 estimate.\n",
    "\n",
    "nonulls = pd.isnull(subjects.day0_pcr_ct) & pd.notnull(subjects.day7_pcr_ct)\n",
    "\n",
    "greaterThanOneDayDifference = (subjects.day7startestimate-subjects.day0startestimate) > pd.to_timedelta('1 day')\n",
    "\n",
    "\n",
    "subjects.loc[nonulls & greaterThanOneDayDifference, 'time_of_symptom_onset'] = subjects.day7startestimate\n",
    "subjects.loc[pd.notnull(subjects.day0_pcr_ct), 'day0_days_since_symptom_onset'] = (subjects.day0_sample_date - subjects.time_of_symptom_onset).dt.days\n",
    "subjects.loc[pd.notnull(subjects.day7_pcr_ct), 'day7_days_since_symptom_onset'] = (subjects.day7_sample_date - subjects.time_of_symptom_onset).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up columns that were involved in estimating likely onset of symptoms for each patient\n",
    "subjects = subjects.drop(columns=['day7_time_of_symptom_onset','day0startestimate','day7startestimate','day0_days_since_symptom_onset','day7_days_since_symptom_onset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects['season'] = '17-18'\n",
    "subjects.loc[(subjects.time_of_symptom_onset > pd.to_datetime('7-1-16'))&(subjects.time_of_symptom_onset < pd.to_datetime('7-1-17')), 'season'] = '16-17'\n",
    "subjects.loc[(subjects.time_of_symptom_onset > pd.to_datetime('7-1-18'))&(subjects.time_of_symptom_onset < pd.to_datetime('7-1-19')), 'season'] = '18-19'\n",
    "\n",
    "noOnsetTime = pd.isna(subjects.time_of_symptom_onset)\n",
    "hasDay0Date = pd.notna(subjects.day0_sample_date)\n",
    "hasDay7Date = pd.notna(subjects.day7_sample_date)\n",
    "if len(subjects.loc[noOnsetTime])>0:\n",
    "    if len(subjects.loc[noOnsetTime&hasDay0Date])>0:\n",
    "        subjects.loc[noOnsetTime&hasDay0Date&(subjects.day0_sample_date > pd.to_datetime('7-1-16'))&(subjects.day0_sample_date < pd.to_datetime('7-1-17')), 'season'] = '16-17'\n",
    "        subjects.loc[noOnsetTime&hasDay0Date&(subjects.day0_sample_date > pd.to_datetime('7-1-18'))&(subjects.day0_sample_date < pd.to_datetime('7-1-19')), 'season'] = '18-19'\n",
    "    if len(subjects.loc[noOnsetTime&hasDay7Date])>0:\n",
    "        subjects.loc[noOnsetTime&hasDay7Date&(subjects.day7_sample_date > pd.to_datetime('7-1-16'))&(subjects.day7_sample_date < pd.to_datetime('7-1-17')), 'season'] = '16-17'\n",
    "        subjects.loc[noOnsetTime&hasDay7Date&(subjects.day7_sample_date > pd.to_datetime('7-1-18'))&(subjects.day7_sample_date < pd.to_datetime('7-1-19')), 'season'] = '18-19'        \n",
    "    else:\n",
    "        raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sample metadata...\n",
      "Calculating sampling dates...\n"
     ]
    }
   ],
   "source": [
    "#OK, we've processed subject data. Time to bring in info related to each *sample* taken\n",
    "#(These are distinct; some subjects have more than one sample)\n",
    "print ('loading sample metadata...')\n",
    "metadata = pd.read_csv(metadatafile, **read_csv_args).rename({'Unnamed: 0':'ID'},axis=1)\n",
    "metadata = metadata.loc[metadata.Sequenced].reset_index(drop=True)\n",
    "metadata = metadata.drop('Sequenced', axis=1)\n",
    "successfulSequences = []\n",
    "for vcf in vcfdirs:\n",
    "    successfulSequences.extend(glob.glob(vcf+'/*.vcf'))\n",
    "\n",
    "successfulSequences = [seq.split('/')[-1].split('.')[0] for seq in successfulSequences]\n",
    "metadata['mapped'] = metadata.ID.isin(successfulSequences)\n",
    "\n",
    "#Calculate the date of sample collection\n",
    "print ('Calculating sampling dates...')\n",
    "metadata['sample_date'] = pd.to_datetime(metadata.date)\n",
    "metadata['decimalDate']=metadata.sample_date.apply(lambda x: x.year+(x.dayofyear-1)/365)\n",
    "metadata['seasonalDate'] = metadata.sample_date.apply(lambda x:(((x.dayofyear-1)/365)+.5)%1) #June 1st will be start of season\n",
    "metadata['week'] = metadata.sample_date.apply(lambda x: x.isocalendar()[1])\n",
    "metadata['season'] = metadata.decimalDate.apply(lambda x: str(int(x-0.5))[-2:]+ \"-\" +str(int(x+0.5))[-2:])\n",
    "\n",
    "\n",
    "\n",
    "metadata['subtype'] = metadata['subtype'].map(subtypeDict) #standardize subtype\n",
    "metadata = metadata.loc[metadata.mapped] #only samples which successfully mapped for downstream analysis\n",
    "\n",
    "metadata = metadata.drop(['notes', 'date'], axis = 1)\n",
    "metadata = metadata.reset_index(drop=True).rename(columns={'ID':'sampleID'})\n",
    "\n",
    "\n",
    "#Remove mixed samples\n",
    "metadata = metadata.loc[metadata.subtype.isin(['H3N2', 'H1N1pdm', 'Influenza B'])].reset_index(drop=True) \n",
    "metadata = metadata.loc[~metadata['sampleID'].isin(potentialmixed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973\n"
     ]
    }
   ],
   "source": [
    "#Go through subjects, identify which samples were taken on day0 or day7, fill in relevant info\n",
    "subjects['day0_sample'] = subjects['day0_sample'].astype(object)\n",
    "subjects['day7_sample'] = subjects['day7_sample'].astype(object)\n",
    "metadata['day0_or_day7_sample'] = 'Day 0'\n",
    "print(len(pd.notna(subjects.day0_sample)))\n",
    "\n",
    "for i, subjectrow in subjects.iterrows():\n",
    "    ptsamples = metadata.loc[metadata.ptID == subjectrow.participant_ID]\n",
    "    sampledate = pd.to_datetime(ptsamples.sample_date)\n",
    "    #the dates don't always line up perfectly, so just call the earlier sample day0 and the later one day7\n",
    "    if len(ptsamples) >1:\n",
    "        ptsamples.sort_values('sample_date').reset_index(drop=True)\n",
    "        subjects.at[i, 'day0_sample'] = ptsamples.iloc[0].sampleID\n",
    "        subjects.at[i, 'day0_pcr_ct'] = ptsamples.iloc[0].CT\n",
    "        subjects.at[i, 'day7_sample'] = ptsamples.iloc[1].sampleID\n",
    "        subjects.at[i, 'day7_pcr_ct'] = ptsamples.iloc[1].CT\n",
    "        metadata.loc[metadata.sampleID == ptsamples.iloc[1].sampleID,'day0_or_day7_sample'] = 'Day 7'\n",
    "\n",
    "    elif len(ptsamples) == 1:\n",
    "        if pd.notnull(subjectrow.day0_sample_date) & pd.notnull(subjectrow.day7_sample_date):\n",
    "            if (abs(subjectrow.day0_sample_date-ptsamples.iloc[0].sample_date) > abs(subjectrow.day7_sample_date-ptsamples.iloc[0].sample_date)):\n",
    "                subjects.at[i, 'day7_sample'] = ptsamples.iloc[0].sampleID\n",
    "                metadata.loc[metadata.sampleID == ptsamples.iloc[0].sampleID,'day0_or_day7_sample'] = 'Day 7'\n",
    "                if pd.isnull(subjectrow.day7_pcr_ct):\n",
    "                    subjects.at[i, 'day7_pcr_ct'] = ptsamples.iloc[0].CT\n",
    "            else:\n",
    "                subjects.at[i, 'day0_sample'] = ptsamples.iloc[0].sampleID\n",
    "                if pd.isnull(subjectrow.day0_pcr_ct):\n",
    "                    subjects.at[i, 'day0_pcr_ct'] = ptsamples.iloc[0].CT\n",
    "        elif pd.notnull(subjectrow.day0_sample_date):\n",
    "            subjects.at[i, 'day0_sample'] = ptsamples.iloc[0].sampleID\n",
    "            if pd.isnull(subjectrow.day0_pcr_ct):\n",
    "                subjects.at[i, 'day0_pcr_ct'] = ptsamples.iloc[0].CT\n",
    "        elif pd.notnull(subjectrow.day7_sample_date):\n",
    "            subjects.at[i, 'day7_sample'] = ptsamples.iloc[0].sampleID\n",
    "            metadata.loc[metadata.sampleID == ptsamples.iloc[0].sampleID,'day0_or_day7_sample'] = 'Day 7'\n",
    "            if pd.isnull(subjectrow.day7_pcr_ct):\n",
    "                subjects.at[i, 'day7_pcr_ct'] = ptsamples.iloc[0].CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlalli/.local/lib/python3.8/site-packages/pandas/core/generic.py:5494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>household</th>\n",
       "      <th>participant_ID</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>time_of_symptom_onset</th>\n",
       "      <th>recieved_flu_vaccine</th>\n",
       "      <th>school</th>\n",
       "      <th>jackson_score</th>\n",
       "      <th>pcr_subtype</th>\n",
       "      <th>day0_sample</th>\n",
       "      <th>day7_sample</th>\n",
       "      <th>day0_sample_date</th>\n",
       "      <th>day7_sample_date</th>\n",
       "      <th>day0_symptom_severity</th>\n",
       "      <th>day7_symptom_severity</th>\n",
       "      <th>day0_pcr_ct</th>\n",
       "      <th>day7_pcr_ct</th>\n",
       "      <th>saw_a_doctor_day0</th>\n",
       "      <th>saw_a_doctor_day7</th>\n",
       "      <th>antipyretic_use</th>\n",
       "      <th>flu_antiviral_treatment</th>\n",
       "      <th>school_absence</th>\n",
       "      <th>number_in_household</th>\n",
       "      <th>recent_travel</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1825</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-04-01 15:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>H1N1pdm</td>\n",
       "      <td>19VR006778</td>\n",
       "      <td>19VR007266</td>\n",
       "      <td>2019-04-02 16:00:00</td>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.66</td>\n",
       "      <td>27.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>1825</td>\n",
       "      <td>1825.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-03 00:00:00</td>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>1825</td>\n",
       "      <td>1825.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-04-02 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-03 00:00:00</td>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>1825</td>\n",
       "      <td>1825.3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-03-31 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-03 00:00:00</td>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1825</td>\n",
       "      <td>1825.2</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-03 00:00:00</td>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>1825</td>\n",
       "      <td>1825.1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-04-02 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>H1N1pdm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19VR007284</td>\n",
       "      <td>2019-04-03 00:00:00</td>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     household  participant_ID   age  gender time_of_symptom_onset  \\\n",
       "280       1825          1825.0   8.0     1.0   2019-04-01 15:00:00   \n",
       "304       1825          1825.5   9.0     1.0                   NaT   \n",
       "359       1825          1825.4  11.0     2.0   2019-04-02 00:00:00   \n",
       "533       1825          1825.3  13.0     1.0   2019-03-31 00:00:00   \n",
       "742       1825          1825.2  36.0     1.0                   NaT   \n",
       "963       1825          1825.1  38.0     2.0   2019-04-02 00:00:00   \n",
       "\n",
       "     recieved_flu_vaccine  school  jackson_score pcr_subtype day0_sample  \\\n",
       "280                   1.0     6.0           12.0     H1N1pdm  19VR006778   \n",
       "304                   1.0     NaN            NaN         NaN         NaN   \n",
       "359                   1.0     NaN            NaN         NaN         NaN   \n",
       "533                   NaN     NaN            NaN         NaN         NaN   \n",
       "742                   0.0     NaN            NaN         NaN         NaN   \n",
       "963                   1.0     NaN            NaN     H1N1pdm         NaN   \n",
       "\n",
       "    day7_sample    day0_sample_date day7_sample_date  day0_symptom_severity  \\\n",
       "280  19VR007266 2019-04-02 16:00:00       2019-04-10                    2.0   \n",
       "304         NaN 2019-04-03 00:00:00       2019-04-10                    NaN   \n",
       "359         NaN 2019-04-03 00:00:00       2019-04-10                    1.0   \n",
       "533         NaN 2019-04-03 00:00:00       2019-04-10                    1.0   \n",
       "742         NaN 2019-04-03 00:00:00       2019-04-10                    NaN   \n",
       "963  19VR007284 2019-04-03 00:00:00       2019-04-10                    1.0   \n",
       "\n",
       "     day7_symptom_severity  day0_pcr_ct  day7_pcr_ct  saw_a_doctor_day0  \\\n",
       "280                    2.0        25.66        27.46                0.0   \n",
       "304                    NaN          NaN          NaN                NaN   \n",
       "359                    1.0          NaN          NaN                0.0   \n",
       "533                    1.0          NaN          NaN                0.0   \n",
       "742                    NaN          NaN          NaN                NaN   \n",
       "963                    2.0          NaN        22.29                0.0   \n",
       "\n",
       "     saw_a_doctor_day7  antipyretic_use  flu_antiviral_treatment  \\\n",
       "280                0.0              0.0                      0.0   \n",
       "304                NaN              NaN                      NaN   \n",
       "359                0.0              NaN                      NaN   \n",
       "533                0.0              NaN                      NaN   \n",
       "742                NaN              NaN                      NaN   \n",
       "963                0.0              NaN                      NaN   \n",
       "\n",
       "     school_absence  number_in_household  recent_travel season  \n",
       "280             1.0                  5.0            1.0  18-19  \n",
       "304             NaN                  NaN            NaN  18-19  \n",
       "359             NaN                  NaN            NaN  18-19  \n",
       "533             NaN                  NaN            NaN  18-19  \n",
       "742             NaN                  NaN            NaN  18-19  \n",
       "963             NaN                  NaN            NaN  18-19  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidatecases = subjects.loc[subjects.household == 1825.0]\n",
    "# candidatecases[['day0_sample','day7_sample', 'isIndexCase']]\n",
    "candidatecases.time_of_symptom_onset = pd.to_datetime(candidatecases.time_of_symptom_onset)\n",
    "first_onset = pd.to_datetime(candidatecases.time_of_symptom_onset.dropna().dt.date.min(skipna=True))\n",
    "potentialindexcases = candidatecases.loc[abs(candidatecases.time_of_symptom_onset - first_onset - pd.to_timedelta('12 hours')) < pd.to_timedelta('24 hours')]\n",
    "\n",
    "indexcase = candidatecases.loc[pd.to_datetime(candidatecases.time_of_symptom_onset.dt.date) == first_onset, 'participant_ID']\n",
    "indexcase = indexcase.values[0]\n",
    "\n",
    "candidatecases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that each subject has an accurate estimated time of symptom onset and an update PCR ct,\n",
    "#we can finally go back and identify which members of each household got sick first\n",
    "theleftovers = pd.DataFrame()\n",
    "def findIndexCase(candidatecases, theleftovers):\n",
    "    # I calculate the index by first looking at all samples that within 24 hours of the earliest symptom onset case. \n",
    "    # (I calculate the minimum *date*, not minimum time, since only some samples have hour of onset numbers,\n",
    "    # and those estimates are likely to be inaccurate. I also add 12 hours before calculating the distance to ensure that\n",
    "    # samples with time-of-days are rounded to the nearest date, not truncated (i.e., 12/8/19 @ 4pm is considered to be 'on the same day'\n",
    "    # as 12/9/19 @ 0:00:00))\n",
    "\n",
    "    for _ in range(len(candidatecases)):\n",
    "        #if an index infection is more than 10 days prior to the next hh infection, then it was probably unrelated.\n",
    "        #I will make the index infection the next infection in the list.\n",
    "        first_onset = pd.to_datetime(candidatecases.time_of_symptom_onset.dropna().dt.date.min(skipna=True))\n",
    "        potentialindexcases = candidatecases.loc[abs(candidatecases.time_of_symptom_onset - first_onset - pd.to_timedelta('12 hours')) < pd.to_timedelta('24 hours')]\n",
    "\n",
    "        if len(candidatecases.loc[candidatecases.time_of_symptom_onset-first_onset < pd.to_timedelta('10 days')]) > 0:\n",
    "            indexcase = candidatecases.loc[pd.to_datetime(candidatecases.time_of_symptom_onset.dt.date) == first_onset, 'participant_ID']\n",
    "            break\n",
    "        elif len(candidatecases) == 1:\n",
    "            indexcase = candidatecases['participant_ID']\n",
    "            break\n",
    "        else:\n",
    "            candidatecases = candidatecases.loc[~(candidatecases.time_of_symptom_onset.dt.date == first_onset)]\n",
    "    else:\n",
    "        theleftovers = theleftovers.append(hh)\n",
    "        return np.NaN, theleftovers\n",
    "    \n",
    "    \n",
    "    # Afterwards I break ties like so:\n",
    "    #    1) Highest day0 ct wins (ct peaks early in infection, so lower ct = longer infected)\n",
    "    #    2) Highest day7 ct wins\n",
    "    #    3) Worst symptoms wins (doesn't actually come into play in this data set, since all sequenced samples have a ct)\n",
    "    if len(indexcase) > 1:\n",
    "        candidatecases = candidatecases.loc[candidatecases.participant_ID.isin(indexcase)]\n",
    "        if len(candidatecases.day0_pcr_ct.dropna())!=0:\n",
    "            indexcase = candidatecases.loc[candidatecases.day0_pcr_ct == candidatecases.day0_pcr_ct.max(skipna=True), 'participant_ID']\n",
    "        elif len(candidatecases.day7_pcr_ct.dropna())!=0:\n",
    "            indexcase = candidatecases.loc[candidatecases.day7_pcr_ct == candidatecases.day7_pcr_ct.max(skipna=True), 'participant_ID']\n",
    "        elif len(candidatecases.day0_symptom_severity.dropna())!=0:\n",
    "            indexcase = candidatecases.loc[candidatecases.day0_symptom_severity == candidatecases.day0_symptom_severity.max(skipna=True), 'participant_ID']\n",
    "        elif len(candidatecases.day7_symptom_severity.dropna())!=0:\n",
    "            indexcase = candidatecases.loc[candidatecases.day7_symptom_severity == candidatecases.day7_symptom_severity.max(skipna=True), 'participant_ID']\n",
    "        else:\n",
    "            indexcase = []\n",
    "    \n",
    "    if len(indexcase) == 0:\n",
    "        return np.nan, theleftovers\n",
    "    return indexcase.values[0],theleftovers\n",
    "\n",
    "#Assign HHindexcase\n",
    "subjects['isIndexCase'] = False\n",
    "subjects['%ofHHinfected'] = np.NaN\n",
    "for i, hh in subjects.groupby('household'):\n",
    "    \n",
    "    indexcase, theleftovers = findIndexCase(hh, theleftovers)\n",
    "    \n",
    "    if indexcase:\n",
    "        subjects.at[subjects.participant_ID == indexcase, 'isIndexCase'] = True\n",
    "        tmp = hh.day0_pcr_ct\n",
    "        tmp.update(hh.day7_pcr_ct)\n",
    "        num_infected = len(tmp.dropna())\n",
    "        \n",
    "        if num_infected <1:\n",
    "            continue\n",
    "        if len(hh.number_in_household.dropna()) > 0:\n",
    "            num_in_hh = int(hh.number_in_household.dropna().item())\n",
    "            if num_in_hh >= num_infected:\n",
    "                if (num_in_hh == num_infected) & (num_infected ==1):\n",
    "                    pass\n",
    "                else:\n",
    "                    subjects.at[subjects.participant_ID == indexcase, '%ofHHinfected'] = (num_infected-1)/(num_in_hh-1)\n",
    "            elif num_in_hh < num_infected:\n",
    "                pass\n",
    "            \n",
    "\n",
    "    \n",
    "#if an index infection is more than 10 days prior to the next hh infection, then it was probably unrelated.\n",
    "#I will make the index infection the next infection in the list.\n",
    "#df = df.sort_values(by='time_of_symptom_onset').reset_index(drop=True)\n",
    "#df = df.loc[df.time_of_symptom_onset.notna()].reset_index(drop=True)\n",
    "# I calculate the index by first looking at all samples that within 24 hours of the earliest symptom onset case. \n",
    "# (I calculate the minimum *date*, not minimum time, since only some samples have hour of onset numbers,\n",
    "# and those estimates are likely to be inaccurate. I also add 12 hours before calculating the distance to ensure that\n",
    "# samples with time-of-days are rounded to the nearest date, not truncated (i.e., 12/8/19 @ 4pm is considered to be 'on the same day'\n",
    "# as 12/9/19 @ 0:00:00))\n",
    "\n",
    "# Afterwards I break ties by using the highest measured ct. #Changed to highest from lowest on 5/4/2020\n",
    "\n",
    "#leftovers are households that could not be assigned a clear index case. This was a debugging tool; at this point, there should be no leftovers.\n",
    "assert len(theleftovers) == 0, theleftovers\n",
    "hhWithNoIndexCase = [hh for hh, df in subjects.groupby('household') if np.nansum(df.isIndexCase) == 0]\n",
    "assert len(hhWithNoIndexCase) == 0, hhWithNoIndexCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The subjects dataframe contains information on every person in our study, regardless of whether they were infected or not.\n",
    "#I am only interested in those not infected when calculating the % of ppl in each household that were infected.\n",
    "#We just did that.\n",
    "#For the purposes of space/computing time, I will now get rid of the entries corresponding to people we do not have a sample for.\n",
    "\n",
    "subjects = subjects.loc[pd.notna(subjects.day0_sample)|pd.notna(subjects.day7_sample)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create categoried age\n",
    "subjects['age_category'] = '18 or Under'\n",
    "subjects.loc[subjects.age>18, 'age_category'] = 'Over 18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge subject metadata into sample metadata\n",
    "samples = metadata.merge(subjects, left_on=['sampleID','season'],right_on=['day0_sample','season'], how='left')\n",
    "samples = samples.merge(subjects, left_on=['sampleID','season'],right_on=['day7_sample','season'], how='left', suffixes=('','_y'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up replicated columns\n",
    "samplecolumns = [column for column in samples.columns if '_y' not in column]\n",
    "samplecolumns = samplecolumns[samplecolumns.index('day0_or_day7_sample')+1:]\n",
    "for column in samplecolumns:\n",
    "    samples[column].update(samples[column+'_y'])\n",
    "    samples = samples.drop(column+'_y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['days_since_symptom_onset'] = pd.to_timedelta(samples.sample_date.dt.date-samples.time_of_symptom_onset.dt.date).dt.days.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['symptom_severity'] = samples.day0_symptom_severity\n",
    "samples['saw_a_doctor'] = samples.saw_a_doctor_day0\n",
    "samples['taken_on_hh_infection_day'] = 0\n",
    "samples.loc[samples.day0_or_day7_sample=='Day 7', 'symptom_severity'] = samples.day7_symptom_severity\n",
    "samples.loc[samples.day0_or_day7_sample=='Day 7', 'saw_a_doctor'] = samples.saw_a_doctor_day7\n",
    "\n",
    "subjects['symptom_severity'] = np.max(subjects[['day0_symptom_severity', 'day7_symptom_severity']],axis=1)\n",
    "subjects['saw_a_doctor'] = np.max(subjects[['saw_a_doctor_day0', 'saw_a_doctor_day7']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that all day0/day7 specific info has been combined into one day0/7 agnostic column in our samples dataframe,\n",
    "#get rid of day0/7 specific columns\n",
    "samples = samples.drop(columns=['day0_sample', 'day0_sample_date', 'day0_symptom_severity', \n",
    "                      'day0_pcr_ct', 'saw_a_doctor_day0',\n",
    "                      'day7_sample', 'day7_sample_date', 'day7_symptom_severity', \n",
    "                      'day7_pcr_ct', 'saw_a_doctor_day7'])\n",
    "\n",
    "samples = samples.rename(columns={'ID':'sample'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of the patients, especially those with only day7 samples, do not have a positive ID of a subtype in their metadata,\n",
    "#But their samples have a subtype assigned.\n",
    "#I could make sure those subjects aquire their sample subtype via a pandas merge,\n",
    "#But I already have this clunky code that does the trick.\n",
    "for ix, row in samples.iterrows():\n",
    "    if row.day0_or_day7_sample == 'Day 0':\n",
    "        samplecolumn = 'day0_sample'\n",
    "    else:\n",
    "        samplecolumn = 'day7_sample'\n",
    "    rowsubtype = row.subtype\n",
    "    subjectSubtype = subjects.loc[subjects[samplecolumn]==row.sampleID, 'pcr_subtype']\n",
    "    if len(subjectSubtype) == 0:\n",
    "        continue\n",
    "    if rowsubtype != subjectSubtype.iloc[0]:\n",
    "        subjects.loc[subjects[samplecolumn]==row.sampleID, 'pcr_subtype'] = rowsubtype\n",
    "\n",
    "subjects = subjects.rename(columns={'pcr_subtype':'subtype'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use preexisting calculated subtype-specific HA trees to assign each sample to a clade\n",
    "if usePreCalcClades == False:\n",
    "    clades = {}#{sample:None for sample in samples['sample']}\n",
    "    subclades = {}#{sample:None for sample in samples['sample']}\n",
    "\n",
    "    with open(clade_references, 'r') as f:\n",
    "        references = {line.split(' - ')[1].strip().replace('/','_'):line.split(' - ')[0].strip() for line in f.readlines() if len(line.split(' - ')) > 1}\n",
    "\n",
    "\n",
    "    # Algorithm:\n",
    "    # For each leaf that is not a reference:\n",
    "    # Go up one level. If a reference is now a leaf,\n",
    "    # that is the sample's reference. Else,\n",
    "    # keep going up. \n",
    "    # AKA assign closest related reference file\n",
    "    for treefile in treefiles:\n",
    "        tree = ete3.Tree(treebase+treefile, format=1)\n",
    "        for leaf in tqdm(tree.get_leaves()):\n",
    "            if leaf.name not in references:\n",
    "                parent = leaf.up\n",
    "                while True:\n",
    "                    for leafname in parent.get_leaf_names():\n",
    "                        if leafname in references: #Only present in reference sequence names\n",
    "                            subclade = references[leafname]\n",
    "                            subclades[leaf.name] = subclade\n",
    "                            clades[leaf.name] = subclade.split('|')[0]\n",
    "                            break\n",
    "                    else:\n",
    "                        parent = parent.up\n",
    "                        if parent.is_root():\n",
    "                            clades[leaf.name] = '?'\n",
    "                            break\n",
    "                        continue\n",
    "                    break\n",
    "    clades = pd.DataFrame(dict(clades = clades, subclades = subclades)).reset_index()\n",
    "    clades['subclades'] = clades.subclades.str.split('|').str[1]\n",
    "    clades = clades.rename(columns={'index':'sampleID','clades':'clade','subclades':'subclade'})\n",
    "    clades.to_csv(figures+'/clades.csv', sep='\\t')\n",
    "else:\n",
    "    clades = pd.read_csv(figures+'/clades.csv', **read_tsv_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampleID                object\n",
      "subtype                 object\n",
      "CT                     float64\n",
      "season                  object\n",
      "day0_or_day7_sample     object\n",
      "isIndexCase               bool\n",
      "age_category            object\n",
      "clade                   object\n",
      "subclade                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "samples = samples.merge(clades[['sampleID','clade','subclade']], on='sampleID', how='left')\n",
    "samples = samples.drop(columns=['pcr_subtype'])\n",
    "\n",
    "#Downstream merging is complicated by CT, and potentially other columns, being objects when they could be float.\n",
    "#This code checks for that and converts object dtypes to numeric if possible.\n",
    "objcols = samples.select_dtypes(include='object').columns\n",
    "samples[objcols] = samples[objcols].select_dtypes(include='object').apply(pd.to_numeric, errors='ignore')\n",
    "print(samples[objcols].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also want to make sure our subjects DF has clade info. This isn't the most elegant way, but it works\n",
    "\n",
    "clades = clades.loc[clades['sampleID'].isin(list(samples['sampleID']))]\n",
    "subjects=subjects.merge(clades[['sampleID','clade','subclade']], left_on='day0_sample', right_on='sampleID', how='left').drop(columns='sampleID')\n",
    "subjects=subjects.merge(clades[['sampleID','clade','subclade']], left_on='day7_sample', right_on='sampleID', how='left', suffixes=('_day0','_day7')).drop(columns='sampleID')\n",
    "subjects['clade'] = subjects.clade_day0\n",
    "subjects['subclade'] = subjects.subclade_day0\n",
    "subjects['clade'].update(subjects.clade_day7)\n",
    "subjects['subclade'].update(subjects.subclade_day7)\n",
    "subjects = subjects.drop(columns=['clade_day0','subclade_day0','clade_day7','subclade_day7'])\n",
    "\n",
    "#Finally, if no subclade was identified, then just use the name of the clade\n",
    "samples['subclade'] = samples.subclade.fillna(samples.clade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import vcfClass\n",
    "importlib.reload(vcfClass)\n",
    "from vcfClass import VCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### #Code to filter SNPs\n",
    "# import glob\n",
    "# from vcfClass import VCF\n",
    "\n",
    "# mainSampleFolders = ['/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016', \n",
    "#                      '/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Hong_Kong_4801_2014_EPI834581', \n",
    "#                      '/mnt/d/orchards/h1n1/orchards_run19/A_Michigan_45_2015_H1N1_18', \n",
    "#                      '/mnt/d/orchards/h1n1/orchards_run19/A_Michigan_45_2015_H1N1_19',\n",
    "#                      '/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_3073_2013_17',\n",
    "#                      '/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_3073_2013_18']\n",
    "\n",
    "# filteredsnpDirs = [x + '/snp_calls/filtered_snpcalls' for x in mainSampleFolders]\n",
    "# vcfdirs = [x + '/snp_calls' for x in mainSampleFolders]\n",
    "# vcffiles = [x+'/all_snps.vcf' for x in vcfdirs]\n",
    "# filteredVCFfiles = [x+'/all_snps.vcf' for x in filteredsnpDirs]\n",
    "# allSNPsVCFfiles = [x+'/all_snps_with_depths.vcf' for x in vcfdirs]\n",
    "\n",
    "# references = [x.split('/')[-1] for x in mainSampleFolders]\n",
    "# consensusReferences = [mainSampleFolder + '/consensus/' + reference + '_consensus_noambig.fasta' for mainSampleFolder, reference in zip(mainSampleFolders, references)]\n",
    "# distancefiles = [x + '/snp_calls/sequenceDistances.tsv' for x in mainSampleFolders]\n",
    "# gtfFiles = ['/mnt/d/orchards/h1n1/' + reference + '.gtf' for reference in references]\n",
    "# allnucVCFfiles = [x+'/rerun/all_snps.vcf' for x in mainSampleFolders]\n",
    "if not alreadyFilteredSNPs:\n",
    "    bamfiles = {vcffile:dict() for vcffile in vcffiles}\n",
    "    for folder, allSNPs in zip(mainSampleFolders, vcffiles):\n",
    "        files = glob.glob(folder+'/snp_calls/*[0-9].vcf')\n",
    "        for file in files:\n",
    "            sample = file.split('.')[0].split('/')[-1]\n",
    "            bamfile = folder+'/map_to_consensus/'+sample+'.bam'\n",
    "            bamfiles[allSNPs][sample] = bamfile\n",
    "\n",
    "    for folder, allSNPs in zip(mainSampleFolders[1:2], vcffiles[1:2]):\n",
    "        vcf = VCF(folder+'/snp_calls/all_snps.vcf', bamfiles=bamfiles[allSNPs])\n",
    "        vcf.apply_position_filter()\n",
    "        vcf.to_vcf(folder+'/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Note to future self: If you feel like it, it would be interesting to do this via False Discovery rate: '\n",
    "#assign each mutation a pvalue that ref is greater/lesser than alt. Line the pvalues up, rank them, then do a q-rank.\n",
    "#Of course it would mean running this long-ass program again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# methods={'bonferroni':{'signifigance_at':0.05/len(samples)}}\n",
    "# if not alreadyFilteredSNPs:\n",
    "#     for method, kwargs in methods.items():\n",
    "# #         #Filter individual SNPs\n",
    "# #         for folder in mainSampleFolders:\n",
    "# #             files = glob.glob(folder+'/snp_calls/*[0-9].vcf')\n",
    "# #             for file in files:\n",
    "# #                 sample = file.split('.')[0].split('/')[-1]\n",
    "# #                 bamfile = folder+'/map_to_consensus/'+sample+'.bam'\n",
    "# #                 vcf = VCF(file, bamfiles=bamfile).apply_position_filter(signifigance_at=0.05/len(samples))\n",
    "# #                 vcf.to_vcf(folder+'/snp_calls/filtered_snpcalls/'+sample+'.vcf')\n",
    "\n",
    "\n",
    "#         #Filter the all_snps files\n",
    "#         bamfiles = {vcffile:dict() for vcffile in vcffiles}\n",
    "#         for folder, allSNPs in zip(mainSampleFolders, vcffiles):\n",
    "#             files = glob.glob(folder+'/snp_calls/*[0-9].vcf')\n",
    "#             for file in files:\n",
    "#                 sample = file.split('.')[0].split('/')[-1]\n",
    "#                 bamfile = folder+'/map_to_consensus/'+sample+'.bam'\n",
    "#                 bamfiles[allSNPs][sample] = bamfile\n",
    "\n",
    "#         for folder, allSNPs in zip(mainSampleFolders, vcffiles):\n",
    "#             vcf = VCF(folder+'/snp_calls/all_snps.vcf', bamfiles=bamfiles[allSNPs])\n",
    "#             vcf.apply_position_filter(**kwargs)\n",
    "#             vcf.to_vcf(folder+f'/snp_calls/filtered_snpcalls/all_snps.vcf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob(vcfdirs[0]+'/*all*.vcf')\n",
    "# testvcffiles = [glob.glob(vcfdir+'/*all*.vcf') for vcfdir in vcfdirs]\n",
    "# testvcffiles = {'bonferroni':[],'lenient':[],'old':[]}\n",
    "# for files in [glob.glob(vcfdir+'/*all*.vcf') for vcfdir in vcfdirs]:\n",
    "#     for file in files:\n",
    "#         if 'all_snps.vcf' in file:\n",
    "#             testvcffiles['bonferroni'].append(file)\n",
    "#         elif 'lenient' in file:\n",
    "#             testvcffiles['lenient'].append(file)\n",
    "#         elif 'old' in file:\n",
    "#             testvcffiles['old'].append(file)\n",
    "\n",
    "# # testvcffiles['bonferroni']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possSNPs = pd.read_csv(figures+'/allsnps.tsv', **read_tsv_args, low_memory=False).rename(columns={'shortChrom':'segment'})\n",
    "# possSNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf',\n",
       " '/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf',\n",
       " '/mnt/d/orchards/h1n1/orchards_run19/A_Michigan_45_2015_H1N1_18/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf',\n",
       " '/mnt/d/orchards/h1n1/orchards_run19/A_Michigan_45_2015_H1N1_19/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf',\n",
       " '/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_3073_2013_17/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf',\n",
       " '/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_3073_2013_18/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for v in vcffiles:\n",
    "# v = VCF(vcffiles[0])\n",
    "vcffiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VCFs..\n",
      "Annotating...\n",
      "/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016/consensus/A_Singapore_INFIMH-16-0019_2016_consensus_noambig.fasta\n",
      "/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/consensus/A_Hong_Kong_4801_2014_EPI834581_consensus_noambig.fasta\n",
      "/mnt/d/orchards/h1n1/orchards_run19/A_Michigan_45_2015_H1N1_18/consensus/A_Michigan_45_2015_H1N1_18_consensus_noambig.fasta\n",
      "/mnt/d/orchards/h1n1/orchards_run19/A_Michigan_45_2015_H1N1_19/consensus/A_Michigan_45_2015_H1N1_19_consensus_noambig.fasta\n",
      "/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_3073_2013_17/consensus/B_Phuket_3073_2013_17_consensus_noambig.fasta\n",
      "/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_3073_2013_18/consensus/B_Phuket_3073_2013_18_consensus_noambig.fasta\n"
     ]
    }
   ],
   "source": [
    "#Bring in SNP data\n",
    "#Load VCF files, annotate, and save as one master dataframe\n",
    "#If not already done, apply position filter and save VCFs\n",
    "\n",
    "print ('loading VCFs..')\n",
    "allSNPsVCFs = [VCF(x) for x in vcffiles]\n",
    "\n",
    "#Add annotation info\n",
    "print ('Annotating...')\n",
    "for allSNPsVCF, g, r in zip(allSNPsVCFs, gtfFiles, consensusReferences):\n",
    "    print (r)\n",
    "    regions = allSNPsVCF.annotate(g,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/d/orchards/h1n1/A_Singapore_INFIMH-16-0019_2016_antigenic.gtf',\n",
       " '/mnt/d/orchards/h1n1/A_Hong_Kong_4801_2014_EPI834581_antigenic.gtf',\n",
       " '/mnt/d/orchards/h1n1/A_Michigan_45_2015_H1N1_18_antigenic.gtf',\n",
       " '/mnt/d/orchards/h1n1/A_Michigan_45_2015_H1N1_19_antigenic.gtf',\n",
       " '/mnt/d/orchards/h1n1/B_Phuket_3073_2013_17_antigenic.gtf',\n",
       " '/mnt/d/orchards/h1n1/B_Phuket_3073_2013_18_antigenic.gtf']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtfFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting VCF to DataFrame...\n",
      "/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf\n",
      "VCF containing 83 samples and 2161 mutation calls\n",
      "/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf\n",
      "VCF containing 130 samples and 2580 mutation calls\n",
      "/mnt/d/orchards/h1n1/orchards_run19/A_Michigan_45_2015_H1N1_18/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf\n",
      "VCF containing 25 samples and 685 mutation calls\n",
      "/mnt/d/orchards/h1n1/orchards_run19/A_Michigan_45_2015_H1N1_19/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf\n",
      "VCF containing 71 samples and 1868 mutation calls\n",
      "/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_3073_2013_17/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf\n",
      "VCF containing 5 samples and 65 mutation calls\n",
      "/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_3073_2013_18/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf\n",
      "VCF containing 28 samples and 748 mutation calls\n",
      "exporting backup copy of SNP dataframe...\n"
     ]
    }
   ],
   "source": [
    "print ('converting VCF to DataFrame...')\n",
    "SNPs = pd.DataFrame()\n",
    "for vcffile, vcf, c, reference in zip(vcffiles, allSNPsVCFs, consensusReferences, references):\n",
    "    print(vcffile)\n",
    "    print (vcf)\n",
    "    vcf = vcf.to_dataframe()\n",
    "    vcf['referenceFile'] = c\n",
    "    vcf['subtype'] = referenceDict[reference]\n",
    "    SNPs = SNPs.append(vcf)\n",
    "    SNPs = SNPs.loc[SNPs.DP!=0]\n",
    "    if len(SNPs.loc[SNPs.duplicated()]) > 0:\n",
    "        for i, df in tqdm(SNPs.groupby(['pos','gene','sampleID'])):\n",
    "            if len(df) > 1:\n",
    "                print (len(df), df.iloc[0].pos, df.iloc[0]['gene'], df.iloc[0].sampleID)\n",
    "            if (df.iloc[0].AAstr=='L18L') and (df.iloc[0].sampleID == '17VR005325'):\n",
    "                print (df)\n",
    "        raise Exception\n",
    "#Housekeeping w/r/t gene info from out of reading frame SNPs\n",
    "SNPs.loc[SNPs.AAtype.isna(), 'AAtype'] = \"Out of reading frame\"\n",
    "SNPs['inGenePos'] = SNPs['inGenePos'].replace(r'^\\s*$', np.nan, regex=True).astype(float)\n",
    "#Calc codon position of SNP\n",
    "SNPs = SNPs.rename(columns={'AApos':'codon'})#['codon'] = ((SNPs.inGenePos-SNPs.inGenePos%3)/3)\n",
    "#Turn crazy gtf-specific chrom to common chrom name\n",
    "SNPs['shortChrom'] = SNPs['chrom'].str.split('_').str[-1]\n",
    "#My VCF class keeps and exports one-off mutations calls for all samples, so all samples have SNP entries with 0 depth.\n",
    "#I'll get rid of those here.\n",
    "SNPs = SNPs.loc[SNPs.SDP != 0].reset_index(drop=True)\n",
    "\n",
    "print ('exporting backup copy of SNP dataframe...')\n",
    "SNPs.to_csv(figures+'/allsnps.tsv', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = '''A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t90\t92\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t132\t134\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t180\t182\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t198\t212\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t216\t221\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t225\t230\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t237\t239\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t243\t245\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t252\t257\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t267\t269\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t291\t293\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t300\t302\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t306\t317\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t324\t332\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t339\t344\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t348\t350\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t354\t356\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t372\t377\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t393\t395\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t417\t419\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t429\t434\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t438\t440\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t444\t446\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t450\t467\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t471\t473\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t477\t482\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t486\t488\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t492\t506\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t516\t518\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t522\t524\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t531\t548\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t555\t557\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t561\t563\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t567\t572\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t576\t599\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t603\t605\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t612\t614\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t624\t638\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t642\t650\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t654\t662\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t669\t671\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t675\t677\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t687\t695\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t702\t725\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t744\t758\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t780\t782\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t786\t788\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t792\t794\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t798\t800\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t804\t812\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t846\t854\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t861\t863\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t885\t887\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t891\t896\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t900\t908\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t921\t923\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t948\t950\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t957\t959\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t963\t968\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t978\t983\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t987\t1004\t.\t+\t0\tgene_id \"HA_antigenic\";transcript_id \"HA_antigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tgene\t18\t1718\t.\t+\t.\tgene_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\ttranscript\t18\t1718\t.\t+\t.\tgene_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t18\t89\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t93\t131\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t135\t179\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t183\t197\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t213\t215\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t222\t224\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t231\t236\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t240\t242\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t246\t251\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t258\t266\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t270\t290\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t294\t299\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t303\t305\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t318\t323\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t333\t338\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t345\t347\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t351\t353\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t357\t371\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t378\t392\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t396\t416\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t420\t428\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t435\t437\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t441\t443\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t447\t449\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t468\t470\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t474\t476\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t483\t485\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t489\t491\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t507\t515\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t519\t521\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t525\t530\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t549\t554\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t558\t560\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t564\t566\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t573\t575\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t600\t602\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t606\t611\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t615\t623\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t639\t641\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t651\t653\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t663\t668\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t672\t674\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t678\t686\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t696\t701\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t726\t743\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t759\t779\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t783\t785\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t789\t791\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t795\t797\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t801\t803\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t813\t845\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t855\t860\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t864\t884\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t888\t890\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t897\t899\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t909\t920\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t924\t947\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t951\t956\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t960\t962\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t969\t977\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t984\t986\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";\n",
    "# A_Hong_Kong_4801_2014_834581_HA\tstr_to_gtf\tCDS\t1005\t1718\t.\t+\t0\tgene_id \"HA_nonantigenic\";transcript_id \"HA_nonantigenic\";'''\n",
    "\n",
    "# edited_lines = list()\n",
    "# for line in lines.split('\\n'):\n",
    "#     line = line.split('\\t')\n",
    "#     if line[2] == 'CDS':\n",
    "#         line[3] = str(int(line[3])-3)\n",
    "#         line[4] = str(int(line[4])-3)\n",
    "#     edited_lines.append('\\t'.join(line))\n",
    "# edited_lines = '\\n'.join(edited_lines)\n",
    "# with open('/mnt/d/orchards/new_hk_edits.txt', 'w') as f:\n",
    "#     f.writelines(edited_lines)\n",
    "\n",
    "# edited_lines=list()\n",
    "# for line in lines.split('\\n'):\n",
    "#     line = line.split('\\t')\n",
    "#     line[0] = 'A_Singapore_INFIMH-16-0019_2016_HA'\n",
    "#     if line[2] == 'CDS':\n",
    "#         line[3] = str(int(line[3])-3)\n",
    "#         line[4] = str(int(line[4])-3)\n",
    "#     edited_lines.append('\\t'.join(line))\n",
    "# edited_lines = '\\n'.join(edited_lines)\n",
    "# with open('/mnt/d/orchards/new_sing_edits.txt', 'w') as f:\n",
    "#     f.writelines(edited_lines)\n",
    "# edited_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array(list(antigenic_sites))\n",
    "# x.sort()\n",
    "# print(x*3+18)\n",
    "# # print((x+1)*3+18)\n",
    "# # print(np.array([87, 129, 177, 444]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove SNPs taken from samples that are likely mixed infections\n",
    "SNPs = SNPs.loc[~SNPs.sampleID.isin(potentialmixed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "origNumSNPs = len(SNPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add sample metadata to each SNP\n",
    "# SNPs = SNPs.join(samples.set_index('sampleID')['decimalDate'], on='sampleID').reset_index(drop=True)\n",
    "#Including subtype just causes headaches downstream, SNPs will get subtype info from the samples dataframe\n",
    "SNPs = SNPs.drop(columns=['subtype'])\n",
    "SNPs = SNPs.merge(samples, on='sampleID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(SNPs) == origNumSNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNPs['inGenePos'] = SNPs['inGenePos'].replace(r'^\\s*$', np.nan, regex=True).astype(float)\n",
    "# SNPs['codon'] = ((SNPs.inGenePos-SNPs.inGenePos%3)/3)\n",
    "# SNPs = SNPs.replace('H1N1','H1N1pdm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(SNPs) == origNumSNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few dozen SNPs show an odd bug where freq is not equal to AD/(AD+RD).\n",
    "#A small divergence would be fine, but this can be several percentage points.\n",
    "#The bug is due to varscan only taking read quality into account for RD, not AD. \n",
    "#I think the easiest thing to do is to simply recalculate frequency.\n",
    "SNPs['FREQ'] = np.round(SNPs.AD/(SNPs.AD+SNPs.RD),4)\n",
    "\n",
    "#This is a very handy stat for downstream analysis\n",
    "SNPs['transformed_frequency'] = .50-abs(SNPs.FREQ-.50)\n",
    "SNPs['log_transformed_frequency'] = np.log10(SNPs['transformed_frequency'].replace(0,np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate global average and maximum frequencies of each SNP\n",
    "historicalFreqs = {GlobalH1N1DNAfreqsFile:('DNA','H1N1'),\n",
    "                   GlobalH3N2DNAfreqsFile:('DNA','H3N2'),\n",
    "                   GlobalFluBDNAfreqsFile:('DNA','Influenza B'),\n",
    "                   GlobalH1N1AAfreqsFile:('AA','H1N1'),\n",
    "                   GlobalH3N2AAfreqsFile:('AA','H3N2'),\n",
    "                   GlobalFluBAAfreqsFile:('AA','Influenza B')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sampleID', 'chrom', 'pos', 'id', 'ref', 'alt', 'qual', 'GT', 'GQ',\n",
       "       'SDP', 'DP', 'RD', 'AD', 'FREQ', 'PVAL', 'RBQ', 'ABQ', 'RDF', 'RDR',\n",
       "       'ADF', 'ADR', 'gene', 'inGenePos', 'refAA', 'codon', 'altAA', 'AAstr',\n",
       "       'AAtype', 'referenceFile', 'shortChrom', 'ptID', 'subtype', 'CT',\n",
       "       'mapped', 'sample_date', 'decimalDate', 'seasonalDate', 'week',\n",
       "       'season', 'day0_or_day7_sample', 'household', 'participant_ID', 'age',\n",
       "       'gender', 'time_of_symptom_onset', 'recieved_flu_vaccine', 'school',\n",
       "       'jackson_score', 'antipyretic_use', 'flu_antiviral_treatment',\n",
       "       'school_absence', 'number_in_household', 'recent_travel', 'isIndexCase',\n",
       "       '%ofHHinfected', 'age_category', 'days_since_symptom_onset',\n",
       "       'symptom_severity', 'saw_a_doctor', 'taken_on_hh_infection_day',\n",
       "       'clade', 'subclade', 'transformed_frequency',\n",
       "       'log_transformed_frequency'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNPs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sampleID', 'chrom', 'pos', 'id', 'ref', 'alt', 'qual', 'GT', 'GQ',\n",
       "       'SDP', 'DP', 'RD', 'AD', 'FREQ', 'PVAL', 'RBQ', 'ABQ', 'RDF', 'RDR',\n",
       "       'ADF', 'ADR', 'gene', 'inGenePos', 'refAA', 'codon', 'altAA', 'AAstr',\n",
       "       'AAtype', 'referenceFile', 'shortChrom', 'ptID', 'subtype', 'CT',\n",
       "       'mapped', 'sample_date', 'decimalDate', 'seasonalDate', 'week',\n",
       "       'season', 'day0_or_day7_sample', 'household', 'participant_ID', 'age',\n",
       "       'gender', 'time_of_symptom_onset', 'recieved_flu_vaccine', 'school',\n",
       "       'jackson_score', 'antipyretic_use', 'flu_antiviral_treatment',\n",
       "       'school_absence', 'number_in_household', 'recent_travel', 'isIndexCase',\n",
       "       '%ofHHinfected', 'age_category', 'days_since_symptom_onset',\n",
       "       'symptom_severity', 'saw_a_doctor', 'taken_on_hh_infection_day',\n",
       "       'clade', 'subclade', 'transformed_frequency',\n",
       "       'log_transformed_frequency'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNPs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVariableMutationSites(jsonFile,cutoff=0.02, cutoffyear = 2009):\n",
    "    with open(jsonFile, 'r') as freqFile:\n",
    "        frequencies = json.load(freqFile)\n",
    "    dates = frequencies.pop('pivots')\n",
    "    offKeys = [key for key in frequencies.keys() if len(frequencies[key]) != len(dates)]\n",
    "    if len(offKeys) > 0:\n",
    "        print (f'offKey length is {len(offKeys)}')\n",
    "        offVals = [frequencies.pop(key) for key in offKeys]\n",
    "    freqDF = pd.DataFrame(frequencies, index=dates)\n",
    "    freqDF = freqDF.loc[freqDF.index > cutoffyear]\n",
    "    freqDF = freqDF.unstack().reset_index().rename(columns={'level_0':'mutkey','level_1':'month',0:'freq'})\n",
    "    return freqDF.loc[(freqDF.freq>cutoff)&(freqDF.freq<(1-cutoff)),'mutkey'].str[:-1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gene_to_seg_dict={'HA':'HA','NA':'NA','PB1':'PB1','PB2':'PB2','PA':'PA','NP':'NP',\n",
    "                 'NEP':'NS', 'NS1':'NS','M1':'MP','M2':'MP','PB1-F2':'PB1','PA-X':'PA','NB':'NA','BM2':'MP'}\n",
    "\n",
    "\n",
    "# variableSites = pd.DataFrame()\n",
    "# for key, item in tqdm(historicalFreqs.items()):\n",
    "#     acid, subtype = item\n",
    "#     df = pd.DataFrame(getVariableMutationSites(key,cutoffyear=2014))\n",
    "#     df['subtype'] = subtype\n",
    "#     df['acidType'] = acid\n",
    "#     df[['gene','pos']] = df[0].str.split(':',expand=True)\n",
    "#     variableSites = variableSites.append(df)\n",
    "# variableSites['segment'] = variableSites.gene.map(gene_to_seg_dict)\n",
    "# variableSites = variableSites.groupby(['segment','pos']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = variableSites.loc[(variableSites.gene.isin(['HA_antigenic','HA_nonantigenic']))]\n",
    "# df['gene'] = 'HA'\n",
    "# variableSites = variableSites.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNPs.loc[(SNPs.gene=='HA')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "genelengths={'H3N2': {'NEP': 366,\n",
    "  'HA': 1701,\n",
    "  'HA_antigenic':len(antigenic_sites)*3,\n",
    "  'HA_nonantigenic': 1701-len(antigenic_sites)*3,\n",
    "  'M1': 759,\n",
    "  'M2': 294,\n",
    "  'NA': 1410,\n",
    "  'NP': 1497,\n",
    "  'NS1': 693,\n",
    "  'PA': 2151,\n",
    "  'PA-X': 759,\n",
    "  'PB1': 2274,\n",
    "  'PB1-F2': 273,\n",
    "  'PB2': 2280},\n",
    " 'H1N1': {'HA_antigenic':len(H1N1_antigenic_sites)*3,\n",
    "  'HA_nonantigenic':1701-len(H1N1_antigenic_sites)*3,\n",
    "  'HA': 1701,\n",
    "  'M1': 759,\n",
    "  'M2': 294,\n",
    "  'NA': 1410,\n",
    "  'NP': 1497,\n",
    "  'NEP': 366,\n",
    "  'NS1': 660,\n",
    "  'PA': 2151,\n",
    "  'PA-X': 699,\n",
    "  'PB1': 2274,\n",
    "  'PB1-F2': 273,\n",
    "  'PB2': 2280},\n",
    " 'Influenza B': {'HA': 1755,\n",
    "  'M1': 747,\n",
    "  'NA': 1401,\n",
    "  'NP': 1683,\n",
    "  'NEP': 369,\n",
    "  'NS1': 846,\n",
    "  'PA': 2181,\n",
    "  'PB1': 2259,\n",
    "  'PB2': 2313,\n",
    "  'BM2': 330,\n",
    "  'NB': 303}}\n",
    "\n",
    "genelengths['H1N1pdm'] = genelengths['H1N1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding historical DNA frequencies for H1N1 samples\n",
      "json loaded...\n",
      "dataframe made...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 1/6 [00:20<01:42, 20.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding historical DNA frequencies for H3N2 samples\n",
      "json loaded...\n",
      "dataframe made...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:36<01:16, 19.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding historical DNA frequencies for Influenza B samples\n",
      "json loaded...\n",
      "dataframe made...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:42<00:45, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding historical AA frequencies for H1N1 samples\n",
      "json loaded...\n",
      "offKey length is 644\n",
      "dataframe made...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:49<00:25, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding historical AA frequencies for H3N2 samples\n",
      "json loaded...\n",
      "offKey length is 1\n",
      "dataframe made...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:51<00:09,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding historical AA frequencies for Influenza B samples\n",
      "json loaded...\n",
      "dataframe made...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:53<00:00,  8.92s/it]\n"
     ]
    }
   ],
   "source": [
    "rollingFrames = {'DNA':pd.DataFrame(), 'AA':pd.DataFrame()}\n",
    "\n",
    "#Calculate DNA/AA specific mean/max info\n",
    "for key, item in tqdm(historicalFreqs.items()):\n",
    "    genetic_code, subtype = item\n",
    "    print (f'adding historical {genetic_code} frequencies for {subtype} samples')\n",
    "    with open(key, 'r') as freqFile:\n",
    "        frequencies = json.load(freqFile)\n",
    "    print ('json loaded...')\n",
    "    dates = frequencies.pop('pivots')\n",
    "    offKeys = [key for key in frequencies.keys() if len(frequencies[key]) != len(dates)]\n",
    "    if len(offKeys) > 0:\n",
    "        print (f'offKey length is {len(offKeys)}')\n",
    "        offVals = [frequencies.pop(key) for key in offKeys]\n",
    "    \n",
    "    freqDF = pd.DataFrame(frequencies, index=dates)\n",
    "    print ('dataframe made...')\n",
    "    freqDF = freqDF.loc[freqDF.index > 2012]\n",
    "    rollingMean = freqDF.rolling(window=12, center=True).mean().unstack()\n",
    "    rollingPeak = freqDF.rolling(window=12, center=True).max().unstack()\n",
    "    rollingMean.name = (f'rolling_{genetic_code}_mean_global_freq')\n",
    "    rollingPeak.name = (f'rolling_{genetic_code}_max_global_freq')\n",
    "    \n",
    "    rolling = rollingMean.reset_index().merge(rollingPeak.reset_index(), on=['level_0','level_1'])\n",
    "    \n",
    "    \n",
    "    rolling = rolling.rename(columns={'level_0':f'Global{genetic_code}FreqKey','level_1':'decimalDate'})\n",
    "    rolling[f'Global{genetic_code}FreqKey'] = subtype+':'+rolling[f'Global{genetic_code}FreqKey']\n",
    "                                                                  \n",
    "    rollingFrames[genetic_code] = rollingFrames[genetic_code].append(rolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNPs = SNPs.rename(columns={'product':'gene'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort and merge mean/max freq info into SNP data, merging on closest date\n",
    "rollingFrames['AA'] = rollingFrames['AA'].sort_values('decimalDate').dropna(subset=['rolling_AA_mean_global_freq','rolling_AA_max_global_freq'])\n",
    "rollingFrames['DNA'] = rollingFrames['DNA'].sort_values('decimalDate').dropna(subset=['rolling_DNA_mean_global_freq','rolling_DNA_max_global_freq'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNPs['GlobalAAFreqKey'] = SNPs.subtype+':'+SNPs.gene+':'+SNPs.codon.astype(str)+SNPs.altAA\n",
    "SNPs['GlobalDNAFreqKey'] = SNPs.subtype+':'+SNPs.shortChrom+':'+SNPs.pos.astype(str)+SNPs.alt\n",
    "# SNPs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SNPs = SNPs.reset_index(drop=True).dropna(subset=['decimalDate']).sort_values('decimalDate')\n",
    "\n",
    "SNPs = pd.merge_asof(SNPs, rollingFrames['AA'], on='decimalDate', by='GlobalAAFreqKey')\n",
    "SNPs = pd.merge_asof(SNPs, rollingFrames['DNA'], on='decimalDate', by='GlobalDNAFreqKey')\n",
    "\n",
    "#Do the same to figure out what the following year's data is:\n",
    "SNPs['decimalDateFollowingYear'] = SNPs.decimalDate+1\n",
    "SNPs = SNPs.reset_index(drop=True).sort_values('decimalDateFollowingYear')\n",
    "SNPs = pd.merge_asof(SNPs,rollingFrames['AA'], left_on='decimalDateFollowingYear',right_on='decimalDate',by='GlobalAAFreqKey',suffixes=('','_following_year'))\n",
    "SNPs = pd.merge_asof(SNPs,rollingFrames['DNA'], left_on='decimalDateFollowingYear',right_on='decimalDate',by='GlobalDNAFreqKey',suffixes=('','_following_year'))\n",
    "\n",
    "#Create true/false columns to assess whether we even globally observe the SNP this season/next season\n",
    "SNPs['AAobservedGlobally'] = pd.notna(SNPs.rolling_AA_mean_global_freq)&(SNPs.AAtype=='Nonsynonymous')\n",
    "SNPs['AAobservedGloballyFollowingSeason'] = pd.notna(SNPs.rolling_AA_mean_global_freq_following_year) & (SNPs.AAtype=='Nonsynonymous')\n",
    "SNPs['DNAobservedGlobally'] = pd.notna(SNPs.rolling_DNA_mean_global_freq)\n",
    "SNPs['DNAobservedGloballyFollowingSeason'] = pd.notna(SNPs.rolling_DNA_mean_global_freq_following_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846 2994 3839\n"
     ]
    }
   ],
   "source": [
    "print(len(SNPs.loc[SNPs.gene=='HA_antigenic']), len(SNPs.loc[SNPs.gene=='HA_nonantigenic']), len(SNPs.loc[(SNPs.subtype != 'Influenza B')&(SNPs.gene=='HA')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3840"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2994+846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26901"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origNumSNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26901"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SNPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(SNPs) == origNumSNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNPs['antigenic_site'] = None\n",
    "SNPs.loc[(SNPs['gene']=='HA')&(SNPs.subtype=='H3N2'), 'antigenic_site'] = 'No'\n",
    "SNPs.loc[(SNPs['gene']=='HA')&(SNPs.subtype=='H3N2')&(SNPs.codon.isin(antigenic_sites)), 'antigenic_site'] = 'Yes'\n",
    "\n",
    "SNPs['antigenic_product'] = SNPs.gene #Now that antigenic sites are in the GTF file, this takes care of itself in annotation\n",
    "# SNPs.loc[(SNPs['gene']=='HA')&(SNPs.subtype=='H3N2'), 'antigenic_product'] = 'HA_nonantigenic'\n",
    "# SNPs.loc[(SNPs['gene']=='HA')&(SNPs.subtype=='H3N2') & (SNPs.codon.isin(antigenic_sites)), 'antigenic_product'] = 'HA_antigenic'\n",
    "\n",
    "\n",
    "SNPs.loc[(SNPs['gene']=='HA')&(SNPs.subtype=='H1N1'), 'antigenic_site'] = 'No'\n",
    "SNPs.loc[(SNPs['gene']=='HA')&(SNPs.subtype=='H1N1')&(SNPs.codon.isin(H1N1_antigenic_sites)), 'antigenic_site'] = 'Yes'\n",
    "\n",
    "# SNPs.loc[(SNPs['gene']=='HA')&(SNPs.subtype=='H1N1'), 'antigenic_product'] = 'HA_nonantigenic'\n",
    "# SNPs.loc[(SNPs['gene']=='HA')&(SNPs.subtype=='H1N1') & (SNPs.codon.isin(H1N1_antigenic_sites)), 'antigenic_product'] = 'HA_antigenic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sampleID', 'chrom', 'pos', 'id', 'ref', 'alt', 'qual', 'GT', 'GQ',\n",
      "       'SDP', 'DP', 'RD', 'AD', 'FREQ', 'PVAL', 'RBQ', 'ABQ', 'RDF', 'RDR',\n",
      "       'ADF', 'ADR', 'gene', 'inGenePos', 'refAA', 'codon', 'altAA', 'AAstr',\n",
      "       'AAtype', 'referenceFile', 'shortChrom', 'ptID', 'subtype', 'CT',\n",
      "       'mapped', 'sample_date', 'decimalDate', 'seasonalDate', 'week',\n",
      "       'season', 'day0_or_day7_sample', 'household', 'participant_ID', 'age',\n",
      "       'gender', 'time_of_symptom_onset', 'recieved_flu_vaccine', 'school',\n",
      "       'jackson_score', 'antipyretic_use', 'flu_antiviral_treatment',\n",
      "       'school_absence', 'number_in_household', 'recent_travel', 'isIndexCase',\n",
      "       '%ofHHinfected', 'age_category', 'days_since_symptom_onset',\n",
      "       'symptom_severity', 'saw_a_doctor', 'taken_on_hh_infection_day',\n",
      "       'clade', 'subclade', 'transformed_frequency',\n",
      "       'log_transformed_frequency', 'GlobalAAFreqKey', 'GlobalDNAFreqKey',\n",
      "       'rolling_AA_mean_global_freq', 'rolling_AA_max_global_freq',\n",
      "       'rolling_DNA_mean_global_freq', 'rolling_DNA_max_global_freq',\n",
      "       'decimalDateFollowingYear', 'decimalDate_following_year',\n",
      "       'rolling_AA_mean_global_freq_following_year',\n",
      "       'rolling_AA_max_global_freq_following_year',\n",
      "       'decimalDate_following_year',\n",
      "       'rolling_DNA_mean_global_freq_following_year',\n",
      "       'rolling_DNA_max_global_freq_following_year', 'AAobservedGlobally',\n",
      "       'AAobservedGloballyFollowingSeason', 'DNAobservedGlobally',\n",
      "       'DNAobservedGloballyFollowingSeason', 'antigenic_site',\n",
      "       'antigenic_product'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(SNPs.columns)\n",
    "#Clear out unnecessary columns, delete the big rollingFrames item which is taking up a lot of memory\n",
    "SNPs=SNPs.drop(['GlobalAAFreqKey','GlobalDNAFreqKey','decimalDateFollowingYear','decimalDate_following_year'], axis=1)\n",
    "del rollingFrames\n",
    "assert len(SNPs) == origNumSNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['H3N2', 'H1N1pdm'], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNPs.loc[(SNPs.transformed_frequency > SNP_frequency_cutoff)&SNPs.antigenic_product.isin(['HA_antigenic','HA_nonantigenic'])].subtype.unique()#.groupby(['sampleID','antigenic_product']).count()[['ABQ','subtype']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convertListofClassicH3N2SitestoZeroIndexedMStart(listOfSites):\n",
    "#     return [site+15 for site in listOfSites]\n",
    "# #Classify HA sites as antigenic or not\n",
    "# #Wolf et al 2006 'classical' antigenic sites\n",
    "# #Note: starts at the first methionine, not with the classic start codon\n",
    "# #Note: Wolf has an extra - at the beginning due to an insertion in one of their aligned sequences.\n",
    "# anti_sites = '-----------------------------------------------------------CCCCC-CC-CC--E-E--EE---E-------E--E-EEEE--EEE--EE-E-D-----DD-----E-------D---DA-A-A-BBAAAA-A-AA-A-AAAAA---A-A--BBBBB---B-B-DA-DDDDDDDD-D--D---BBBBB-BBB-BBB--D-D---DDD--DDDDDDDD------DDDDD-------D-D-D-D-DDD-----------EEE--E-------C-CC-CCC-------------C--C-CC---CC-CCCCCC----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------'\n",
    "# antigenic_sites = set()\n",
    "# for i, site in enumerate(anti_sites):\n",
    "#     if site != '-':\n",
    "#         antigenic_sites.add(i)\n",
    "\n",
    "# #antigenic sites, taken from tavera and deem, converted to 0-indexed starting with methinine format\n",
    "# antigenic_sites2 = {59, 60, 61, 62, 63, 65, 66, 68, 69, 72, 74, 77, 78, 82, 90, 93, 95, 96, 97, 98, 101, 102, 103, 106, 107, 109, 111, 117, 118, 124, 132, 136, 137, 139, 141, 143, 144, 145, 146, 147, 148, 150, 152, 153, 155, 157, 158, 159, 160, 161, 165, 167, 170, 171, 172, 173, 174, 178, 180, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 194, 197, 201, 202, 203, 204, 205, 207, 208, 209, 211, 212, 213, 216, 218, 222, 223, 224, 227, 228, 229, 230, 231, 232, 233, 234, 241, 242, 243, 244, 245, 253, 255, 257, 259, 261, 262, 263, 275, 276, 277, 280, 288, 290, 291, 293, 294, 295, 309, 312, 314, 315, 319, 320, 322, 323, 324, 325, 326, 327}\n",
    "# assert antigenic_sites == antigenic_sites2\n",
    "# print(len(antigenic_sites)*3)\n",
    "# print((len(anti_sites)-len(antigenic_sites))*3)\n",
    "# print (len(anti_sites)*3)\n",
    "# #antigenic_sites is a zero-indexed list of sites, starting with methionine\n",
    "# glycosylation_sites = set(convertListofClassicH3N2SitestoZeroIndexedMStart([8,22,38,45,63,81,133,126,159,160,165,246,285]))\n",
    "# antigenic_ridge = set(convertListofClassicH3N2SitestoZeroIndexedMStart([122,127,133,135,145,152,153,155,156,158,159,160,189,193,224]))\n",
    "# #converting above to 0-indexed starting with methionine format:\n",
    "\n",
    "\n",
    "# antigenic_sites = antigenic_sites.union(glycosylation_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downstream merging is complicated by columns that could be numeric being classified as object columns. \n",
    "#This code converts those columns to numeric.\n",
    "objcols = SNPs.select_dtypes(include='object').columns\n",
    "SNPs[objcols] = SNPs[objcols].select_dtypes(include='object').apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's also complicated by not having a unique SNP identifier. I will create one here.\n",
    "SNPs['snpKey'] = SNPs.sampleID+':'+SNPs.shortChrom+':'+SNPs.pos.astype(str)+':'+SNPs.alt + ':' + SNPs['gene'].fillna('OORF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNPs = SNPs.drop(columns=['mapped','id','ptID','day0_or_day7_sample']).rename(columns={\n",
    "    'alt':'alt_nuc', 'ref':'ref_nuc', 'shortChrom':'segment','DP':'depth','FREQ':'SNP_frequency',\n",
    "    'gene':'product'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(SNPs) == origNumSNPs\n",
    "assert len(SNPs.snpKey.unique())==len(SNPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_SNPs(df):\n",
    "    df = df.loc[~df['product'].isin(['PA-X','PB1-F2', 'NB'])]\n",
    "    if len(df.AAtype.unique()) == 1:\n",
    "        return df.iloc[0]\n",
    "    elif 'HA' in df['product'].values:\n",
    "        return df.iloc[0]\n",
    "    else:\n",
    "        df.AAtype = 'Mixed'\n",
    "        return df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now that all SNPs are processed, calculate number of valid SNPs per sample\n",
    "#First group SNPs by position in segment\n",
    "#Currently SNPs are per-gene, since you can have a SNP with different properties(e.g., synon/nonsynon) depending on which gene we're considering it in\n",
    "#'rawSNPs' will be a dataframe where each observation is a mutation on a segment, w/o regard to it's location in a coding region. \n",
    "# SNPs.replace('H1N1pdm','H1N1')\n",
    "rawSNPs = SNPs.copy().set_index('sampleID')\n",
    "rawSNPs = rawSNPs.sort_values(by=['sampleID','segment','product','pos'])\n",
    "rawSNPs = rawSNPs.groupby(['sampleID', 'segment','pos']).first().reset_index()\n",
    "rawSNPs_over_cutoff = (rawSNPs.SNP_frequency > SNP_frequency_cutoff) & (rawSNPs.SNP_frequency < (1-SNP_frequency_cutoff))\n",
    "rawSNPs = rawSNPs.loc[rawSNPs_over_cutoff]\n",
    "\n",
    "NSrawSNPs = SNPs.copy().set_index(['sampleID','segment','pos'])\n",
    "synonSNPs_over_cutoff = (NSrawSNPs.SNP_frequency > SNP_frequency_cutoff) & (NSrawSNPs.SNP_frequency < (1-SNP_frequency_cutoff))\n",
    "NSrawSNPs = NSrawSNPs.loc[synonSNPs_over_cutoff]\n",
    "NSrawSNPs = NSrawSNPs.groupby(['sampleID','segment','pos']).apply(reduce_SNPs).reset_index()\n",
    "\n",
    "numofSNPs = rawSNPs.groupby('sampleID').count()\n",
    "numofSNPs = numofSNPs['ABQ'].reset_index().rename(columns={'ABQ':'num_of_snps'}).replace(np.nan,0)\n",
    "\n",
    "numofSynonSNPs = NSrawSNPs.groupby(['sampleID', 'AAtype']).count().reset_index()#[['sampleID','AAtype','ABQ']]\n",
    "\n",
    "numofSynonSNPs = numofSynonSNPs.pivot(index=['sampleID'], columns='AAtype')['ABQ'].reset_index()\n",
    "numofSynonSNPs = numofSynonSNPs.replace(np.nan, 0).rename(columns={'Nonsynonymous':'num_of_nonsynon_muts',\n",
    "                                                                   'Synonymous':'num_of_synon_muts',\n",
    "                                                                   'Out of reading frame':'num_of_oof_muts',\n",
    "                                                                   'Mixed':'num_of_mixed_muts'})#.reset_index()\n",
    "\n",
    "divergence = rawSNPs.groupby('sampleID').sum()['transformed_frequency']\n",
    "divergence = divergence.reset_index().rename(columns={'transformed_frequency':'Xue_divergence'}).replace(np.nan,0)\n",
    "\n",
    "synonDivergence = NSrawSNPs.groupby(['sampleID', 'AAtype']).sum()['transformed_frequency'].reset_index()\n",
    "synonDivergence = synonDivergence.pivot_table(index=['sampleID'], columns='AAtype')['transformed_frequency']\n",
    "synonDivergence = synonDivergence.replace(np.nan, 0).rename(columns={'Nonsynonymous':'Xue_nonsynon_divergence',\n",
    "                                                                     'Synonymous':'Xue_synon_divergence',\n",
    "                                                                     'Out of reading frame':'Xue_oof_divergence',\n",
    "                                                                     'Mixed':'Xue_mixed_divergence'}).reset_index()\n",
    "\n",
    "samples = samples.merge(numofSNPs, on='sampleID', how='left')\n",
    "samples = samples.merge(numofSynonSNPs, on='sampleID',how='left')\n",
    "samples = samples.merge(divergence, on='sampleID',how='left')\n",
    "samples = samples.merge(synonDivergence, on='sampleID',how='left')\n",
    "\n",
    "merged_columns = ['num_of_snps', 'num_of_nonsynon_muts', 'num_of_synon_muts', 'num_of_oof_muts', 'num_of_mixed_muts',\n",
    "                  'Xue_divergence', 'Xue_nonsynon_divergence', 'Xue_synon_divergence', 'Xue_oof_divergence', 'Xue_mixed_divergence']\n",
    "\n",
    "samples[merged_columns] = samples[merged_columns].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segfolder=SnpGenieSegFolders[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob(segfolder+'/*')\n",
    "# x = pd.read_csv(os.path.join(segfolder, \"codon_results.txt\"), **read_tsv_args)\n",
    "# x.loc[(x.file=='temp_vcf4_19VR009188.vcf') & (2.65>x.N_sites) &(x.N_sites>2.34)]\n",
    "# x.groupby('file').sum()['N_sites'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016/SNPGenie_bootstrapped_filter/SNPGenie_output/A_Singapore_INFIMH-16-0019_2016_HA/SNPGenie_Results/codon_results.txt',\n",
       " '/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016/SNPGenie_bootstrapped_filter/SNPGenie_output/A_Singapore_INFIMH-16-0019_2016_HA/SNPGenie_Results/population_summary.txt',\n",
       " '/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016/SNPGenie_bootstrapped_filter/SNPGenie_output/A_Singapore_INFIMH-16-0019_2016_HA/SNPGenie_Results/product_results.txt',\n",
       " '/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016/SNPGenie_bootstrapped_filter/SNPGenie_output/A_Singapore_INFIMH-16-0019_2016_HA/SNPGenie_Results/site_results.txt',\n",
       " '/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016/SNPGenie_bootstrapped_filter/SNPGenie_output/A_Singapore_INFIMH-16-0019_2016_HA/SNPGenie_Results/sliding_window_length50_results.txt',\n",
       " '/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016/SNPGenie_bootstrapped_filter/SNPGenie_output/A_Singapore_INFIMH-16-0019_2016_HA/SNPGenie_Results/SNPGenie_LOG.txt',\n",
       " '/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016/SNPGenie_bootstrapped_filter/SNPGenie_output/A_Singapore_INFIMH-16-0019_2016_HA/SNPGenie_Results/SNPGenie_parameters.txt']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Singapore_INFIMH-16-0019_2016/SNPGenie_bootstrapped_filter/SNPGenie_output/A_Singapore_INFIMH-16-0019_2016_HA/SNPGenie_Results/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ogSegFolders = SnpGenieSegFolders\n",
    "# SnpGenieSegFolders = ogSegFolders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not useSNPGenie2:\n",
    "    #Load SNPGenie1 diversity statistics\n",
    "    print ('loading nucleotide diversity statistics...')\n",
    "\n",
    "    popDiversity = pd.DataFrame()\n",
    "    geneDiversity = pd.DataFrame()\n",
    "    siteDiversity = pd.DataFrame()\n",
    "    segfolder_index = -1\n",
    "    if SnpGenieSegFolders[0]+'/population_summary.txt' not in glob.glob(SnpGenieSegFolders[0]+'/*.txt'):\n",
    "        SnpGenieSegFolders = [f+'/SNPGenie_Results' for f in SnpGenieSegFolders]\n",
    "        segfolder_index = -2\n",
    "\n",
    "    for segfolder in tqdm(SnpGenieSegFolders):\n",
    "        try:\n",
    "            tmppopDiversity = pd.read_csv(os.path.join(segfolder, \"population_summary.txt\"), **read_tsv_args)\n",
    "        except:\n",
    "            continue\n",
    "        tmpgeneDiversity = pd.read_csv(os.path.join(segfolder, \"product_results.txt\"), **read_tsv_args)\n",
    "        tmpsiteDiversity = pd.read_csv(os.path.join(segfolder, \"site_results.txt\"), **read_tsv_args)\n",
    "        \n",
    "        segment = segfolder.split('/')[segfolder_index].split(\"_\")[-1]\n",
    "        tmppopDiversity['segment'] = segment\n",
    "        tmpgeneDiversity['segment'] = segment\n",
    "        tmpsiteDiversity['segment'] = segment\n",
    "\n",
    "        tmppopDiversity['sampleID'] = tmppopDiversity.file.str.split('_').str[-1].str.split('.').str[0]\n",
    "        tmpgeneDiversity['sampleID'] = tmpgeneDiversity.file.str.split('_').str[-1].str.split('.').str[0]\n",
    "        tmpsiteDiversity['sampleID'] = tmpsiteDiversity.file.str.split('_').str[-1].str.split('.').str[0]\n",
    "        \n",
    "        popDiversity = popDiversity.append(tmppopDiversity, ignore_index=True).reset_index(drop=True)\n",
    "        geneDiversity = geneDiversity.append(tmpgeneDiversity, ignore_index=True).reset_index(drop=True)\n",
    "        siteDiversity = siteDiversity.append(tmpsiteDiversity, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    popDiversity = popDiversity.replace(r'^\\*$', np.nan, regex=True)\n",
    "    geneDiversity = geneDiversity.replace(r'^\\*$', np.nan, regex=True)\n",
    "    siteDiversity = siteDiversity.replace(r'^\\*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not useSNPGenie2:\n",
    "    popDiversity.loc[popDiversity.segment=='HA',['N_sites','S_sites']] = geneDiversity.loc[geneDiversity['product']=='HA', ['N_sites','S_sites']].values\n",
    "    popDiversity.loc[popDiversity.segment=='NS',['N_sites','S_sites']] = geneDiversity.loc[geneDiversity['product']=='NS1', ['N_sites','S_sites']]\n",
    "    popDiversity.loc[popDiversity.segment=='MP',['N_sites','S_sites']] = geneDiversity.loc[geneDiversity['product']=='M1', ['N_sites','S_sites']]\n",
    "    popDiversity.loc[popDiversity.segment=='PB1',['N_sites','S_sites']] = geneDiversity.loc[geneDiversity['product']=='PB1', ['N_sites','S_sites']]\n",
    "    popDiversity.loc[popDiversity.segment=='PA',['N_sites','S_sites']] = geneDiversity.loc[geneDiversity['product']=='PA', ['N_sites','S_sites']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popDiversity.groupby(['sampleID']).sum()[['N_sites', 'S_sites']]\n",
    "# popDiversity.set_index('sampleID').loc['17VR013816'][['segment','N_sites','S_sites', 'piN', 'piS']]\n",
    "# popDiversity.set_index('sampleID').loc['17VR013816'][['segment','N_sites','S_sites', 'piN', 'piS']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, df in popDiversity.loc[popDiversity.segment=='MP'].groupby('sampleID'):\n",
    "#     print(i)\n",
    "#     print(df)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n",
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n",
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n",
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n",
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n",
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n",
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n",
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n",
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n",
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n",
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n",
      "/mnt/d/snpgenie2/SNPGenie2.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(piCalcs[:,:,1:],axis=2)/((piCalcs[:,:,0]**2-piCalcs[:,:,0])/2)\n"
     ]
    }
   ],
   "source": [
    "if useSNPGenie2:\n",
    "    sys.path.append('/mnt/d/snpgenie2')\n",
    "    import importlib\n",
    "    import SNPGenie2\n",
    "    importlib.reload(SNPGenie2)\n",
    "    from SNPGenie2 import *\n",
    "\n",
    "#Generate diversity stats\n",
    "# SNPs.referenceFile.unique()\n",
    "if useSNPGenie2:\n",
    "    popDiversity = pd.DataFrame()\n",
    "    segDiversity = pd.DataFrame()\n",
    "    geneDiversity = pd.DataFrame()\n",
    "    siteDiversity = pd.DataFrame()\n",
    "    for ref, df in SNPs.groupby('referenceFile'):\n",
    "        df = df.rename(columns={'sampleID':'sample'})\n",
    "        perSamplePiDF, perSegmentPiDF, genePis, perSitePiDF = calcPi(df, gtf=gtfFileDict[df.subtype.iloc[0]][df.season.iloc[0]], refseq=ref)\n",
    "        popDiversity = popDiversity.append(perSamplePiDF)\n",
    "        segDiversity = segDiversity.append(perSegmentPiDF)\n",
    "        geneDiversity = geneDiversity.append(genePis)\n",
    "        siteDiversity = siteDiversity.append(perSitePiDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "42\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for ref, df in SNPs.groupby('referenceFile'):\n",
    "    print(len(df.loc[df.sampleID=='18VR002212']))\n",
    "# popDiversity.loc['18VR002212']\n",
    "# sorted(geneDiversity.loc[(geneDiversity['product']=='HA')&(geneDiversity.type=='piN'), 'sites'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# geneDiversity.loc[geneDiversity['product']=='HA'].mean()\n",
    "# print(genePis.loc[genePis.sampleID=='18VR001131'])\n",
    "# print(popDiversity.loc[popDiversity.sampleID == '18VR001131'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove mixed infections\n",
    "\n",
    "popDiversity = popDiversity.reset_index()\n",
    "popDiversity = popDiversity.rename(columns={0:'pi_sample'})\n",
    "\n",
    "if useSNPGenie2:\n",
    "    popDiversity = popDiversity.rename(columns={'index':'sampleID'})\n",
    "    segDiversity = segDiversity.loc[~segDiversity['sampleID'].isin(potentialmixed)].rename(columns={'pi_seg':'pi_segment'})\n",
    "    segDiversity['piN_segment'] = 0 # I ended up not needing these stats, but future code\n",
    "    segDiversity['piS_segment'] = 0 # assumes these exist, so I'll just fill them in\n",
    "popDiversity = popDiversity.loc[~popDiversity['sampleID'].isin(potentialmixed)]\n",
    "geneDiversity = geneDiversity.loc[~geneDiversity['sampleID'].isin(potentialmixed)]\n",
    "siteDiversity = siteDiversity.loc[~siteDiversity['sampleID'].isin(potentialmixed)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not useSNPGenie2:\n",
    "#     # Prepare for calculating overall pi statistic by weighted mean of segmental pi\n",
    "#     seglengths = {'H3N2':(2316,2316,2209,1737,1541,1441,1002,865),'H1N1':(2316,2317,2208,1753,1540,1433,1002,865),'H1N1pdm':(2316,2317,2208,1753,1540,1433,1002,865),'Influenza B':(2367,2340,2275,1853,1815,1530,1155,1062)}\n",
    "#     segnames = ('PB2','PB1','NP','HA','NA','PA','MP','NS')\n",
    "\n",
    "#     weightdict = {subtype:{name:weight/sum(seglengths[subtype]) for name, weight in zip(segnames,seglengths[subtype])} for subtype in seglengths.keys()}\n",
    "\n",
    "#     def weightavg(group, idcolumn, values, weights):\n",
    "#         subtype = group.subtype.iloc[0]\n",
    "#         return sum([val * weights[subtype][idcolumnval] for idcolumnval, val in zip(group[idcolumn].values, group[values].values)])\n",
    "    \n",
    "#     popDiversity = popDiversity.rename(columns={'piN':'piN_segment','piS':'piS_segment','pi':'pi_segment'})\n",
    "#     popDiversity['pi_sample'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(geneDiversity.columns)\n",
    "# x = geneDiversity.pivot_table(index=['sampleID','product', 'segment'], columns='type', values=['pi', 'sites']).reset_index()\n",
    "# x.columns = ['sampleID','product', 'segment', 'pi','piN','piS','N_sites','S_sites']\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sampleID</th>\n",
       "      <th>product</th>\n",
       "      <th>segment</th>\n",
       "      <th>pi_gene</th>\n",
       "      <th>piN_gene</th>\n",
       "      <th>piS_gene</th>\n",
       "      <th>N_sites_gene</th>\n",
       "      <th>S_sites_gene</th>\n",
       "      <th>ptID</th>\n",
       "      <th>subtype</th>\n",
       "      <th>CT</th>\n",
       "      <th>mapped</th>\n",
       "      <th>sample_date</th>\n",
       "      <th>decimalDate</th>\n",
       "      <th>seasonalDate</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>day0_or_day7_sample</th>\n",
       "      <th>household</th>\n",
       "      <th>participant_ID</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>time_of_symptom_onset</th>\n",
       "      <th>recieved_flu_vaccine</th>\n",
       "      <th>school</th>\n",
       "      <th>jackson_score</th>\n",
       "      <th>antipyretic_use</th>\n",
       "      <th>flu_antiviral_treatment</th>\n",
       "      <th>school_absence</th>\n",
       "      <th>number_in_household</th>\n",
       "      <th>recent_travel</th>\n",
       "      <th>isIndexCase</th>\n",
       "      <th>%ofHHinfected</th>\n",
       "      <th>age_category</th>\n",
       "      <th>days_since_symptom_onset</th>\n",
       "      <th>symptom_severity</th>\n",
       "      <th>saw_a_doctor</th>\n",
       "      <th>taken_on_hh_infection_day</th>\n",
       "      <th>clade</th>\n",
       "      <th>subclade</th>\n",
       "      <th>num_of_snps</th>\n",
       "      <th>num_of_mixed_muts</th>\n",
       "      <th>num_of_nonsynon_muts</th>\n",
       "      <th>num_of_oof_muts</th>\n",
       "      <th>num_of_synon_muts</th>\n",
       "      <th>Xue_divergence</th>\n",
       "      <th>Xue_mixed_divergence</th>\n",
       "      <th>Xue_nonsynon_divergence</th>\n",
       "      <th>Xue_oof_divergence</th>\n",
       "      <th>Xue_synon_divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1437</td>\n",
       "      <td>18VR002752</td>\n",
       "      <td>BM2</td>\n",
       "      <td>MP</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>244.361986</td>\n",
       "      <td>82.638014</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>30.13</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018.046575</td>\n",
       "      <td>0.546575</td>\n",
       "      <td>3</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>1438</td>\n",
       "      <td>18VR002752</td>\n",
       "      <td>HA</td>\n",
       "      <td>HA</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>1233.813263</td>\n",
       "      <td>518.186737</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>30.13</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018.046575</td>\n",
       "      <td>0.546575</td>\n",
       "      <td>3</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>1439</td>\n",
       "      <td>18VR002752</td>\n",
       "      <td>M1</td>\n",
       "      <td>MP</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>521.691978</td>\n",
       "      <td>222.308022</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>30.13</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018.046575</td>\n",
       "      <td>0.546575</td>\n",
       "      <td>3</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>1440</td>\n",
       "      <td>18VR002752</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>982.549630</td>\n",
       "      <td>415.450370</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>30.13</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018.046575</td>\n",
       "      <td>0.546575</td>\n",
       "      <td>3</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>1441</td>\n",
       "      <td>18VR002752</td>\n",
       "      <td>NB</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>214.349602</td>\n",
       "      <td>85.650398</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>30.13</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018.046575</td>\n",
       "      <td>0.546575</td>\n",
       "      <td>3</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>1442</td>\n",
       "      <td>18VR002752</td>\n",
       "      <td>NEP</td>\n",
       "      <td>NS</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>238.571869</td>\n",
       "      <td>94.428131</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>30.13</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018.046575</td>\n",
       "      <td>0.546575</td>\n",
       "      <td>3</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>1443</td>\n",
       "      <td>18VR002752</td>\n",
       "      <td>NP</td>\n",
       "      <td>NP</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1197.622058</td>\n",
       "      <td>482.377942</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>30.13</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018.046575</td>\n",
       "      <td>0.546575</td>\n",
       "      <td>3</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>1444</td>\n",
       "      <td>18VR002752</td>\n",
       "      <td>NS1</td>\n",
       "      <td>NS</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>559.326538</td>\n",
       "      <td>217.673462</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>30.13</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018.046575</td>\n",
       "      <td>0.546575</td>\n",
       "      <td>3</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>1445</td>\n",
       "      <td>18VR002752</td>\n",
       "      <td>PA</td>\n",
       "      <td>PA</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>1561.879502</td>\n",
       "      <td>616.102480</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>30.13</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018.046575</td>\n",
       "      <td>0.546575</td>\n",
       "      <td>3</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>1446</td>\n",
       "      <td>18VR002752</td>\n",
       "      <td>PB1</td>\n",
       "      <td>PB1</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>1608.067791</td>\n",
       "      <td>647.932209</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>30.13</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018.046575</td>\n",
       "      <td>0.546575</td>\n",
       "      <td>3</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>1447</td>\n",
       "      <td>18VR002752</td>\n",
       "      <td>PB2</td>\n",
       "      <td>PB2</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>1655.896539</td>\n",
       "      <td>654.103461</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>30.13</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018.046575</td>\n",
       "      <td>0.546575</td>\n",
       "      <td>3</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index    sampleID product segment   pi_gene  piN_gene  piS_gene  \\\n",
       "1437   1437  18VR002752     BM2      MP  0.000109  0.000000  0.000434   \n",
       "1438   1438  18VR002752      HA      HA  0.000086  0.000046  0.000182   \n",
       "1439   1439  18VR002752      M1      MP  0.000106  0.000152  0.000000   \n",
       "1440   1440  18VR002752      NA      NA  0.000117  0.000146  0.000050   \n",
       "1441   1441  18VR002752      NB      NA  0.000132  0.000000  0.000467   \n",
       "1442   1442  18VR002752     NEP      NS  0.000119  0.000185  0.000000   \n",
       "1443   1443  18VR002752      NP      NP  0.000062  0.000047  0.000100   \n",
       "1444   1444  18VR002752     NS1      NS  0.000458  0.000434  0.000666   \n",
       "1445   1445  18VR002752      PA      PA  0.000130  0.000152  0.000075   \n",
       "1446   1446  18VR002752     PB1     PB1  0.000132  0.000075  0.000273   \n",
       "1447   1447  18VR002752     PB2     PB2  0.000111  0.000044  0.000281   \n",
       "\n",
       "      N_sites_gene  S_sites_gene    ptID      subtype     CT  mapped  \\\n",
       "1437    244.361986     82.638014  1072.1  Influenza B  30.13    True   \n",
       "1438   1233.813263    518.186737  1072.1  Influenza B  30.13    True   \n",
       "1439    521.691978    222.308022  1072.1  Influenza B  30.13    True   \n",
       "1440    982.549630    415.450370  1072.1  Influenza B  30.13    True   \n",
       "1441    214.349602     85.650398  1072.1  Influenza B  30.13    True   \n",
       "1442    238.571869     94.428131  1072.1  Influenza B  30.13    True   \n",
       "1443   1197.622058    482.377942  1072.1  Influenza B  30.13    True   \n",
       "1444    559.326538    217.673462  1072.1  Influenza B  30.13    True   \n",
       "1445   1561.879502    616.102480  1072.1  Influenza B  30.13    True   \n",
       "1446   1608.067791    647.932209  1072.1  Influenza B  30.13    True   \n",
       "1447   1655.896539    654.103461  1072.1  Influenza B  30.13    True   \n",
       "\n",
       "     sample_date  decimalDate  seasonalDate  week season day0_or_day7_sample  \\\n",
       "1437  2018-01-18  2018.046575      0.546575     3  17-18               Day 0   \n",
       "1438  2018-01-18  2018.046575      0.546575     3  17-18               Day 0   \n",
       "1439  2018-01-18  2018.046575      0.546575     3  17-18               Day 0   \n",
       "1440  2018-01-18  2018.046575      0.546575     3  17-18               Day 0   \n",
       "1441  2018-01-18  2018.046575      0.546575     3  17-18               Day 0   \n",
       "1442  2018-01-18  2018.046575      0.546575     3  17-18               Day 0   \n",
       "1443  2018-01-18  2018.046575      0.546575     3  17-18               Day 0   \n",
       "1444  2018-01-18  2018.046575      0.546575     3  17-18               Day 0   \n",
       "1445  2018-01-18  2018.046575      0.546575     3  17-18               Day 0   \n",
       "1446  2018-01-18  2018.046575      0.546575     3  17-18               Day 0   \n",
       "1447  2018-01-18  2018.046575      0.546575     3  17-18               Day 0   \n",
       "\n",
       "      household  participant_ID   age  gender time_of_symptom_onset  \\\n",
       "1437     1072.0          1072.1  35.0     2.0            2018-01-14   \n",
       "1438     1072.0          1072.1  35.0     2.0            2018-01-14   \n",
       "1439     1072.0          1072.1  35.0     2.0            2018-01-14   \n",
       "1440     1072.0          1072.1  35.0     2.0            2018-01-14   \n",
       "1441     1072.0          1072.1  35.0     2.0            2018-01-14   \n",
       "1442     1072.0          1072.1  35.0     2.0            2018-01-14   \n",
       "1443     1072.0          1072.1  35.0     2.0            2018-01-14   \n",
       "1444     1072.0          1072.1  35.0     2.0            2018-01-14   \n",
       "1445     1072.0          1072.1  35.0     2.0            2018-01-14   \n",
       "1446     1072.0          1072.1  35.0     2.0            2018-01-14   \n",
       "1447     1072.0          1072.1  35.0     2.0            2018-01-14   \n",
       "\n",
       "      recieved_flu_vaccine  school  jackson_score  antipyretic_use  \\\n",
       "1437                   1.0     NaN            NaN              NaN   \n",
       "1438                   1.0     NaN            NaN              NaN   \n",
       "1439                   1.0     NaN            NaN              NaN   \n",
       "1440                   1.0     NaN            NaN              NaN   \n",
       "1441                   1.0     NaN            NaN              NaN   \n",
       "1442                   1.0     NaN            NaN              NaN   \n",
       "1443                   1.0     NaN            NaN              NaN   \n",
       "1444                   1.0     NaN            NaN              NaN   \n",
       "1445                   1.0     NaN            NaN              NaN   \n",
       "1446                   1.0     NaN            NaN              NaN   \n",
       "1447                   1.0     NaN            NaN              NaN   \n",
       "\n",
       "      flu_antiviral_treatment  school_absence  number_in_household  \\\n",
       "1437                      NaN             NaN                  NaN   \n",
       "1438                      NaN             NaN                  NaN   \n",
       "1439                      NaN             NaN                  NaN   \n",
       "1440                      NaN             NaN                  NaN   \n",
       "1441                      NaN             NaN                  NaN   \n",
       "1442                      NaN             NaN                  NaN   \n",
       "1443                      NaN             NaN                  NaN   \n",
       "1444                      NaN             NaN                  NaN   \n",
       "1445                      NaN             NaN                  NaN   \n",
       "1446                      NaN             NaN                  NaN   \n",
       "1447                      NaN             NaN                  NaN   \n",
       "\n",
       "      recent_travel  isIndexCase  %ofHHinfected age_category  \\\n",
       "1437            NaN         True            0.5      Over 18   \n",
       "1438            NaN         True            0.5      Over 18   \n",
       "1439            NaN         True            0.5      Over 18   \n",
       "1440            NaN         True            0.5      Over 18   \n",
       "1441            NaN         True            0.5      Over 18   \n",
       "1442            NaN         True            0.5      Over 18   \n",
       "1443            NaN         True            0.5      Over 18   \n",
       "1444            NaN         True            0.5      Over 18   \n",
       "1445            NaN         True            0.5      Over 18   \n",
       "1446            NaN         True            0.5      Over 18   \n",
       "1447            NaN         True            0.5      Over 18   \n",
       "\n",
       "      days_since_symptom_onset  symptom_severity  saw_a_doctor  \\\n",
       "1437                       4.0               2.0           1.0   \n",
       "1438                       4.0               2.0           1.0   \n",
       "1439                       4.0               2.0           1.0   \n",
       "1440                       4.0               2.0           1.0   \n",
       "1441                       4.0               2.0           1.0   \n",
       "1442                       4.0               2.0           1.0   \n",
       "1443                       4.0               2.0           1.0   \n",
       "1444                       4.0               2.0           1.0   \n",
       "1445                       4.0               2.0           1.0   \n",
       "1446                       4.0               2.0           1.0   \n",
       "1447                       4.0               2.0           1.0   \n",
       "\n",
       "      taken_on_hh_infection_day  clade subclade  num_of_snps  \\\n",
       "1437                          0  1088G        9         40.0   \n",
       "1438                          0  1088G        9         40.0   \n",
       "1439                          0  1088G        9         40.0   \n",
       "1440                          0  1088G        9         40.0   \n",
       "1441                          0  1088G        9         40.0   \n",
       "1442                          0  1088G        9         40.0   \n",
       "1443                          0  1088G        9         40.0   \n",
       "1444                          0  1088G        9         40.0   \n",
       "1445                          0  1088G        9         40.0   \n",
       "1446                          0  1088G        9         40.0   \n",
       "1447                          0  1088G        9         40.0   \n",
       "\n",
       "      num_of_mixed_muts  num_of_nonsynon_muts  num_of_oof_muts  \\\n",
       "1437                0.0                  23.0              2.0   \n",
       "1438                0.0                  23.0              2.0   \n",
       "1439                0.0                  23.0              2.0   \n",
       "1440                0.0                  23.0              2.0   \n",
       "1441                0.0                  23.0              2.0   \n",
       "1442                0.0                  23.0              2.0   \n",
       "1443                0.0                  23.0              2.0   \n",
       "1444                0.0                  23.0              2.0   \n",
       "1445                0.0                  23.0              2.0   \n",
       "1446                0.0                  23.0              2.0   \n",
       "1447                0.0                  23.0              2.0   \n",
       "\n",
       "      num_of_synon_muts  Xue_divergence  Xue_mixed_divergence  \\\n",
       "1437               15.0          0.9801                   0.0   \n",
       "1438               15.0          0.9801                   0.0   \n",
       "1439               15.0          0.9801                   0.0   \n",
       "1440               15.0          0.9801                   0.0   \n",
       "1441               15.0          0.9801                   0.0   \n",
       "1442               15.0          0.9801                   0.0   \n",
       "1443               15.0          0.9801                   0.0   \n",
       "1444               15.0          0.9801                   0.0   \n",
       "1445               15.0          0.9801                   0.0   \n",
       "1446               15.0          0.9801                   0.0   \n",
       "1447               15.0          0.9801                   0.0   \n",
       "\n",
       "      Xue_nonsynon_divergence  Xue_oof_divergence  Xue_synon_divergence  \n",
       "1437                   0.5216              0.0555                 0.403  \n",
       "1438                   0.5216              0.0555                 0.403  \n",
       "1439                   0.5216              0.0555                 0.403  \n",
       "1440                   0.5216              0.0555                 0.403  \n",
       "1441                   0.5216              0.0555                 0.403  \n",
       "1442                   0.5216              0.0555                 0.403  \n",
       "1443                   0.5216              0.0555                 0.403  \n",
       "1444                   0.5216              0.0555                 0.403  \n",
       "1445                   0.5216              0.0555                 0.403  \n",
       "1446                   0.5216              0.0555                 0.403  \n",
       "1447                   0.5216              0.0555                 0.403  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if useSNPGenie2:\n",
    "    geneDiversity = geneDiversity.pivot_table(index=['sampleID','product', 'segment'], columns='type', values=['pi', 'sites']).reset_index()\n",
    "    geneDiversity.columns = ['sampleID','product', 'segment', 'pi','piN','piS','N_sites_gene','S_sites_gene']\n",
    "geneDiversity = geneDiversity.reset_index().rename(columns={'pi':'pi_gene','piN':'piN_gene', 'piS':'piS_gene'})\n",
    "geneDiversity = geneDiversity.merge(samples, on='sampleID', how='left')\n",
    "geneDiversity.loc[geneDiversity['sampleID']=='18VR002752']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sampleID</th>\n",
       "      <th>product</th>\n",
       "      <th>segment</th>\n",
       "      <th>pi_gene</th>\n",
       "      <th>piN_gene</th>\n",
       "      <th>piS_gene</th>\n",
       "      <th>N_sites_gene</th>\n",
       "      <th>S_sites_gene</th>\n",
       "      <th>ptID</th>\n",
       "      <th>subtype</th>\n",
       "      <th>CT</th>\n",
       "      <th>mapped</th>\n",
       "      <th>sample_date</th>\n",
       "      <th>decimalDate</th>\n",
       "      <th>seasonalDate</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>day0_or_day7_sample</th>\n",
       "      <th>household</th>\n",
       "      <th>participant_ID</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>time_of_symptom_onset</th>\n",
       "      <th>recieved_flu_vaccine</th>\n",
       "      <th>school</th>\n",
       "      <th>jackson_score</th>\n",
       "      <th>antipyretic_use</th>\n",
       "      <th>flu_antiviral_treatment</th>\n",
       "      <th>school_absence</th>\n",
       "      <th>number_in_household</th>\n",
       "      <th>recent_travel</th>\n",
       "      <th>isIndexCase</th>\n",
       "      <th>%ofHHinfected</th>\n",
       "      <th>age_category</th>\n",
       "      <th>days_since_symptom_onset</th>\n",
       "      <th>symptom_severity</th>\n",
       "      <th>saw_a_doctor</th>\n",
       "      <th>taken_on_hh_infection_day</th>\n",
       "      <th>clade</th>\n",
       "      <th>subclade</th>\n",
       "      <th>num_of_snps</th>\n",
       "      <th>num_of_mixed_muts</th>\n",
       "      <th>num_of_nonsynon_muts</th>\n",
       "      <th>num_of_oof_muts</th>\n",
       "      <th>num_of_synon_muts</th>\n",
       "      <th>Xue_divergence</th>\n",
       "      <th>Xue_mixed_divergence</th>\n",
       "      <th>Xue_nonsynon_divergence</th>\n",
       "      <th>Xue_oof_divergence</th>\n",
       "      <th>Xue_synon_divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>924</td>\n",
       "      <td>18VR001131</td>\n",
       "      <td>HA</td>\n",
       "      <td>HA</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>1231.866538</td>\n",
       "      <td>520.133462</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>32.15</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2018.00274</td>\n",
       "      <td>0.50274</td>\n",
       "      <td>1</td>\n",
       "      <td>17-18</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-12-31 15:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088A</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.8849</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>1.1902</td>\n",
       "      <td>0.0651</td>\n",
       "      <td>0.6015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    sampleID product segment   pi_gene  piN_gene  piS_gene  \\\n",
       "924    924  18VR001131      HA      HA  0.000242  0.000307  0.000091   \n",
       "\n",
       "     N_sites_gene  S_sites_gene    ptID      subtype     CT  mapped  \\\n",
       "924   1231.866538    520.133462  1036.0  Influenza B  32.15    True   \n",
       "\n",
       "    sample_date  decimalDate  seasonalDate  week season day0_or_day7_sample  \\\n",
       "924  2018-01-02   2018.00274       0.50274     1  17-18               Day 0   \n",
       "\n",
       "     household  participant_ID  age  gender time_of_symptom_onset  \\\n",
       "924     1036.0          1036.0  7.0     1.0   2017-12-31 15:00:00   \n",
       "\n",
       "     recieved_flu_vaccine  school  jackson_score  antipyretic_use  \\\n",
       "924                   0.0     5.0           12.0              0.0   \n",
       "\n",
       "     flu_antiviral_treatment  school_absence  number_in_household  \\\n",
       "924                      0.0             1.0                  3.0   \n",
       "\n",
       "     recent_travel  isIndexCase  %ofHHinfected age_category  \\\n",
       "924            1.0        False            NaN  18 or Under   \n",
       "\n",
       "     days_since_symptom_onset  symptom_severity  saw_a_doctor  \\\n",
       "924                       2.0               2.0           0.0   \n",
       "\n",
       "     taken_on_hh_infection_day  clade subclade  num_of_snps  \\\n",
       "924                          0  1088A        0         29.0   \n",
       "\n",
       "     num_of_mixed_muts  num_of_nonsynon_muts  num_of_oof_muts  \\\n",
       "924                1.0                  19.0              2.0   \n",
       "\n",
       "     num_of_synon_muts  Xue_divergence  Xue_mixed_divergence  \\\n",
       "924                7.0          1.8849                0.0281   \n",
       "\n",
       "     Xue_nonsynon_divergence  Xue_oof_divergence  Xue_synon_divergence  \n",
       "924                   1.1902              0.0651                0.6015  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tmp=popDiversity.reset_index().rename(columns={'index':'sampleID'}).merge(samples, on='sampleID',suffixes=('_2','_1'))\n",
    "# print (tmp.columns)\n",
    "# sns.scatterplot(x='N_sites_sample_1', y='pi_sample_1', data=tmp)\n",
    "geneDiversity.loc[(geneDiversity.segment=='HA')&(geneDiversity.sampleID=='18VR001131')]\n",
    "# genes.loc[(genes.segment=='HA')&(genes.num_of_snps==0)]\n",
    "# samples.loc[samples.num_of_snps==0]\n",
    "# genes.loc[genes.sampleID=='18VR001131']\n",
    "# geneDiversity[['sampleID','product','pi_gene','piN_gene','piS_gene','piN_gene']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bamfiles = {vcffile:dict() for vcffile in vcffiles[1:2]}\n",
    "#1349.666667\t402.333333\n",
    "#1333.440680\t418.559320\t\n",
    "# for folder, allSNPs in zip(mainSampleFolders[1:2], vcffiles[1:2]):\n",
    "#     files = glob.glob(folder+'/snp_calls/*[0-9].vcf')\n",
    "#     for file in files:\n",
    "#         sample = file.split('.')[0].split('/')[-1]\n",
    "#         bamfile = folder+'/map_to_consensus/'+sample+'.bam'\n",
    "#         bamfiles[allSNPs][sample] = bamfile\n",
    "# siteDiversity.loc[(siteDiversity['sampleID']=='18VR004622')&(siteDiversity['product']=='M2')]\n",
    "# # geneDiversity.loc[(geneDiversity['sampleID']=='18VR004622')&(geneDiversity['product']=='M2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNPs.loc[(SNPs['sampleID']=='18VR004622')&(SNPs['product']=='M2')&(SNPs.SNP_frequency < .99)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bamfiles['/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import vcfClass\n",
    "# importlib.reload(vcfClass)\n",
    "# from vcfClass import VCF\n",
    "# SNPGenie_input = '/mnt/d/ORCHARDS/H1N1/ORCHARDS_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/SNPGenie_output/A_Hong_Kong_4801_2014_834576_MP/'\n",
    "# # '\\ORCHARDS\\H1N1\\ORCHARDS_run19H3N2\\A_Hong_Kong_4801_2014_EPI834581\\SNP_Genie_061220_loose_Filter\\A_Hong_Kong_4801_2014_834576_MP'\n",
    "# M = VCF(SNPGenie_input+'A_Hong_Kong_4801_2014_834576_MP.vcf', bamfiles=bamfiles['/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Hong_Kong_4801_2014_EPI834581/snp_calls/filtered_snpcalls/all_snps_lenient_filter.vcf'], gtfFile=SNPGenie_input+'A_Hong_Kong_4801_2014_834576_MP.gtf', refFile=SNPGenie_input+'A_Hong_Kong_4801_2014_834576_MP.fasta')\n",
    "# M.annotate(gtffile=SNPGenie_input+'A_Hong_Kong_4801_2014_834576_MP.gtf', reference=SNPGenie_input+'A_Hong_Kong_4801_2014_834576_MP.fasta')\n",
    "# # M.add_bamfile_locations()\n",
    "# df = M.to_dataframe()\n",
    "# print(len(df.loc[df.FREQ > 0.01]))\n",
    "# M = M.apply_position_filter(in_read_cutoff=0.1)\n",
    "# M = M.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # M.loc[(M.sampleID=='18VR004622')&(M.DP>0)]\n",
    "# alt = np.random.lognormal(size=10000)*20\n",
    "# ref = np.random.lognormal(size=10000)*20\n",
    "# def bootstrap_np(array, bootsize):\n",
    "#     return np.random.choice(array, len(array)*bootsize, replace=True).reshape(-1, bootsize).mean(axis=0)\n",
    "# def bootstrap_py(array, bootsize):\n",
    "#     return np.array([np.random.choice(array, len(array), replace=True) for _ in range(bootsize)])\n",
    "# # %timeit bootstrap_np(ref, 10000)\n",
    "# # %timeit bootstrap_py(ref, 10000)\n",
    "# # %timeit print(bootstrap_np(alt, 10000))\n",
    "# # %timeit print(bootstrap_py(alt, 10000))\n",
    "# print(np.mean(bootstrap_np(ref, 10)))\n",
    "# print(np.mean(bootstrap_py(ref, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sampleID</th>\n",
       "      <th>pi_sample</th>\n",
       "      <th>piN_sum</th>\n",
       "      <th>piN_sites</th>\n",
       "      <th>piS_sum</th>\n",
       "      <th>piS_sites</th>\n",
       "      <th>piN_sample</th>\n",
       "      <th>piS_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>18VR002212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9359.209612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3687.790388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sampleID  pi_sample  piN_sum    piN_sites  piS_sum    piS_sites  \\\n",
       "179  18VR002212        0.0      0.0  9359.209612      0.0  3687.790388   \n",
       "\n",
       "     piN_sample  piS_sample  \n",
       "179         0.0         0.0  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popDiversity.loc[popDiversity.sampleID=='18VR002212']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geneDiversity = geneDiversity.merge(samples, on='sampleID', how='left')\n",
    "\n",
    "# if useSNPGenie2:\n",
    "#     geneDiversity['sample_contrib_N'] = geneDiversity.apply(lambda x: x.piN_gene*genelengths[x.subtype][x['product']]/np.sum(list(genelengths['H3N2'].values())), axis=1)\n",
    "#     geneDiversity['sample_contrib_S'] = geneDiversity.apply(lambda x: x.piS_gene*genelengths[x.subtype][x['product']]/np.sum(list(genelengths['H3N2'].values())), axis=1)\n",
    "#     sample_piN_piS = geneDiversity[['sampleID','sample_contrib_N','sample_contrib_S']].groupby('sampleID').sum().reset_index().rename(columns={'sample_contrib_N':'piN_sample','sample_contrib_S':'piS_sample'})\n",
    "#     samples = samples.merge(sample_piN_piS, on='sampleID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useSNPGenie2:\n",
    "    samples = samples.merge(popDiversity[['sampleID','pi_sample', 'piN_sites','piS_sites','piN_sample','piS_sample']], on='sampleID',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = samples.merge(popDiversity, on='sampleID')\n",
    "# samples\n",
    "# samples = samples.merge(popDiversity, )\n",
    "# samples.loc[samples.piS_sample.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load rolling window Pi data (warning: this can take a long time)\n",
    "if False:\n",
    "    if False:\n",
    "        slidingWindowDiversity = pd.DataFrame()\n",
    "\n",
    "        for segfolder in tqdm(SnpGenieSegFolders):\n",
    "            try:\n",
    "                slidingwindowdiversityfile = glob.glob(os.path.join(segfolder, 'sliding_window*.txt'))[0]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            tmpslidingWindowDiversity = pd.read_csv(slidingwindowdiversityfile, **read_tsv_args)\n",
    "            currentsegment = segfolder.split('/')[-1].split(\"_\")[-1]\n",
    "            tmpslidingWindowDiversity['segment'] = currentsegment\n",
    "            tmpslidingWindowDiversity['sampleID'] = tmpslidingWindowDiversity.file.str.split('_').str[-1].str.split('.').str[0]\n",
    "            slidingWindowDiversity = slidingWindowDiversity.append(tmpslidingWindowDiversity, ignore_index=True).reset_index(drop=True)\n",
    "        \n",
    "        slidingWindowDiversity = slidingWindowDiversity.replace(r'^\\*$', np.nan, regex=True)\n",
    "        \n",
    "        #process slidingWindowDiversity files\n",
    "        slidingWindowDiversity = slidingWindowDiversity.merge(samples[['sampleID','subtype']], on='sampleID',how='left')\n",
    "        slidingWindowDiversity = slidingWindowDiversity.loc[~slidingWindowDiversity['file'].isna()]\n",
    "        slidingWindowDiversity = slidingWindowDiversity.loc[pd.notna(slidingWindowDiversity['subtype'])]\n",
    "        slidingWindowDiversity.to_csv(figures+'/slidingWindowDiversity-collected.tsv', sep='\\t',index=False)\n",
    "    else:\n",
    "        slidingWindowDiversity = pd.read_csv(figures+'/slidingWindowDiversity-collected.tsv', **read_tsv_args)\n",
    "        \n",
    "    \n",
    "    slidingwindowdiversityfile = glob.glob(os.path.join(SnpGenieSegFolders[0], 'sliding_window*.txt'))[0]\n",
    "        \n",
    "    slidingWindowDiversity['inGenePos']=0\n",
    "    slidingWindowDiversity['codon'] = 0\n",
    "    slidingWindowDiversity['windowSize'] = slidingwindowdiversityfile.split('length')[-1].split('_')[0]\n",
    "    slidingWindowDiversity = slidingWindowDiversity.merge(samples[['sampleID','season']], how='left', on='sampleID')\n",
    "    \n",
    "    with tqdm(total=len(slidingWindowDiversity['subtype'].dropna().unique())*12*2) as pbar:\n",
    "        for subtype in slidingWindowDiversity['subtype'].dropna().unique():\n",
    "#             print (subtype)\n",
    "            insubtype = slidingWindowDiversity['subtype'] == subtype\n",
    "            samplenames = slidingWindowDiversity.loc[insubtype, 'sampleID'].dropna().unique()\n",
    "            productnames = slidingWindowDiversity.loc[insubtype, 'product'].dropna().unique()\n",
    "            seasonnames = slidingWindowDiversity.loc[insubtype, 'season'].dropna().unique()\n",
    "\n",
    "            for season in seasonnames:\n",
    "                isseason = slidingWindowDiversity['season']==season\n",
    "#                 print (season)\n",
    "\n",
    "                for product in productnames:\n",
    "                    isproduct = (slidingWindowDiversity['product']==product) & isseason & insubtype\n",
    "                    locsegment=slidingWindowDiversity.loc[isproduct, 'segment'].unique()[0]\n",
    "                    codonfile = snpGenieDict[subtype][season]+locsegment+'/codon_results.txt'\n",
    "                    codondf = pd.read_csv(codonfile,**read_tsv_args)\n",
    "                    inGenePositions = list(codondf.loc[codondf['product']==product,'site'].unique())\n",
    "                    slidingWindowDiversity.loc[isproduct, 'codon'] = slidingWindowDiversity.loc[isproduct,'first_site'].apply(lambda x: inGenePositions.index(x))\n",
    "                    slidingWindowDiversity.loc[isproduct, 'inGenePos'] = slidingWindowDiversity.loc[isproduct, 'codon']*3\n",
    "                    pbar.update(1)\n",
    "    slidingWindowDiversity.to_csv(figures+'/slidingWindowDiversity.tsv', sep='\\t',index=False)\n",
    "    \n",
    "else:\n",
    "    slidingWindowDiversity = pd.read_csv(figures+'/slidingWindowDiversity.tsv', **read_tsv_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(genes)#.loc[genes.piN_gene>0, 'piN_gene'])\n",
    "# len(geneDiversity)#.loc[geneDiversity.piN_gene>0, 'piN_gene'])\n",
    "# join(g)\n",
    "# sns.scatterplot(x = genes.loc[genes.piN_gene>0, 'piN_gene'], y=)\n",
    "# sns.scatterplot(x='piN_gene', y='piN_gene_alt',data=pd.DataFrame(genes['piN_gene'], geneDiversity['piN_gene'].rename('piN_gene_alt')).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'sampleID', 'product', 'segment', 'pi_gene', 'piN_gene',\n",
       "       'piS_gene', 'N_sites_gene', 'S_sites_gene', 'ptID', 'subtype', 'CT',\n",
       "       'mapped', 'sample_date', 'decimalDate', 'seasonalDate', 'week',\n",
       "       'season', 'day0_or_day7_sample', 'household', 'participant_ID', 'age',\n",
       "       'gender', 'time_of_symptom_onset', 'recieved_flu_vaccine', 'school',\n",
       "       'jackson_score', 'antipyretic_use', 'flu_antiviral_treatment',\n",
       "       'school_absence', 'number_in_household', 'recent_travel', 'isIndexCase',\n",
       "       '%ofHHinfected', 'age_category', 'days_since_symptom_onset',\n",
       "       'symptom_severity', 'saw_a_doctor', 'taken_on_hh_infection_day',\n",
       "       'clade', 'subclade', 'num_of_snps', 'num_of_mixed_muts',\n",
       "       'num_of_nonsynon_muts', 'num_of_oof_muts', 'num_of_synon_muts',\n",
       "       'Xue_divergence', 'Xue_mixed_divergence', 'Xue_nonsynon_divergence',\n",
       "       'Xue_oof_divergence', 'Xue_synon_divergence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geneDiversity.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not useSNPGenie2:\n",
    "    popDiversity = popDiversity[['sampleID','segment','pi', 'piN', 'piS', 'N_sites','S_sites']].rename(columns={'pi':'pi_segment', 'piN':'piN_segment', 'piS':'piS_segment', 'N_sites':'num_N_sites_seg', 'S_sites':'num_S_sites_seg'})\n",
    "    segments = popDiversity.merge(samples, on='sampleID', how='left')\n",
    "    segments = segments.loc[~segments.sampleID.isin(potentialmixed)]\n",
    "else:\n",
    "    segments = segDiversity.merge(samples, on='sampleID', how='left')\n",
    "    segments = segments.loc[~segments.sampleID.isin(potentialmixed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SNPs.referenceFile.unique()\n",
    "# popDiversity = pd.DataFrame()\n",
    "# segDiversity = pd.DataFrame()\n",
    "# geneDiversity = pd.DataFrame()\n",
    "# siteDiversity = pd.DataFrame()\n",
    "# for ref, df in SNPs.groupby('referenceFile'):\n",
    "#     print (ref)\n",
    "#     print (gtfFileDict[df.subtype.iloc[0]][df.season.iloc[0]])\n",
    "#     df = df.rename(columns={'sampleID':'sample'})\n",
    "#     perSamplePiDF, perSegmentPiDF, genePis, perSitePiDF = SNPGenie2(df, gtfFileDict[df.subtype.iloc[0]][df.season.iloc[0]], ref)\n",
    "#     popDiversity = popDiversity.append(perSamplePiDF)\n",
    "#     segDiversity = segDiversity.append(perSegmentPiDF)\n",
    "#     geneDiversity = geneDiversity.append(genePis)\n",
    "#     siteDiversity = siteDiversity.append(perSitePiDF)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for calculating overall pi statistic by weighted mean of segmental pi\n",
    "seglengths = {'H3N2':(2316,2316,2209,1737,1541,1441,1002,865),'H1N1':(2316,2317,2208,1753,1540,1433,1002,865),'H1N1pdm':(2316,2317,2208,1753,1540,1433,1002,865),'Influenza B':(2367,2340,2275,1853,1815,1530,1155,1062)}\n",
    "segnames = ('PB2','PB1','NP','HA','NA','PA','MP','NS')\n",
    "\n",
    "weightdict = {subtype:{name:weight/sum(seglengths[subtype]) for name, weight in zip(segnames,seglengths[subtype])} for subtype in seglengths.keys()}\n",
    "\n",
    "def weightavg(group, idcolumn, values, weights):\n",
    "    subtype = group.subtype.iloc[0]\n",
    "    return sum([val * weights[subtype][idcolumnval] for idcolumnval, val in zip(group[idcolumn].values,group[values].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that segments have been created, go back and calculate number of SNPs in each segment\n",
    "\n",
    "segCounts = rawSNPs.groupby(['sampleID','segment']).count()['pos'].reset_index().rename(columns={'pos':'num_of_snps_segment'}).replace(np.nan,0)\n",
    "segSynonCounts = NSrawSNPs.groupby(['sampleID','segment','AAtype']).count().pivot_table(index=['sampleID','segment'], columns='AAtype')['ABQ'].replace(np.nan, 0).rename(columns={'Nonsynonymous':'num_of_nonsynon_muts_segment','Synonymous':'num_of_synon_muts_segment','Out of reading frame':'num_of_oof_muts_segment','Mixed':'num_of_mixed_muts_segment'}).reset_index()\n",
    "segDivergence = rawSNPs.groupby(['sampleID','segment']).sum()['transformed_frequency'].reset_index().rename(columns={'transformed_frequency':'Xue_divergence_segment'}).replace(np.nan,0)\n",
    "segSynonDivergence = NSrawSNPs.groupby(['sampleID', 'segment','AAtype']).sum()['transformed_frequency'].reset_index().pivot_table(index=['sampleID','segment'], columns='AAtype')['transformed_frequency'].replace(np.nan, 0).rename(columns={'Nonsynonymous':'Xue_nonsynon_divergence_segment','Synonymous':'Xue_synon_divergence_segment','Out of reading frame':'Xue_oof_divergence_segment','Mixed':'Xue_mixed_divergence_segment'}).reset_index()\n",
    "\n",
    "segments = segments.merge(segCounts, on=['sampleID', 'segment'], how='left')\n",
    "segments = segments.merge(segSynonCounts, on=['sampleID', 'segment'], how='left')\n",
    "segments = segments.merge(segDivergence, on=['sampleID', 'segment'], how='left')\n",
    "segments = segments.merge(segSynonDivergence, on=['sampleID', 'segment'], how='left')\n",
    "\n",
    "newly_added_columns = ['num_of_snps_segment','num_of_nonsynon_muts_segment', 'num_of_synon_muts_segment', 'num_of_oof_muts_segment', 'num_of_mixed_muts_segment',\n",
    "                       'Xue_divergence_segment', 'Xue_nonsynon_divergence_segment', 'Xue_synon_divergence_segment', 'Xue_oof_divergence_segment', 'Xue_mixed_divergence_segment']\n",
    "segments[newly_added_columns] = segments[newly_added_columns].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y =segments.loc[segments.sampleID=='18VR001031',['segment','pi_segment']].set_index('segment')\n",
    "# y.loc[list(segnames)].to_numpy().flatten()*np.array([2316,2316,2209,1737,1541,1441,1002,865])/13427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sampleID', 'chrom', 'pos', 'ref_nuc', 'alt_nuc', 'qual', 'GT', 'GQ',\n",
       "       'SDP', 'depth', 'RD', 'AD', 'SNP_frequency', 'PVAL', 'RBQ', 'ABQ',\n",
       "       'RDF', 'RDR', 'ADF', 'ADR', 'product', 'inGenePos', 'refAA', 'codon',\n",
       "       'altAA', 'AAstr', 'AAtype', 'referenceFile', 'segment', 'subtype', 'CT',\n",
       "       'sample_date', 'decimalDate', 'seasonalDate', 'week', 'season',\n",
       "       'household', 'participant_ID', 'age', 'gender', 'time_of_symptom_onset',\n",
       "       'recieved_flu_vaccine', 'school', 'jackson_score', 'antipyretic_use',\n",
       "       'flu_antiviral_treatment', 'school_absence', 'number_in_household',\n",
       "       'recent_travel', 'isIndexCase', '%ofHHinfected', 'age_category',\n",
       "       'days_since_symptom_onset', 'symptom_severity', 'saw_a_doctor',\n",
       "       'taken_on_hh_infection_day', 'clade', 'subclade',\n",
       "       'transformed_frequency', 'log_transformed_frequency',\n",
       "       'rolling_AA_mean_global_freq', 'rolling_AA_max_global_freq',\n",
       "       'rolling_DNA_mean_global_freq', 'rolling_DNA_max_global_freq',\n",
       "       'rolling_AA_mean_global_freq_following_year',\n",
       "       'rolling_AA_max_global_freq_following_year',\n",
       "       'rolling_DNA_mean_global_freq_following_year',\n",
       "       'rolling_DNA_max_global_freq_following_year', 'AAobservedGlobally',\n",
       "       'AAobservedGloballyFollowingSeason', 'DNAobservedGlobally',\n",
       "       'DNAobservedGloballyFollowingSeason', 'antigenic_site',\n",
       "       'antigenic_product', 'snpKey'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segnames.index('HA')\n",
    "# np.array([2316,2316,2209,1737,1541,1441,1002,865])/13427\n",
    "SNPs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not useSNPGenie2:\n",
    "    samples = samples.merge(segments.groupby('sampleID').apply(weightavg, 'segment','pi_segment',weightdict).reset_index().rename(columns={0:'pi_sample'}), on='sampleID')\n",
    "    samples = samples.merge(segments.groupby('sampleID').apply(weightavg, 'segment','piN_segment',weightdict).reset_index().rename(columns={0:'piN_sample'}), on='sampleID')\n",
    "    samples = samples.merge(segments.groupby('sampleID').apply(weightavg, 'segment','piS_segment',weightdict).reset_index().rename(columns={0:'piS_sample'}), on='sampleID')\n",
    "    samples = samples.merge(segments[['sampleID','num_N_sites_seg']].groupby('sampleID').sum().reset_index().rename(columns={'num_N_sites_seg':'num_N_sites_samp'}), on='sampleID')\n",
    "    samples = samples.merge(segments[['sampleID','num_S_sites_seg']].groupby('sampleID').sum().reset_index().rename(columns={'num_S_sites_seg':'num_S_sites_samp'}), on='sampleID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sampleID</th>\n",
       "      <th>product</th>\n",
       "      <th>segment</th>\n",
       "      <th>pi_gene</th>\n",
       "      <th>piN_gene</th>\n",
       "      <th>piS_gene</th>\n",
       "      <th>N_sites_gene</th>\n",
       "      <th>S_sites_gene</th>\n",
       "      <th>ptID</th>\n",
       "      <th>subtype</th>\n",
       "      <th>CT</th>\n",
       "      <th>mapped</th>\n",
       "      <th>sample_date</th>\n",
       "      <th>decimalDate</th>\n",
       "      <th>seasonalDate</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>day0_or_day7_sample</th>\n",
       "      <th>household</th>\n",
       "      <th>participant_ID</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>time_of_symptom_onset</th>\n",
       "      <th>recieved_flu_vaccine</th>\n",
       "      <th>school</th>\n",
       "      <th>jackson_score</th>\n",
       "      <th>antipyretic_use</th>\n",
       "      <th>flu_antiviral_treatment</th>\n",
       "      <th>school_absence</th>\n",
       "      <th>number_in_household</th>\n",
       "      <th>recent_travel</th>\n",
       "      <th>isIndexCase</th>\n",
       "      <th>%ofHHinfected</th>\n",
       "      <th>age_category</th>\n",
       "      <th>days_since_symptom_onset</th>\n",
       "      <th>symptom_severity</th>\n",
       "      <th>saw_a_doctor</th>\n",
       "      <th>taken_on_hh_infection_day</th>\n",
       "      <th>clade</th>\n",
       "      <th>subclade</th>\n",
       "      <th>num_of_snps</th>\n",
       "      <th>num_of_mixed_muts</th>\n",
       "      <th>num_of_nonsynon_muts</th>\n",
       "      <th>num_of_oof_muts</th>\n",
       "      <th>num_of_synon_muts</th>\n",
       "      <th>Xue_divergence</th>\n",
       "      <th>Xue_mixed_divergence</th>\n",
       "      <th>Xue_nonsynon_divergence</th>\n",
       "      <th>Xue_oof_divergence</th>\n",
       "      <th>Xue_synon_divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17VR005325</td>\n",
       "      <td>BM2</td>\n",
       "      <td>MP</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>242.179553</td>\n",
       "      <td>84.820447</td>\n",
       "      <td>720.0</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>28.43</td>\n",
       "      <td>True</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2017.194521</td>\n",
       "      <td>0.694521</td>\n",
       "      <td>11</td>\n",
       "      <td>16-17</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-10 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17VR005325</td>\n",
       "      <td>HA</td>\n",
       "      <td>HA</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1232.454681</td>\n",
       "      <td>519.545319</td>\n",
       "      <td>720.0</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>28.43</td>\n",
       "      <td>True</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2017.194521</td>\n",
       "      <td>0.694521</td>\n",
       "      <td>11</td>\n",
       "      <td>16-17</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-10 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17VR005325</td>\n",
       "      <td>M1</td>\n",
       "      <td>MP</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>522.189091</td>\n",
       "      <td>221.810909</td>\n",
       "      <td>720.0</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>28.43</td>\n",
       "      <td>True</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2017.194521</td>\n",
       "      <td>0.694521</td>\n",
       "      <td>11</td>\n",
       "      <td>16-17</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-10 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17VR005325</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>981.841409</td>\n",
       "      <td>416.158591</td>\n",
       "      <td>720.0</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>28.43</td>\n",
       "      <td>True</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2017.194521</td>\n",
       "      <td>0.694521</td>\n",
       "      <td>11</td>\n",
       "      <td>16-17</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-10 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17VR005325</td>\n",
       "      <td>NB</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>214.031674</td>\n",
       "      <td>85.968326</td>\n",
       "      <td>720.0</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>28.43</td>\n",
       "      <td>True</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2017.194521</td>\n",
       "      <td>0.694521</td>\n",
       "      <td>11</td>\n",
       "      <td>16-17</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-10 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4614</th>\n",
       "      <td>4614</td>\n",
       "      <td>19VR009188</td>\n",
       "      <td>PA</td>\n",
       "      <td>PA</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1553.079849</td>\n",
       "      <td>597.920151</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>H3N2</td>\n",
       "      <td>31.45</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>2019.361644</td>\n",
       "      <td>0.861644</td>\n",
       "      <td>20</td>\n",
       "      <td>18-19</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-05-09 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3c3.A</td>\n",
       "      <td>2a</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.9855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4615</th>\n",
       "      <td>4615</td>\n",
       "      <td>19VR009188</td>\n",
       "      <td>PA-X</td>\n",
       "      <td>PA</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>544.604849</td>\n",
       "      <td>214.395151</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>H3N2</td>\n",
       "      <td>31.45</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>2019.361644</td>\n",
       "      <td>0.861644</td>\n",
       "      <td>20</td>\n",
       "      <td>18-19</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-05-09 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3c3.A</td>\n",
       "      <td>2a</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.9855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4616</th>\n",
       "      <td>4616</td>\n",
       "      <td>19VR009188</td>\n",
       "      <td>PB1</td>\n",
       "      <td>PB1</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1639.537874</td>\n",
       "      <td>631.462126</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>H3N2</td>\n",
       "      <td>31.45</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>2019.361644</td>\n",
       "      <td>0.861644</td>\n",
       "      <td>20</td>\n",
       "      <td>18-19</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-05-09 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3c3.A</td>\n",
       "      <td>2a</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.9855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4617</th>\n",
       "      <td>4617</td>\n",
       "      <td>19VR009188</td>\n",
       "      <td>PB1-F2</td>\n",
       "      <td>PB1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>198.094983</td>\n",
       "      <td>71.905017</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>H3N2</td>\n",
       "      <td>31.45</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>2019.361644</td>\n",
       "      <td>0.861644</td>\n",
       "      <td>20</td>\n",
       "      <td>18-19</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-05-09 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3c3.A</td>\n",
       "      <td>2a</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.9855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4618</th>\n",
       "      <td>4618</td>\n",
       "      <td>19VR009188</td>\n",
       "      <td>PB2</td>\n",
       "      <td>PB2</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>1629.513731</td>\n",
       "      <td>647.486269</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>H3N2</td>\n",
       "      <td>31.45</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>2019.361644</td>\n",
       "      <td>0.861644</td>\n",
       "      <td>20</td>\n",
       "      <td>18-19</td>\n",
       "      <td>Day 0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-05-09 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3c3.A</td>\n",
       "      <td>2a</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.9855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4619 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index    sampleID product segment   pi_gene  piN_gene  piS_gene  \\\n",
       "0         0  17VR005325     BM2      MP  0.000115  0.000000  0.000449   \n",
       "1         1  17VR005325      HA      HA  0.000000  0.000000  0.000000   \n",
       "2         2  17VR005325      M1      MP  0.000067  0.000096  0.000000   \n",
       "3         3  17VR005325      NA      NA  0.000026  0.000000  0.000087   \n",
       "4         4  17VR005325      NB      NA  0.000000  0.000000  0.000000   \n",
       "...     ...         ...     ...     ...       ...       ...       ...   \n",
       "4614   4614  19VR009188      PA      PA  0.000120  0.000166  0.000000   \n",
       "4615   4615  19VR009188    PA-X      PA  0.000000  0.000000  0.000000   \n",
       "4616   4616  19VR009188     PB1     PB1  0.000055  0.000076  0.000000   \n",
       "4617   4617  19VR009188  PB1-F2     PB1  0.000000  0.000000  0.000000   \n",
       "4618   4618  19VR009188     PB2     PB2  0.000207  0.000268  0.000054   \n",
       "\n",
       "      N_sites_gene  S_sites_gene    ptID      subtype     CT  mapped  \\\n",
       "0       242.179553     84.820447   720.0  Influenza B  28.43    True   \n",
       "1      1232.454681    519.545319   720.0  Influenza B  28.43    True   \n",
       "2       522.189091    221.810909   720.0  Influenza B  28.43    True   \n",
       "3       981.841409    416.158591   720.0  Influenza B  28.43    True   \n",
       "4       214.031674     85.968326   720.0  Influenza B  28.43    True   \n",
       "...            ...           ...     ...          ...    ...     ...   \n",
       "4614   1553.079849    597.920151  1910.1         H3N2  31.45    True   \n",
       "4615    544.604849    214.395151  1910.1         H3N2  31.45    True   \n",
       "4616   1639.537874    631.462126  1910.1         H3N2  31.45    True   \n",
       "4617    198.094983     71.905017  1910.1         H3N2  31.45    True   \n",
       "4618   1629.513731    647.486269  1910.1         H3N2  31.45    True   \n",
       "\n",
       "     sample_date  decimalDate  seasonalDate  week season day0_or_day7_sample  \\\n",
       "0     2017-03-13  2017.194521      0.694521    11  16-17               Day 0   \n",
       "1     2017-03-13  2017.194521      0.694521    11  16-17               Day 0   \n",
       "2     2017-03-13  2017.194521      0.694521    11  16-17               Day 0   \n",
       "3     2017-03-13  2017.194521      0.694521    11  16-17               Day 0   \n",
       "4     2017-03-13  2017.194521      0.694521    11  16-17               Day 0   \n",
       "...          ...          ...           ...   ...    ...                 ...   \n",
       "4614  2019-05-13  2019.361644      0.861644    20  18-19               Day 0   \n",
       "4615  2019-05-13  2019.361644      0.861644    20  18-19               Day 0   \n",
       "4616  2019-05-13  2019.361644      0.861644    20  18-19               Day 0   \n",
       "4617  2019-05-13  2019.361644      0.861644    20  18-19               Day 0   \n",
       "4618  2019-05-13  2019.361644      0.861644    20  18-19               Day 0   \n",
       "\n",
       "      household  participant_ID   age  gender time_of_symptom_onset  \\\n",
       "0         720.0           720.0  12.0     1.0   2017-03-10 18:00:00   \n",
       "1         720.0           720.0  12.0     1.0   2017-03-10 18:00:00   \n",
       "2         720.0           720.0  12.0     1.0   2017-03-10 18:00:00   \n",
       "3         720.0           720.0  12.0     1.0   2017-03-10 18:00:00   \n",
       "4         720.0           720.0  12.0     1.0   2017-03-10 18:00:00   \n",
       "...         ...             ...   ...     ...                   ...   \n",
       "4614     1910.0          1910.1  45.0     2.0   2019-05-09 00:00:00   \n",
       "4615     1910.0          1910.1  45.0     2.0   2019-05-09 00:00:00   \n",
       "4616     1910.0          1910.1  45.0     2.0   2019-05-09 00:00:00   \n",
       "4617     1910.0          1910.1  45.0     2.0   2019-05-09 00:00:00   \n",
       "4618     1910.0          1910.1  45.0     2.0   2019-05-09 00:00:00   \n",
       "\n",
       "      recieved_flu_vaccine  school  jackson_score  antipyretic_use  \\\n",
       "0                      1.0     3.0            8.0              0.0   \n",
       "1                      1.0     3.0            8.0              0.0   \n",
       "2                      1.0     3.0            8.0              0.0   \n",
       "3                      1.0     3.0            8.0              0.0   \n",
       "4                      1.0     3.0            8.0              0.0   \n",
       "...                    ...     ...            ...              ...   \n",
       "4614                   1.0     NaN            NaN              NaN   \n",
       "4615                   1.0     NaN            NaN              NaN   \n",
       "4616                   1.0     NaN            NaN              NaN   \n",
       "4617                   1.0     NaN            NaN              NaN   \n",
       "4618                   1.0     NaN            NaN              NaN   \n",
       "\n",
       "      flu_antiviral_treatment  school_absence  number_in_household  \\\n",
       "0                         0.0             1.0                  4.0   \n",
       "1                         0.0             1.0                  4.0   \n",
       "2                         0.0             1.0                  4.0   \n",
       "3                         0.0             1.0                  4.0   \n",
       "4                         0.0             1.0                  4.0   \n",
       "...                       ...             ...                  ...   \n",
       "4614                      NaN             NaN                  NaN   \n",
       "4615                      NaN             NaN                  NaN   \n",
       "4616                      NaN             NaN                  NaN   \n",
       "4617                      NaN             NaN                  NaN   \n",
       "4618                      NaN             NaN                  NaN   \n",
       "\n",
       "      recent_travel  isIndexCase  %ofHHinfected age_category  \\\n",
       "0               0.0         True       0.333333  18 or Under   \n",
       "1               0.0         True       0.333333  18 or Under   \n",
       "2               0.0         True       0.333333  18 or Under   \n",
       "3               0.0         True       0.333333  18 or Under   \n",
       "4               0.0         True       0.333333  18 or Under   \n",
       "...             ...          ...            ...          ...   \n",
       "4614            NaN        False            NaN      Over 18   \n",
       "4615            NaN        False            NaN      Over 18   \n",
       "4616            NaN        False            NaN      Over 18   \n",
       "4617            NaN        False            NaN      Over 18   \n",
       "4618            NaN        False            NaN      Over 18   \n",
       "\n",
       "      days_since_symptom_onset  symptom_severity  saw_a_doctor  \\\n",
       "0                          3.0               2.0           0.0   \n",
       "1                          3.0               2.0           0.0   \n",
       "2                          3.0               2.0           0.0   \n",
       "3                          3.0               2.0           0.0   \n",
       "4                          3.0               2.0           0.0   \n",
       "...                        ...               ...           ...   \n",
       "4614                       4.0               3.0           1.0   \n",
       "4615                       4.0               3.0           1.0   \n",
       "4616                       4.0               3.0           1.0   \n",
       "4617                       4.0               3.0           1.0   \n",
       "4618                       4.0               3.0           1.0   \n",
       "\n",
       "      taken_on_hh_infection_day  clade subclade  num_of_snps  \\\n",
       "0                             0  1088G        9         17.0   \n",
       "1                             0  1088G        9         17.0   \n",
       "2                             0  1088G        9         17.0   \n",
       "3                             0  1088G        9         17.0   \n",
       "4                             0  1088G        9         17.0   \n",
       "...                         ...    ...      ...          ...   \n",
       "4614                          0  3c3.A       2a         27.0   \n",
       "4615                          0  3c3.A       2a         27.0   \n",
       "4616                          0  3c3.A       2a         27.0   \n",
       "4617                          0  3c3.A       2a         27.0   \n",
       "4618                          0  3c3.A       2a         27.0   \n",
       "\n",
       "      num_of_mixed_muts  num_of_nonsynon_muts  num_of_oof_muts  \\\n",
       "0                   0.0                  11.0              0.0   \n",
       "1                   0.0                  11.0              0.0   \n",
       "2                   0.0                  11.0              0.0   \n",
       "3                   0.0                  11.0              0.0   \n",
       "4                   0.0                  11.0              0.0   \n",
       "...                 ...                   ...              ...   \n",
       "4614                0.0                  18.0              0.0   \n",
       "4615                0.0                  18.0              0.0   \n",
       "4616                0.0                  18.0              0.0   \n",
       "4617                0.0                  18.0              0.0   \n",
       "4618                0.0                  18.0              0.0   \n",
       "\n",
       "      num_of_synon_muts  Xue_divergence  Xue_mixed_divergence  \\\n",
       "0                   6.0          0.2395                   0.0   \n",
       "1                   6.0          0.2395                   0.0   \n",
       "2                   6.0          0.2395                   0.0   \n",
       "3                   6.0          0.2395                   0.0   \n",
       "4                   6.0          0.2395                   0.0   \n",
       "...                 ...             ...                   ...   \n",
       "4614                9.0          1.9855                   0.0   \n",
       "4615                9.0          1.9855                   0.0   \n",
       "4616                9.0          1.9855                   0.0   \n",
       "4617                9.0          1.9855                   0.0   \n",
       "4618                9.0          1.9855                   0.0   \n",
       "\n",
       "      Xue_nonsynon_divergence  Xue_oof_divergence  Xue_synon_divergence  \n",
       "0                      0.1342                 0.0                0.1053  \n",
       "1                      0.1342                 0.0                0.1053  \n",
       "2                      0.1342                 0.0                0.1053  \n",
       "3                      0.1342                 0.0                0.1053  \n",
       "4                      0.1342                 0.0                0.1053  \n",
       "...                       ...                 ...                   ...  \n",
       "4614                   1.4783                 0.0                0.5072  \n",
       "4615                   1.4783                 0.0                0.5072  \n",
       "4616                   1.4783                 0.0                0.5072  \n",
       "4617                   1.4783                 0.0                0.5072  \n",
       "4618                   1.4783                 0.0                0.5072  \n",
       "\n",
       "[4619 rows x 51 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'piN_sample_y' in samples.columns:\n",
    "    raise Exception\n",
    "geneDiversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useSNPGenie2:\n",
    "    geneDiversity = geneDiversity[['sampleID','segment','product','piN_gene','piS_gene', 'N_sites_gene','S_sites_gene']]\n",
    "else:\n",
    "    geneDiversity = geneDiversity[['sampleID','segment','product','piN_gene','piS_gene','N_sites','S_sites']].rename(columns = {'piN':'piN_gene', 'piS':'piS_gene', 'N_sites':'N_sites_gene', 'S_sites':'S_sites_gene'})\n",
    "genes = geneDiversity.merge(segments, on=['sampleID', 'segment'], how='left')\n",
    "\n",
    "# slidingWindowDiversity = slidingWindowDiversity[['sampleID', 'subtype','segment', 'product','piN', 'piS', 'inGenePos','codon', 'windowSize']].rename(columns={\n",
    "#     'piN':'piN_sliding_window','piS':'piS_sliding_window'})\n",
    "\n",
    "codons = slidingWindowDiversity.merge(genes, on=['sampleID','segment','product', 'subtype'], how='left')\n",
    "codons['codon'] = codons.codon.replace('',np.nan).astype(float)\n",
    "codons['inGenePos'] = codons.inGenePos.replace('',np.nan).astype(float)\n",
    "\n",
    "if not useSNPGenie2:\n",
    "    siteDiversity = siteDiversity[['sampleID','segment','product','site','pi','class']].rename(columns={'site':'pos','pi':'pi_nucleotide','class':'class_of_mutation_at_site'})\n",
    "else:\n",
    "    siteDiversity = siteDiversity[['sampleID','segment','product','inSegPos','pi','AAtype']].rename(columns={'inSegPos':'pos','pi':'pi_nucleotide','AAtype':'class_of_mutation_at_site'})\n",
    "#     siteDiversity = siteDiversity.rename(columns={'inSegPos':'pos'})\n",
    "#     siteDiversity.pos = siteDiversity.pos.astype(int)\n",
    "SNPs = SNPs.merge(siteDiversity, on=['sampleID','segment','product', 'pos'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'piN_sample_y' in samples.columns:\n",
    "    raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(SNPs) == origNumSNPs\n",
    "assert len(SNPs.snpKey.unique())==len(SNPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the genes df has been created, go back and calculate SNP counts/divergence in each gene\n",
    "inRangeSNPs = SNPs.loc[SNPs.transformed_frequency > SNP_frequency_cutoff]\n",
    "geneCounts = inRangeSNPs.groupby(['sampleID','product']).count()['pos'].reset_index().rename(columns={'pos':'num_of_snps_gene'}).replace(np.nan,0)\n",
    "geneSynonCounts = inRangeSNPs.groupby(['sampleID','product','AAtype']).count().pivot_table(index=['sampleID','product'], columns='AAtype')['ABQ'].replace(np.nan, 0).rename(columns={'Nonsynonymous':'num_of_nonsynon_muts_gene','Synonymous':'num_of_synon_muts_gene'}).reset_index()\n",
    "geneDivergence = inRangeSNPs.groupby(['sampleID','product']).sum()['transformed_frequency'].reset_index().rename(columns={'transformed_frequency':'Xue_divergence_gene'}).replace(np.nan,0)\n",
    "geneSynonDivergence = inRangeSNPs.groupby(['sampleID', 'product','AAtype']).sum()['transformed_frequency'].reset_index().pivot_table(index=['sampleID','product'], columns='AAtype')['transformed_frequency'].replace(np.nan, 0).rename(columns={'Nonsynonymous':'Xue_nonsynon_divergence_gene','Synonymous':'Xue_synon_divergence_gene','Out of reading frame':'Xue_oof_divergence_gene','Mixed':'Xue_mixed_divergence_gene'}).reset_index()\n",
    "\n",
    "genes = genes.merge(geneCounts, on=['sampleID', 'product'], how='left')\n",
    "genes = genes.merge(geneSynonCounts, on=['sampleID', 'product'], how='left')\n",
    "genes = genes.merge(geneDivergence, on=['sampleID', 'product'], how='left')\n",
    "genes = genes.merge(geneSynonDivergence, on=['sampleID', 'product'], how='left')\n",
    "\n",
    "newly_added_columns = ['num_of_snps_gene','num_of_nonsynon_muts_gene', 'num_of_synon_muts_gene',\n",
    "                       'Xue_divergence_gene', 'Xue_nonsynon_divergence_gene', 'Xue_synon_divergence_gene']\n",
    "genes[newly_added_columns] = genes[newly_added_columns].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 442 in H3N2\n",
    "# 297 in H1N1 - only 150 entries for 'HA', 8 for 'HA_antigenic', 133 for HA_nonantigenic, 6 OOF\n",
    "# 150+8+133 +6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add number of synon/nonsynon sites per segment/gene\n",
    "# import sys\n",
    "# sys.path.append('/mnt/d/SNPGenie2')\n",
    "# from SNP_genie2 import *\n",
    "# import importlib\n",
    "# import SNP_genie2\n",
    "# importlib.reload(SNP_genie2)\n",
    "# import SNP_genie2\n",
    "# from SNP_genie2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.replace('H1N1pdm', 'H1N1')\n",
    "segments = segments.replace('H1N1pdm', 'H1N1')\n",
    "genes = genes.replace('H1N1pdm', 'H1N1')\n",
    "SNPs = SNPs.replace('H1N1pdm', 'H1N1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sampleID', 'segment', 'pi_segment', 'piN_segment', 'piS_segment',\n",
       "       'ptID', 'subtype', 'CT', 'mapped', 'sample_date', 'decimalDate',\n",
       "       'seasonalDate', 'week', 'season', 'day0_or_day7_sample', 'household',\n",
       "       'participant_ID', 'age', 'gender', 'time_of_symptom_onset',\n",
       "       'recieved_flu_vaccine', 'school', 'jackson_score', 'antipyretic_use',\n",
       "       'flu_antiviral_treatment', 'school_absence', 'number_in_household',\n",
       "       'recent_travel', 'isIndexCase', '%ofHHinfected', 'age_category',\n",
       "       'days_since_symptom_onset', 'symptom_severity', 'saw_a_doctor',\n",
       "       'taken_on_hh_infection_day', 'clade', 'subclade', 'num_of_snps',\n",
       "       'num_of_mixed_muts', 'num_of_nonsynon_muts', 'num_of_oof_muts',\n",
       "       'num_of_synon_muts', 'Xue_divergence', 'Xue_mixed_divergence',\n",
       "       'Xue_nonsynon_divergence', 'Xue_oof_divergence', 'Xue_synon_divergence',\n",
       "       'pi_sample', 'piN_sites', 'piS_sites', 'piN_sample', 'piS_sample',\n",
       "       'num_of_snps_segment', 'num_of_mixed_muts_segment',\n",
       "       'num_of_nonsynon_muts_segment', 'num_of_oof_muts_segment',\n",
       "       'num_of_synon_muts_segment', 'Xue_divergence_segment',\n",
       "       'Xue_mixed_divergence_segment', 'Xue_nonsynon_divergence_segment',\n",
       "       'Xue_oof_divergence_segment', 'Xue_synon_divergence_segment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genes.loc[genes['product'].isin(['HA_antigenic'])].num_of_muts_gene.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/mnt/d/SNPGenie2')\n",
    "import SNPGenie2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(SNPGenie2)\n",
    "# import SNPGenie2\n",
    "# from SNPGenie2 import *\n",
    "# num_N_sites_seg = pd.DataFrame()\n",
    "# num_S_sites_seg = pd.DataFrame()\n",
    "# num_N_sites_gene = pd.DataFrame()\n",
    "# num_S_sites_gene = pd.DataFrame()\n",
    "# lookout=False\n",
    "# for subtype, seasondict in refFileDict.items():\n",
    "#     for season, refFile in seasondict.items():\n",
    "#         if subtype == 'Influenza B':\n",
    "#             MPgenes = ['M1', 'BM2']\n",
    "#             NAgenes = ['NA', 'NB']\n",
    "#         else:\n",
    "#             MPgenes = ['M1','M2']\n",
    "#             NAgenes = ['NA']\n",
    "#         refFile = refFileDict[subtype][season]\n",
    "#         gtfFile = gtfFileDict[subtype][season]\n",
    "#         print (season, subtype)\n",
    "#         samplesToCalc = samples.loc[(samples.season==season)&(samples.subtype==subtype),'sampleID'].to_list()\n",
    "        \n",
    "#         #in case a sample has no SNPs, be sure one blank line per sample is included\n",
    "#         tocalc = SNPs.loc[SNPs['sampleID'].isin(samplesToCalc)]\n",
    "#         tocalc = pd.DataFrame(samplesToCalc, columns=['sampleID']).merge(tocalc, on='sampleID',how='left')\n",
    "        \n",
    "#         tempnum_N_sites_seg, tempnum_S_sites_seg, _ = calc_synon_sites(tocalc, refseq=refFile, gtf=gtfFile, remove_overlaps=True)\n",
    "#         tempnum_N_sites_gene, tempnum_S_sites_gene, _ = calc_synon_sites(tocalc, refseq=refFile, gtf=gtfFile)\n",
    "        \n",
    "#         tempnum_N_sites_seg['MP'] = tempnum_N_sites_seg[MPgenes].sum(axis=1)\n",
    "#         tempnum_N_sites_seg['NS'] = tempnum_N_sites_seg[['NS1','NEP']].sum(axis=1)\n",
    "#         tempnum_N_sites_seg['NA'] = tempnum_N_sites_seg[NAgenes].sum(axis=1)\n",
    "#         tempnum_N_sites_seg = tempnum_N_sites_seg[['PB1','PB2','NA','NS','NP','HA','MP','PA']]\n",
    "#         tempnum_N_sites_seg = tempnum_N_sites_seg.reset_index().melt(id_vars = 'index', var_name='segment',value_name='num_N_sites_seg')\n",
    "#         tempnum_N_sites_seg = tempnum_N_sites_seg.rename(columns={'index':'sampleID'})\n",
    "#         tempnum_N_sites_seg['subtype'] = subtype\n",
    "#         tempnum_N_sites_seg['season'] = season\n",
    "#         num_N_sites_seg = num_N_sites_seg.append(tempnum_N_sites_seg)\n",
    "        \n",
    "#         tempnum_N_sites_gene = tempnum_N_sites_gene.reset_index().melt(id_vars = 'index', var_name='gene',value_name='num_N_sites_gene')\n",
    "#         tempnum_N_sites_gene = tempnum_N_sites_gene.rename(columns={'index':'sampleID'})\n",
    "#         tempnum_N_sites_gene['subtype'] = subtype\n",
    "#         tempnum_N_sites_gene['season'] = season\n",
    "#         num_N_sites_gene = num_N_sites_gene.append(tempnum_N_sites_gene)\n",
    "\n",
    "#         tempnum_S_sites_seg['MP'] = tempnum_S_sites_seg[MPgenes].sum(axis=1)\n",
    "#         tempnum_S_sites_seg['NS'] = tempnum_S_sites_seg[['NS1','NEP']].sum(axis=1)\n",
    "#         tempnum_S_sites_seg['NA'] = tempnum_S_sites_seg[NAgenes].sum(axis=1)\n",
    "#         tempnum_S_sites_seg = tempnum_S_sites_seg[['PB1','PB2','NA','NS','NP','HA','MP','PA']]\n",
    "#         tempnum_S_sites_seg = tempnum_S_sites_seg.reset_index().melt(id_vars = 'index', var_name='segment',value_name='num_S_sites_seg')\n",
    "#         tempnum_S_sites_seg = tempnum_S_sites_seg.rename(columns={'index':'sampleID'})\n",
    "#         tempnum_S_sites_seg['subtype'] = subtype\n",
    "#         tempnum_S_sites_seg['season'] = season\n",
    "#         num_S_sites_seg = num_S_sites_seg.append(tempnum_S_sites_seg)\n",
    "        \n",
    "#         tempnum_S_sites_gene = tempnum_S_sites_gene.reset_index().melt(id_vars = 'index', var_name='gene',value_name='num_S_sites_gene')\n",
    "#         tempnum_S_sites_gene = tempnum_S_sites_gene.rename(columns={'index':'sampleID'})\n",
    "#         tempnum_S_sites_gene['subtype'] = subtype\n",
    "#         tempnum_S_sites_gene['season'] = season\n",
    "#         num_S_sites_gene = num_S_sites_gene.append(tempnum_S_sites_gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BM2', 'HA', 'M1', 'NA', 'NB', 'NEP', 'NP', 'NS1', 'PA', 'PB1',\n",
       "       'PB2', 'HA_antigenic', 'HA_nonantigenic', 'M2', 'PA-X', 'PB1-F2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(num_N_sites_samp.reset_index().groupby('subtype').mean())\n",
    "# print(samples[['subtype','piN_sites']].groupby('subtype').mean())\n",
    "genes['product'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(num_N_sites_gene.loc[num_N_sites_gene.subtype=='Influenza B'].reset_index().groupby('gene').mean())\n",
    "# print(genes.loc[genes.subtype=='Influenza B', ['product','piN_sites']].groupby('product').mean())\n",
    "# '''SNP2: HA=403 S, 1352 N\n",
    "#    SNP1: HA=418.5 S, 1333.5 N\n",
    "# '''\n",
    "# SNPs.loc[(SNPs.sampleID=='18VR001131')&(SNPs.chrom=='B_Phuket_3073_2013_HA')].sort_values('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_N_sites_samp = num_N_sites_seg.groupby(['sampleID','subtype','season']).sum().rename(columns={'num_N_sites_seg':'num_N_sites_samp'})\n",
    "# num_S_sites_samp = num_S_sites_seg.groupby(['sampleID','subtype','season']).sum().rename(columns={'num_S_sites_seg':'num_S_sites_samp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_sites = num_N_sites_samp.merge(num_S_sites_samp, on=['sampleID','subtype','season'])\n",
    "# seg_sites = num_N_sites_seg.merge(num_S_sites_seg, on=['sampleID','segment', 'subtype','season'])\n",
    "# gene_sites = num_N_sites_gene.merge(num_S_sites_gene, on=['sampleID','gene', 'subtype','season']).rename(columns={'gene':'product'})\n",
    "\n",
    "# samples = samples.merge(samp_sites, on = ['sampleID','subtype','season'])\n",
    "# segments = segments.merge(seg_sites, on=['sampleID','segment', 'subtype','season'], how='left')\n",
    "# genes = genes.merge(gene_sites, on=['sampleID','product', 'subtype','season'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segments[['num_N_sites_seg','num_S_sites_seg']] = segments[['num_N_sites_seg','num_S_sites_seg']].replace(np.nan,0)\n",
    "# genes[['num_N_sites_gene','num_S_sites_gene']] = genes[['num_N_sites_gene','num_S_sites_gene']].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>pos</th>\n",
       "      <th>inGenePos</th>\n",
       "      <th>ref_nuc</th>\n",
       "      <th>alt_nuc</th>\n",
       "      <th>codon</th>\n",
       "      <th>depth</th>\n",
       "      <th>AD</th>\n",
       "      <th>RD</th>\n",
       "      <th>SNP_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>25</td>\n",
       "      <td>6.0</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>197</td>\n",
       "      <td>23</td>\n",
       "      <td>174</td>\n",
       "      <td>0.1168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>60</td>\n",
       "      <td>41.0</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>13.0</td>\n",
       "      <td>547</td>\n",
       "      <td>547</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>66</td>\n",
       "      <td>47.0</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>15.0</td>\n",
       "      <td>576</td>\n",
       "      <td>576</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>73</td>\n",
       "      <td>54.0</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>18.0</td>\n",
       "      <td>630</td>\n",
       "      <td>14</td>\n",
       "      <td>616</td>\n",
       "      <td>0.0222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>219</td>\n",
       "      <td>200.0</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1068</td>\n",
       "      <td>13</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.0122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>546</td>\n",
       "      <td>527.0</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>175.0</td>\n",
       "      <td>987</td>\n",
       "      <td>986</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>558</td>\n",
       "      <td>539.0</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>179.0</td>\n",
       "      <td>981</td>\n",
       "      <td>981</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>600</td>\n",
       "      <td>581.0</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1008</td>\n",
       "      <td>1008</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>631</td>\n",
       "      <td>612.0</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>204.0</td>\n",
       "      <td>968</td>\n",
       "      <td>35</td>\n",
       "      <td>933</td>\n",
       "      <td>0.0362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>711</td>\n",
       "      <td>692.0</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1016</td>\n",
       "      <td>1015</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>960</td>\n",
       "      <td>941.0</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>313.0</td>\n",
       "      <td>951</td>\n",
       "      <td>951</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>1095</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>358.0</td>\n",
       "      <td>946</td>\n",
       "      <td>943</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>1599</td>\n",
       "      <td>1580.0</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>526.0</td>\n",
       "      <td>1327</td>\n",
       "      <td>32</td>\n",
       "      <td>1295</td>\n",
       "      <td>0.0241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>1678</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>553.0</td>\n",
       "      <td>1065</td>\n",
       "      <td>18</td>\n",
       "      <td>1047</td>\n",
       "      <td>0.0169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>1799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>580</td>\n",
       "      <td>25</td>\n",
       "      <td>555</td>\n",
       "      <td>0.0431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      chrom   pos  inGenePos ref_nuc alt_nuc  codon  depth  \\\n",
       "2784  B_Phuket_3073_2013_HA    25        6.0       G       A    2.0    197   \n",
       "2801  B_Phuket_3073_2013_HA    60       41.0       C       T   13.0    547   \n",
       "2821  B_Phuket_3073_2013_HA    66       47.0       T       C   15.0    576   \n",
       "2751  B_Phuket_3073_2013_HA    73       54.0       T       C   18.0    630   \n",
       "2764  B_Phuket_3073_2013_HA   219      200.0       A       C   66.0   1068   \n",
       "2769  B_Phuket_3073_2013_HA   546      527.0       A       G  175.0    987   \n",
       "2887  B_Phuket_3073_2013_HA   558      539.0       C       T  179.0    981   \n",
       "2885  B_Phuket_3073_2013_HA   600      581.0       T       C  193.0   1008   \n",
       "2883  B_Phuket_3073_2013_HA   631      612.0       T       C  204.0    968   \n",
       "2881  B_Phuket_3073_2013_HA   711      692.0       T       C  230.0   1016   \n",
       "2771  B_Phuket_3073_2013_HA   960      941.0       T       C  313.0    951   \n",
       "2772  B_Phuket_3073_2013_HA  1095     1076.0       G       A  358.0    946   \n",
       "2746  B_Phuket_3073_2013_HA  1599     1580.0       T       C  526.0   1327   \n",
       "2757  B_Phuket_3073_2013_HA  1678     1659.0       A       G  553.0   1065   \n",
       "2770  B_Phuket_3073_2013_HA  1799        NaN       T       C    NaN    580   \n",
       "\n",
       "        AD    RD  SNP_frequency  \n",
       "2784    23   174         0.1168  \n",
       "2801   547     0         1.0000  \n",
       "2821   576     0         1.0000  \n",
       "2751    14   616         0.0222  \n",
       "2764    13  1055         0.0122  \n",
       "2769   986     1         0.9990  \n",
       "2887   981     0         1.0000  \n",
       "2885  1008     0         1.0000  \n",
       "2883    35   933         0.0362  \n",
       "2881  1015     1         0.9990  \n",
       "2771   951     0         1.0000  \n",
       "2772   943     3         0.9968  \n",
       "2746    32  1295         0.0241  \n",
       "2757    18  1047         0.0169  \n",
       "2770    25   555         0.0431  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNPs.loc[(SNPs.chrom == 'B_Phuket_3073_2013_HA')&(SNPs.sampleID=='18VR001131'),['chrom','pos','inGenePos','ref_nuc','alt_nuc','codon','depth','AD','RD', 'SNP_frequency']].sort_values('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in SNPs.columns if 'sites' in c]\n",
    "# segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useSNPGenie2:\n",
    "    samples = samples.rename(columns={'piS_sites':'num_S_sites_samp','piN_sites':'num_N_sites_samp'})\n",
    "    segments = segments.rename(columns={'piS_sites':'num_S_sites_samp','piN_sites':'num_N_sites_samp'})\n",
    "    genes = genes.rename(columns={'piS_sites':'num_S_sites_samp','piN_sites':'num_N_sites_samp'})\n",
    "    segments['num_S_sites_seg'] = segments['num_N_sites_seg'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calculate mutation rate, Xue divergence rate for all samples\n",
    "#Samples:\n",
    "incubation_time={'H3N2': 1.4, 'H1N1':1.4, 'Influenza B':0.6}\n",
    "average_incubation_time = 1.4 #days\n",
    "# Per paper: Incubation periods of acute respiratory viral infections: a systematic review\n",
    "\n",
    "#Calculating sample mutation rates\n",
    "subtypeLengths = {'H3N2':13427, 'H1N1':13434, 'Influenza B':13705, 'H1N1pdm':13434}\n",
    "samples['snps_per_day_samp'] = samples.num_of_snps/(samples.days_since_symptom_onset+average_incubation_time)\n",
    "samples['synon_snps_per_day_samp'] = samples.num_of_synon_muts/(samples.days_since_symptom_onset+average_incubation_time)\n",
    "samples['nonsynon_snps_per_day_samp'] = samples.num_of_nonsynon_muts/(samples.days_since_symptom_onset+average_incubation_time)\n",
    "samples['oof_snps_per_day_samp'] = samples.num_of_oof_muts/(samples.days_since_symptom_onset+average_incubation_time)\n",
    "samples.loc[samples.snps_per_day_samp < 0, ['snps_per_day_samp', 'synon_snps_per_day_samp','nonsynon_snps_per_day_samp','oof_snps_per_day_samp']] = np.nan\n",
    "\n",
    "samples['subtypeLengths'] = samples.subtype.map(subtypeLengths)\n",
    "samples['mutation_rate_samp'] = samples.snps_per_day_samp/samples.subtypeLengths\n",
    "samples['synon_mutation_rate_samp'] = samples.synon_snps_per_day_samp/samples.num_S_sites_samp\n",
    "samples['nonsynon_mutation_rate_samp'] = samples.nonsynon_snps_per_day_samp/samples.num_N_sites_samp\n",
    "samples.loc[samples.mutation_rate_samp < 0,['mutation_rate_samp','synon_mutation_rate_samp','nonsynon_mutation_rate_samp']] = np.nan\n",
    "            \n",
    "samples['Xue_divergence_per_day'] = samples.Xue_divergence/(samples.days_since_symptom_onset+average_incubation_time)\n",
    "samples['Xue_synon_divergence_per_day'] = samples.Xue_synon_divergence/(samples.days_since_symptom_onset+average_incubation_time)\n",
    "samples['Xue_nonsynon_divergence_per_day'] = samples.Xue_nonsynon_divergence/(samples.days_since_symptom_onset+average_incubation_time)\n",
    "samples['divergence_rate'] = samples.Xue_divergence_per_day/samples.subtypeLengths\n",
    "samples['synon_divergence_rate'] = samples.Xue_synon_divergence_per_day/samples.num_S_sites_samp\n",
    "samples['nonsynon_divergence_rate'] = samples.Xue_nonsynon_divergence_per_day/samples.num_N_sites_samp\n",
    "samples.loc[samples.divergence_rate < 0,['divergence_rate','synon_divergence_rate','nonsynon_divergence_rate']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "seglengths = {'H3N2':(2316,2316,2209,1737,1541,1441,1002,865),'H1N1pdm':(2316,2317,2208,1753,1540,1433,1002,865),'H1N1':(2316,2317,2208,1753,1540,1433,1002,865),'Influenza B':(2367,2340,2275,1853,1815,1530,1155,1062)}\n",
    "segnames = ('PB2','PB1','NP','HA','NA','PA','MP','NS')\n",
    "seglengths = {key:dict(zip(segnames, value)) for key, value in seglengths.items()}\n",
    "segments['seglength'] = segments.apply(lambda x: seglengths[x.subtype][x.segment], axis=1)\n",
    "\n",
    "segments['snps_per_day_seg'] = segments.num_of_snps_segment/(segments.days_since_symptom_onset+average_incubation_time)\n",
    "segments['nonsynon_snps_per_day_seg'] = segments.num_of_nonsynon_muts_segment/(segments.days_since_symptom_onset+average_incubation_time)\n",
    "segments['synon_snps_per_day_seg'] = segments.num_of_synon_muts_segment/(segments.days_since_symptom_onset+average_incubation_time)\n",
    "segments.loc[segments.snps_per_day_seg < 0,['snps_per_day_seg', 'nonsynon_snps_per_day_seg','synon_snps_per_day_seg']] = np.nan\n",
    "\n",
    "segments['mutation_rate_seg'] = segments.snps_per_day_seg/segments.seglength\n",
    "segments['nonsynon_mutation_rate_seg'] = segments.nonsynon_snps_per_day_seg/segments.num_N_sites_seg\n",
    "segments['synon_mutation_rate_seg'] = segments.synon_snps_per_day_seg/segments.num_S_sites_seg\n",
    "segments.loc[segments.mutation_rate_seg < 0,['mutation_rate_seg', 'nonsynon_mutation_rate_seg','synon_mutation_rate_seg']] = np.nan\n",
    "\n",
    "segments['divergence_per_day_seg'] = segments.Xue_divergence_segment/(segments.days_since_symptom_onset+average_incubation_time)\n",
    "segments['nonsynon_divergence_per_day_seg'] = segments.Xue_nonsynon_divergence_segment/(segments.days_since_symptom_onset+average_incubation_time)\n",
    "segments['synon_divergence_per_day_seg'] = segments.Xue_synon_divergence_segment/(segments.days_since_symptom_onset+average_incubation_time)\n",
    "segments.loc[segments.snps_per_day_seg < 0,['divergence_per_day_seg', 'nonsynon_divergence_per_day_seg','synon_divergence_per_day_seg']] = np.nan\n",
    "\n",
    "segments['divergence_rate_seg'] = segments.divergence_per_day_seg/segments.seglength\n",
    "segments['nonsynon_divergence_rate_seg'] = segments.nonsynon_divergence_per_day_seg/segments.num_N_sites_seg\n",
    "segments['synon_divergence_rate_seg'] = segments.synon_divergence_per_day_seg/segments.num_S_sites_seg\n",
    "segments.loc[segments.snps_per_day_seg < 0,['divergence_rate_seg', 'nonsynon_divergence_rate_seg','synon_divergence_rate_seg']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes['genelength'] = genes.apply(lambda x: genelengths[x.subtype][x['product']], axis=1)\n",
    "\n",
    "genes['snps_per_day_gene'] = genes.num_of_snps_gene/(genes.days_since_symptom_onset+average_incubation_time)\n",
    "genes['nonsynon_snps_per_day_gene'] = genes.num_of_nonsynon_muts_gene/(genes.days_since_symptom_onset+average_incubation_time)\n",
    "genes['synon_snps_per_day_gene'] = genes.num_of_synon_muts_gene/(genes.days_since_symptom_onset+average_incubation_time)\n",
    "genes.loc[(genes.days_since_symptom_onset+average_incubation_time) < 0,['snps_per_day_gene', 'nonsynon_snps_per_day_gene','synon_snps_per_day_gene']] = np.nan\n",
    "\n",
    "genes['mutation_rate_gene'] = genes.snps_per_day_gene/genes.genelength\n",
    "genes['nonsynon_mutation_rate_gene'] = genes.nonsynon_snps_per_day_gene/genes.N_sites_gene\n",
    "genes['synon_mutation_rate_gene'] = genes.synon_snps_per_day_gene/genes.S_sites_gene\n",
    "genes.loc[(genes.days_since_symptom_onset+average_incubation_time) < 0,['mutation_rate_gene', 'nonsynon_mutation_rate_gene','synon_mutation_rate_gene']] = np.nan\n",
    "\n",
    "genes['divergence_per_day_gene'] = genes.Xue_divergence_gene/(genes.days_since_symptom_onset+average_incubation_time)\n",
    "genes['nonsynon_divergence_per_day_gene'] = genes.Xue_nonsynon_divergence_gene/(genes.days_since_symptom_onset+average_incubation_time)\n",
    "genes['synon_divergence_per_day_gene'] = genes.Xue_synon_divergence_gene/(genes.days_since_symptom_onset+average_incubation_time)\n",
    "genes.loc[(genes.days_since_symptom_onset+average_incubation_time) < 0,['divergence_per_day_gene', 'nonsynon_divergence_per_day_gene','synon_divergence_per_day_gene']] = np.nan\n",
    "\n",
    "genes['divergence_rate_gene'] = genes.divergence_per_day_gene/genes.genelength\n",
    "genes['nonsynon_divergence_rate_gene'] = genes.nonsynon_divergence_per_day_gene/genes.N_sites_gene\n",
    "genes['synon_divergence_rate_gene'] = genes.synon_divergence_per_day_gene/genes.S_sites_gene\n",
    "genes.loc[(genes.days_since_symptom_onset+average_incubation_time) < 0,['divergence_rate_gene', 'nonsynon_divergence_rate_gene','synon_divergence_rate_gene']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# having the number of sites per gene is VERY helpful for fig 5 divergence/gene. These data will be incorporated into transmission SNPs\n",
    "# SNPs = SNPs.merge(genes[['sampleID','product', 'N_sites_gene','S_sites_gene']], on=['sampleID','product'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sampleID</th>\n",
       "      <th>chrom</th>\n",
       "      <th>pos</th>\n",
       "      <th>ref_nuc</th>\n",
       "      <th>alt_nuc</th>\n",
       "      <th>qual</th>\n",
       "      <th>GT</th>\n",
       "      <th>GQ</th>\n",
       "      <th>SDP</th>\n",
       "      <th>depth</th>\n",
       "      <th>RD</th>\n",
       "      <th>AD</th>\n",
       "      <th>SNP_frequency</th>\n",
       "      <th>PVAL</th>\n",
       "      <th>RBQ</th>\n",
       "      <th>ABQ</th>\n",
       "      <th>RDF</th>\n",
       "      <th>RDR</th>\n",
       "      <th>ADF</th>\n",
       "      <th>ADR</th>\n",
       "      <th>product</th>\n",
       "      <th>inGenePos</th>\n",
       "      <th>refAA</th>\n",
       "      <th>codon</th>\n",
       "      <th>altAA</th>\n",
       "      <th>AAstr</th>\n",
       "      <th>AAtype</th>\n",
       "      <th>referenceFile</th>\n",
       "      <th>segment</th>\n",
       "      <th>subtype</th>\n",
       "      <th>CT</th>\n",
       "      <th>sample_date</th>\n",
       "      <th>decimalDate</th>\n",
       "      <th>seasonalDate</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>household</th>\n",
       "      <th>participant_ID</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>time_of_symptom_onset</th>\n",
       "      <th>recieved_flu_vaccine</th>\n",
       "      <th>school</th>\n",
       "      <th>jackson_score</th>\n",
       "      <th>antipyretic_use</th>\n",
       "      <th>flu_antiviral_treatment</th>\n",
       "      <th>school_absence</th>\n",
       "      <th>number_in_household</th>\n",
       "      <th>recent_travel</th>\n",
       "      <th>isIndexCase</th>\n",
       "      <th>%ofHHinfected</th>\n",
       "      <th>age_category</th>\n",
       "      <th>days_since_symptom_onset</th>\n",
       "      <th>symptom_severity</th>\n",
       "      <th>saw_a_doctor</th>\n",
       "      <th>taken_on_hh_infection_day</th>\n",
       "      <th>clade</th>\n",
       "      <th>subclade</th>\n",
       "      <th>transformed_frequency</th>\n",
       "      <th>log_transformed_frequency</th>\n",
       "      <th>rolling_AA_mean_global_freq</th>\n",
       "      <th>rolling_AA_max_global_freq</th>\n",
       "      <th>rolling_DNA_mean_global_freq</th>\n",
       "      <th>rolling_DNA_max_global_freq</th>\n",
       "      <th>rolling_AA_mean_global_freq_following_year</th>\n",
       "      <th>rolling_AA_max_global_freq_following_year</th>\n",
       "      <th>rolling_DNA_mean_global_freq_following_year</th>\n",
       "      <th>rolling_DNA_max_global_freq_following_year</th>\n",
       "      <th>AAobservedGlobally</th>\n",
       "      <th>AAobservedGloballyFollowingSeason</th>\n",
       "      <th>DNAobservedGlobally</th>\n",
       "      <th>DNAobservedGloballyFollowingSeason</th>\n",
       "      <th>antigenic_site</th>\n",
       "      <th>antigenic_product</th>\n",
       "      <th>snpKey</th>\n",
       "      <th>pi_nucleotide</th>\n",
       "      <th>class_of_mutation_at_site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17VR005325</td>\n",
       "      <td>B_Phuket_3073_2013_MP</td>\n",
       "      <td>524</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>.</td>\n",
       "      <td>0/1</td>\n",
       "      <td>35</td>\n",
       "      <td>1554</td>\n",
       "      <td>1462</td>\n",
       "      <td>1447</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>2.513400e-04</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>876</td>\n",
       "      <td>571</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>M1</td>\n",
       "      <td>517.0</td>\n",
       "      <td>V</td>\n",
       "      <td>172.0</td>\n",
       "      <td>A</td>\n",
       "      <td>V172A</td>\n",
       "      <td>Nonsynonymous</td>\n",
       "      <td>/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_30...</td>\n",
       "      <td>MP</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>28.43</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2017.194521</td>\n",
       "      <td>0.694521</td>\n",
       "      <td>11</td>\n",
       "      <td>16-17</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-10 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>-1.987163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>M1</td>\n",
       "      <td>17VR005325:MP:524:C:M1</td>\n",
       "      <td>0.020323</td>\n",
       "      <td>Nonsynonymous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17VR005325</td>\n",
       "      <td>B_Phuket_3073_2013_PB2</td>\n",
       "      <td>1892</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>.</td>\n",
       "      <td>0/1</td>\n",
       "      <td>83</td>\n",
       "      <td>2045</td>\n",
       "      <td>2001</td>\n",
       "      <td>1966</td>\n",
       "      <td>35</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>4.481700e-09</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>890</td>\n",
       "      <td>1076</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>PB2</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>L</td>\n",
       "      <td>627.0</td>\n",
       "      <td>L</td>\n",
       "      <td>L627L</td>\n",
       "      <td>Synonymous</td>\n",
       "      <td>/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_30...</td>\n",
       "      <td>PB2</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>28.43</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2017.194521</td>\n",
       "      <td>0.694521</td>\n",
       "      <td>11</td>\n",
       "      <td>16-17</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-10 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>-1.756962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>PB2</td>\n",
       "      <td>17VR005325:PB2:1892:G:PB2</td>\n",
       "      <td>0.034388</td>\n",
       "      <td>Synonymous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17VR005325</td>\n",
       "      <td>B_Phuket_3073_2013_HA</td>\n",
       "      <td>60</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>.</td>\n",
       "      <td>1/1</td>\n",
       "      <td>255</td>\n",
       "      <td>1591</td>\n",
       "      <td>1545</td>\n",
       "      <td>1</td>\n",
       "      <td>1544</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1328</td>\n",
       "      <td>216</td>\n",
       "      <td>HA</td>\n",
       "      <td>41.0</td>\n",
       "      <td>N</td>\n",
       "      <td>13.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N13N</td>\n",
       "      <td>Synonymous</td>\n",
       "      <td>/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_30...</td>\n",
       "      <td>HA</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>28.43</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2017.194521</td>\n",
       "      <td>0.694521</td>\n",
       "      <td>11</td>\n",
       "      <td>16-17</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-10 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>-3.221849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>HA</td>\n",
       "      <td>17VR005325:HA:60:T:HA</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Synonymous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17VR005325</td>\n",
       "      <td>B_Phuket_3073_2013_PB1</td>\n",
       "      <td>1056</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>.</td>\n",
       "      <td>0/1</td>\n",
       "      <td>33</td>\n",
       "      <td>1461</td>\n",
       "      <td>1400</td>\n",
       "      <td>1386</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>4.746700e-04</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>677</td>\n",
       "      <td>709</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>PB1</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>R</td>\n",
       "      <td>349.0</td>\n",
       "      <td>R</td>\n",
       "      <td>R349R</td>\n",
       "      <td>Synonymous</td>\n",
       "      <td>/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_30...</td>\n",
       "      <td>PB1</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>28.43</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2017.194521</td>\n",
       "      <td>0.694521</td>\n",
       "      <td>11</td>\n",
       "      <td>16-17</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-10 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>PB1</td>\n",
       "      <td>17VR005325:PB1:1056:G:PB1</td>\n",
       "      <td>0.019814</td>\n",
       "      <td>Synonymous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17VR005325</td>\n",
       "      <td>B_Phuket_3073_2013_NA</td>\n",
       "      <td>814</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>.</td>\n",
       "      <td>0/1</td>\n",
       "      <td>138</td>\n",
       "      <td>3304</td>\n",
       "      <td>3163</td>\n",
       "      <td>3105</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>1.295600e-14</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>1664</td>\n",
       "      <td>1441</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>NA</td>\n",
       "      <td>773.0</td>\n",
       "      <td>E</td>\n",
       "      <td>257.0</td>\n",
       "      <td>E</td>\n",
       "      <td>E257E</td>\n",
       "      <td>Synonymous</td>\n",
       "      <td>/mnt/d/orchards/h1n1/orchards_runB/B_Phuket_30...</td>\n",
       "      <td>NA</td>\n",
       "      <td>Influenza B</td>\n",
       "      <td>28.43</td>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2017.194521</td>\n",
       "      <td>0.694521</td>\n",
       "      <td>11</td>\n",
       "      <td>16-17</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-10 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088G</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>-1.737549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.999908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999226</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>NA</td>\n",
       "      <td>17VR005325:NA:814:A:NA</td>\n",
       "      <td>0.036013</td>\n",
       "      <td>Synonymous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26896</th>\n",
       "      <td>19VR008902</td>\n",
       "      <td>A_Singapore_INFIMH-16-0019_2016_HA</td>\n",
       "      <td>309</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>.</td>\n",
       "      <td>1/1</td>\n",
       "      <td>255</td>\n",
       "      <td>6493</td>\n",
       "      <td>6395</td>\n",
       "      <td>7</td>\n",
       "      <td>6388</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3685</td>\n",
       "      <td>2703</td>\n",
       "      <td>HA_antigenic</td>\n",
       "      <td>64.0</td>\n",
       "      <td>R</td>\n",
       "      <td>21.0</td>\n",
       "      <td>K</td>\n",
       "      <td>R21K</td>\n",
       "      <td>Nonsynonymous</td>\n",
       "      <td>/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Sing...</td>\n",
       "      <td>HA</td>\n",
       "      <td>H3N2</td>\n",
       "      <td>26.11</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>2019.361644</td>\n",
       "      <td>0.861644</td>\n",
       "      <td>20</td>\n",
       "      <td>18-19</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-05-12 13:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3c3.A</td>\n",
       "      <td>2a</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>-2.958607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>HA_antigenic</td>\n",
       "      <td>19VR008902:HA:309:A:HA_antigenic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Nonsynonymous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26897</th>\n",
       "      <td>19VR008902</td>\n",
       "      <td>A_Singapore_INFIMH-16-0019_2016_NS</td>\n",
       "      <td>681</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>.</td>\n",
       "      <td>1/1</td>\n",
       "      <td>255</td>\n",
       "      <td>583</td>\n",
       "      <td>567</td>\n",
       "      <td>0</td>\n",
       "      <td>567</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>371</td>\n",
       "      <td>NS1</td>\n",
       "      <td>667.0</td>\n",
       "      <td>E</td>\n",
       "      <td>222.0</td>\n",
       "      <td>A</td>\n",
       "      <td>E222A</td>\n",
       "      <td>Nonsynonymous</td>\n",
       "      <td>/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Sing...</td>\n",
       "      <td>NS</td>\n",
       "      <td>H3N2</td>\n",
       "      <td>26.11</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>2019.361644</td>\n",
       "      <td>0.861644</td>\n",
       "      <td>20</td>\n",
       "      <td>18-19</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-05-12 13:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18 or Under</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3c3.A</td>\n",
       "      <td>2a</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>NS1</td>\n",
       "      <td>19VR008902:NS:681:C:NS1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Nonsynonymous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26898</th>\n",
       "      <td>19VR009188</td>\n",
       "      <td>A_Singapore_INFIMH-16-0019_2016_HA</td>\n",
       "      <td>334</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>.</td>\n",
       "      <td>1/1</td>\n",
       "      <td>255</td>\n",
       "      <td>300</td>\n",
       "      <td>295</td>\n",
       "      <td>0</td>\n",
       "      <td>295</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>7.515800e-177</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>123</td>\n",
       "      <td>HA</td>\n",
       "      <td>317.0</td>\n",
       "      <td>R</td>\n",
       "      <td>105.0</td>\n",
       "      <td>R</td>\n",
       "      <td>R105R</td>\n",
       "      <td>Synonymous</td>\n",
       "      <td>/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Sing...</td>\n",
       "      <td>HA</td>\n",
       "      <td>H3N2</td>\n",
       "      <td>31.45</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>2019.361644</td>\n",
       "      <td>0.861644</td>\n",
       "      <td>20</td>\n",
       "      <td>18-19</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-05-09 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3c3.A</td>\n",
       "      <td>2a</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>HA</td>\n",
       "      <td>19VR009188:HA:334:G:HA</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Synonymous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26899</th>\n",
       "      <td>19VR009188</td>\n",
       "      <td>A_Singapore_INFIMH-16-0019_2016_PB2</td>\n",
       "      <td>2171</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>.</td>\n",
       "      <td>1/1</td>\n",
       "      <td>255</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.775800e-63</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>75</td>\n",
       "      <td>PB2</td>\n",
       "      <td>2156.0</td>\n",
       "      <td>G</td>\n",
       "      <td>718.0</td>\n",
       "      <td>G</td>\n",
       "      <td>G718G</td>\n",
       "      <td>Synonymous</td>\n",
       "      <td>/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Sing...</td>\n",
       "      <td>PB2</td>\n",
       "      <td>H3N2</td>\n",
       "      <td>31.45</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>2019.361644</td>\n",
       "      <td>0.861644</td>\n",
       "      <td>20</td>\n",
       "      <td>18-19</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-05-09 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3c3.A</td>\n",
       "      <td>2a</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>PB2</td>\n",
       "      <td>19VR009188:PB2:2171:G:PB2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Synonymous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26900</th>\n",
       "      <td>19VR009188</td>\n",
       "      <td>A_Singapore_INFIMH-16-0019_2016_HA</td>\n",
       "      <td>1222</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>.</td>\n",
       "      <td>0/1</td>\n",
       "      <td>46</td>\n",
       "      <td>216</td>\n",
       "      <td>213</td>\n",
       "      <td>198</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0704</td>\n",
       "      <td>2.364300e-05</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>73</td>\n",
       "      <td>125</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>HA_nonantigenic</td>\n",
       "      <td>806.0</td>\n",
       "      <td>G</td>\n",
       "      <td>268.0</td>\n",
       "      <td>G</td>\n",
       "      <td>G268G</td>\n",
       "      <td>Synonymous</td>\n",
       "      <td>/mnt/d/orchards/h1n1/orchards_run19H3N2/A_Sing...</td>\n",
       "      <td>HA</td>\n",
       "      <td>H3N2</td>\n",
       "      <td>31.45</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>2019.361644</td>\n",
       "      <td>0.861644</td>\n",
       "      <td>20</td>\n",
       "      <td>18-19</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>1910.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-05-09 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Over 18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3c3.A</td>\n",
       "      <td>2a</td>\n",
       "      <td>0.0704</td>\n",
       "      <td>-1.152427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.989486</td>\n",
       "      <td>0.997353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.989486</td>\n",
       "      <td>0.997353</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>HA_nonantigenic</td>\n",
       "      <td>19VR009188:HA:1222:A:HA_nonantigenic</td>\n",
       "      <td>0.131544</td>\n",
       "      <td>Synonymous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26901 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sampleID                                chrom   pos ref_nuc alt_nuc  \\\n",
       "0      17VR005325                B_Phuket_3073_2013_MP   524       T       C   \n",
       "1      17VR005325               B_Phuket_3073_2013_PB2  1892       A       G   \n",
       "2      17VR005325                B_Phuket_3073_2013_HA    60       C       T   \n",
       "3      17VR005325               B_Phuket_3073_2013_PB1  1056       A       G   \n",
       "4      17VR005325                B_Phuket_3073_2013_NA   814       G       A   \n",
       "...           ...                                  ...   ...     ...     ...   \n",
       "26896  19VR008902   A_Singapore_INFIMH-16-0019_2016_HA   309       G       A   \n",
       "26897  19VR008902   A_Singapore_INFIMH-16-0019_2016_NS   681       A       C   \n",
       "26898  19VR009188   A_Singapore_INFIMH-16-0019_2016_HA   334       A       G   \n",
       "26899  19VR009188  A_Singapore_INFIMH-16-0019_2016_PB2  2171       A       G   \n",
       "26900  19VR009188   A_Singapore_INFIMH-16-0019_2016_HA  1222       G       A   \n",
       "\n",
       "      qual   GT   GQ   SDP  depth    RD    AD  SNP_frequency           PVAL  \\\n",
       "0        .  0/1   35  1554   1462  1447    15         0.0103   2.513400e-04   \n",
       "1        .  0/1   83  2045   2001  1966    35         0.0175   4.481700e-09   \n",
       "2        .  1/1  255  1591   1545     1  1544         0.9994   0.000000e+00   \n",
       "3        .  0/1   33  1461   1400  1386    14         0.0100   4.746700e-04   \n",
       "4        .  0/1  138  3304   3163  3105    58         0.0183   1.295600e-14   \n",
       "...    ...  ...  ...   ...    ...   ...   ...            ...            ...   \n",
       "26896    .  1/1  255  6493   6395     7  6388         0.9989   0.000000e+00   \n",
       "26897    .  1/1  255   583    567     0   567         1.0000   0.000000e+00   \n",
       "26898    .  1/1  255   300    295     0   295         1.0000  7.515800e-177   \n",
       "26899    .  1/1  255   106    106     0   106         1.0000   2.775800e-63   \n",
       "26900    .  0/1   46   216    213   198    15         0.0704   2.364300e-05   \n",
       "\n",
       "       RBQ  ABQ   RDF   RDR   ADF   ADR          product  inGenePos refAA  \\\n",
       "0       37   38   876   571     9     6               M1      517.0     V   \n",
       "1       38   36   890  1076    13    22              PB2     1883.0     L   \n",
       "2       37   38     1     0  1328   216               HA       41.0     N   \n",
       "3       38   37   677   709     7     7              PB1     1049.0     R   \n",
       "4       37   37  1664  1441    29    29               NA      773.0     E   \n",
       "...    ...  ...   ...   ...   ...   ...              ...        ...   ...   \n",
       "26896   37   38     5     2  3685  2703     HA_antigenic       64.0     R   \n",
       "26897    0   38     0     0   196   371              NS1      667.0     E   \n",
       "26898    0   37     0     0   172   123               HA      317.0     R   \n",
       "26899    0   38     0     0    31    75              PB2     2156.0     G   \n",
       "26900   38   37    73   125     8     7  HA_nonantigenic      806.0     G   \n",
       "\n",
       "       codon altAA  AAstr         AAtype  \\\n",
       "0      172.0     A  V172A  Nonsynonymous   \n",
       "1      627.0     L  L627L     Synonymous   \n",
       "2       13.0     N   N13N     Synonymous   \n",
       "3      349.0     R  R349R     Synonymous   \n",
       "4      257.0     E  E257E     Synonymous   \n",
       "...      ...   ...    ...            ...   \n",
       "26896   21.0     K   R21K  Nonsynonymous   \n",
       "26897  222.0     A  E222A  Nonsynonymous   \n",
       "26898  105.0     R  R105R     Synonymous   \n",
       "26899  718.0     G  G718G     Synonymous   \n",
       "26900  268.0     G  G268G     Synonymous   \n",
       "\n",
       "                                           referenceFile segment      subtype  \\\n",
       "0      /mnt/d/orchards/h1n1/orchards_runB/B_Phuket_30...      MP  Influenza B   \n",
       "1      /mnt/d/orchards/h1n1/orchards_runB/B_Phuket_30...     PB2  Influenza B   \n",
       "2      /mnt/d/orchards/h1n1/orchards_runB/B_Phuket_30...      HA  Influenza B   \n",
       "3      /mnt/d/orchards/h1n1/orchards_runB/B_Phuket_30...     PB1  Influenza B   \n",
       "4      /mnt/d/orchards/h1n1/orchards_runB/B_Phuket_30...      NA  Influenza B   \n",
       "...                                                  ...     ...          ...   \n",
       "26896  /mnt/d/orchards/h1n1/orchards_run19H3N2/A_Sing...      HA         H3N2   \n",
       "26897  /mnt/d/orchards/h1n1/orchards_run19H3N2/A_Sing...      NS         H3N2   \n",
       "26898  /mnt/d/orchards/h1n1/orchards_run19H3N2/A_Sing...      HA         H3N2   \n",
       "26899  /mnt/d/orchards/h1n1/orchards_run19H3N2/A_Sing...     PB2         H3N2   \n",
       "26900  /mnt/d/orchards/h1n1/orchards_run19H3N2/A_Sing...      HA         H3N2   \n",
       "\n",
       "          CT sample_date  decimalDate  seasonalDate  week season  household  \\\n",
       "0      28.43  2017-03-13  2017.194521      0.694521    11  16-17      720.0   \n",
       "1      28.43  2017-03-13  2017.194521      0.694521    11  16-17      720.0   \n",
       "2      28.43  2017-03-13  2017.194521      0.694521    11  16-17      720.0   \n",
       "3      28.43  2017-03-13  2017.194521      0.694521    11  16-17      720.0   \n",
       "4      28.43  2017-03-13  2017.194521      0.694521    11  16-17      720.0   \n",
       "...      ...         ...          ...           ...   ...    ...        ...   \n",
       "26896  26.11  2019-05-13  2019.361644      0.861644    20  18-19     1910.0   \n",
       "26897  26.11  2019-05-13  2019.361644      0.861644    20  18-19     1910.0   \n",
       "26898  31.45  2019-05-13  2019.361644      0.861644    20  18-19     1910.0   \n",
       "26899  31.45  2019-05-13  2019.361644      0.861644    20  18-19     1910.0   \n",
       "26900  31.45  2019-05-13  2019.361644      0.861644    20  18-19     1910.0   \n",
       "\n",
       "       participant_ID   age  gender time_of_symptom_onset  \\\n",
       "0               720.0  12.0     1.0   2017-03-10 18:00:00   \n",
       "1               720.0  12.0     1.0   2017-03-10 18:00:00   \n",
       "2               720.0  12.0     1.0   2017-03-10 18:00:00   \n",
       "3               720.0  12.0     1.0   2017-03-10 18:00:00   \n",
       "4               720.0  12.0     1.0   2017-03-10 18:00:00   \n",
       "...               ...   ...     ...                   ...   \n",
       "26896          1910.0  12.0     2.0   2019-05-12 13:00:00   \n",
       "26897          1910.0  12.0     2.0   2019-05-12 13:00:00   \n",
       "26898          1910.1  45.0     2.0   2019-05-09 00:00:00   \n",
       "26899          1910.1  45.0     2.0   2019-05-09 00:00:00   \n",
       "26900          1910.1  45.0     2.0   2019-05-09 00:00:00   \n",
       "\n",
       "       recieved_flu_vaccine  school  jackson_score  antipyretic_use  \\\n",
       "0                       1.0     3.0            8.0              0.0   \n",
       "1                       1.0     3.0            8.0              0.0   \n",
       "2                       1.0     3.0            8.0              0.0   \n",
       "3                       1.0     3.0            8.0              0.0   \n",
       "4                       1.0     3.0            8.0              0.0   \n",
       "...                     ...     ...            ...              ...   \n",
       "26896                   1.0     3.0            6.0              0.0   \n",
       "26897                   1.0     3.0            6.0              0.0   \n",
       "26898                   1.0     NaN            NaN              NaN   \n",
       "26899                   1.0     NaN            NaN              NaN   \n",
       "26900                   1.0     NaN            NaN              NaN   \n",
       "\n",
       "       flu_antiviral_treatment  school_absence  number_in_household  \\\n",
       "0                          0.0             1.0                  4.0   \n",
       "1                          0.0             1.0                  4.0   \n",
       "2                          0.0             1.0                  4.0   \n",
       "3                          0.0             1.0                  4.0   \n",
       "4                          0.0             1.0                  4.0   \n",
       "...                        ...             ...                  ...   \n",
       "26896                      1.0             1.0                  3.0   \n",
       "26897                      1.0             1.0                  3.0   \n",
       "26898                      NaN             NaN                  NaN   \n",
       "26899                      NaN             NaN                  NaN   \n",
       "26900                      NaN             NaN                  NaN   \n",
       "\n",
       "       recent_travel  isIndexCase  %ofHHinfected age_category  \\\n",
       "0                0.0         True       0.333333  18 or Under   \n",
       "1                0.0         True       0.333333  18 or Under   \n",
       "2                0.0         True       0.333333  18 or Under   \n",
       "3                0.0         True       0.333333  18 or Under   \n",
       "4                0.0         True       0.333333  18 or Under   \n",
       "...              ...          ...            ...          ...   \n",
       "26896            0.0        False            NaN  18 or Under   \n",
       "26897            0.0        False            NaN  18 or Under   \n",
       "26898            NaN        False            NaN      Over 18   \n",
       "26899            NaN        False            NaN      Over 18   \n",
       "26900            NaN        False            NaN      Over 18   \n",
       "\n",
       "       days_since_symptom_onset  symptom_severity  saw_a_doctor  \\\n",
       "0                           3.0               2.0           0.0   \n",
       "1                           3.0               2.0           0.0   \n",
       "2                           3.0               2.0           0.0   \n",
       "3                           3.0               2.0           0.0   \n",
       "4                           3.0               2.0           0.0   \n",
       "...                         ...               ...           ...   \n",
       "26896                       1.0               1.0           0.0   \n",
       "26897                       1.0               1.0           0.0   \n",
       "26898                       4.0               3.0           1.0   \n",
       "26899                       4.0               3.0           1.0   \n",
       "26900                       4.0               3.0           1.0   \n",
       "\n",
       "       taken_on_hh_infection_day  clade subclade  transformed_frequency  \\\n",
       "0                              0  1088G        9                 0.0103   \n",
       "1                              0  1088G        9                 0.0175   \n",
       "2                              0  1088G        9                 0.0006   \n",
       "3                              0  1088G        9                 0.0100   \n",
       "4                              0  1088G        9                 0.0183   \n",
       "...                          ...    ...      ...                    ...   \n",
       "26896                          0  3c3.A       2a                 0.0011   \n",
       "26897                          0  3c3.A       2a                 0.0000   \n",
       "26898                          0  3c3.A       2a                 0.0000   \n",
       "26899                          0  3c3.A       2a                 0.0000   \n",
       "26900                          0  3c3.A       2a                 0.0704   \n",
       "\n",
       "       log_transformed_frequency  rolling_AA_mean_global_freq  \\\n",
       "0                      -1.987163                          NaN   \n",
       "1                      -1.756962                          NaN   \n",
       "2                      -3.221849                          NaN   \n",
       "3                      -2.000000                          NaN   \n",
       "4                      -1.737549                          NaN   \n",
       "...                          ...                          ...   \n",
       "26896                  -2.958607                          NaN   \n",
       "26897                        NaN                          NaN   \n",
       "26898                        NaN                          NaN   \n",
       "26899                        NaN                          NaN   \n",
       "26900                  -1.152427                          NaN   \n",
       "\n",
       "       rolling_AA_max_global_freq  rolling_DNA_mean_global_freq  \\\n",
       "0                             NaN                           NaN   \n",
       "1                             NaN                           NaN   \n",
       "2                             NaN                      1.000000   \n",
       "3                             NaN                           NaN   \n",
       "4                             NaN                      0.999293   \n",
       "...                           ...                           ...   \n",
       "26896                         NaN                           NaN   \n",
       "26897                         NaN                           NaN   \n",
       "26898                         NaN                           NaN   \n",
       "26899                         NaN                           NaN   \n",
       "26900                         NaN                      0.989486   \n",
       "\n",
       "       rolling_DNA_max_global_freq  \\\n",
       "0                              NaN   \n",
       "1                              NaN   \n",
       "2                         1.000000   \n",
       "3                              NaN   \n",
       "4                         0.999908   \n",
       "...                            ...   \n",
       "26896                          NaN   \n",
       "26897                          NaN   \n",
       "26898                          NaN   \n",
       "26899                          NaN   \n",
       "26900                     0.997353   \n",
       "\n",
       "       rolling_AA_mean_global_freq_following_year  \\\n",
       "0                                             NaN   \n",
       "1                                             NaN   \n",
       "2                                             NaN   \n",
       "3                                             NaN   \n",
       "4                                             NaN   \n",
       "...                                           ...   \n",
       "26896                                         NaN   \n",
       "26897                                         NaN   \n",
       "26898                                         NaN   \n",
       "26899                                         NaN   \n",
       "26900                                         NaN   \n",
       "\n",
       "       rolling_AA_max_global_freq_following_year  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "...                                          ...   \n",
       "26896                                        NaN   \n",
       "26897                                        NaN   \n",
       "26898                                        NaN   \n",
       "26899                                        NaN   \n",
       "26900                                        NaN   \n",
       "\n",
       "       rolling_DNA_mean_global_freq_following_year  \\\n",
       "0                                              NaN   \n",
       "1                                              NaN   \n",
       "2                                         1.000000   \n",
       "3                                              NaN   \n",
       "4                                         0.999226   \n",
       "...                                            ...   \n",
       "26896                                          NaN   \n",
       "26897                                          NaN   \n",
       "26898                                          NaN   \n",
       "26899                                          NaN   \n",
       "26900                                     0.989486   \n",
       "\n",
       "       rolling_DNA_max_global_freq_following_year  AAobservedGlobally  \\\n",
       "0                                             NaN               False   \n",
       "1                                             NaN               False   \n",
       "2                                        1.000000               False   \n",
       "3                                             NaN               False   \n",
       "4                                        0.999995               False   \n",
       "...                                           ...                 ...   \n",
       "26896                                         NaN               False   \n",
       "26897                                         NaN               False   \n",
       "26898                                         NaN               False   \n",
       "26899                                         NaN               False   \n",
       "26900                                    0.997353               False   \n",
       "\n",
       "       AAobservedGloballyFollowingSeason  DNAobservedGlobally  \\\n",
       "0                                  False                False   \n",
       "1                                  False                False   \n",
       "2                                  False                 True   \n",
       "3                                  False                False   \n",
       "4                                  False                 True   \n",
       "...                                  ...                  ...   \n",
       "26896                              False                False   \n",
       "26897                              False                False   \n",
       "26898                              False                False   \n",
       "26899                              False                False   \n",
       "26900                              False                 True   \n",
       "\n",
       "       DNAobservedGloballyFollowingSeason antigenic_site antigenic_product  \\\n",
       "0                                   False           None                M1   \n",
       "1                                   False           None               PB2   \n",
       "2                                    True           None                HA   \n",
       "3                                   False           None               PB1   \n",
       "4                                    True           None                NA   \n",
       "...                                   ...            ...               ...   \n",
       "26896                               False           None      HA_antigenic   \n",
       "26897                               False           None               NS1   \n",
       "26898                               False             No                HA   \n",
       "26899                               False           None               PB2   \n",
       "26900                                True           None   HA_nonantigenic   \n",
       "\n",
       "                                     snpKey  pi_nucleotide  \\\n",
       "0                    17VR005325:MP:524:C:M1       0.020323   \n",
       "1                 17VR005325:PB2:1892:G:PB2       0.034388   \n",
       "2                     17VR005325:HA:60:T:HA       0.000000   \n",
       "3                 17VR005325:PB1:1056:G:PB1       0.019814   \n",
       "4                    17VR005325:NA:814:A:NA       0.036013   \n",
       "...                                     ...            ...   \n",
       "26896      19VR008902:HA:309:A:HA_antigenic       0.000000   \n",
       "26897               19VR008902:NS:681:C:NS1       0.000000   \n",
       "26898                19VR009188:HA:334:G:HA       0.000000   \n",
       "26899             19VR009188:PB2:2171:G:PB2       0.000000   \n",
       "26900  19VR009188:HA:1222:A:HA_nonantigenic       0.131544   \n",
       "\n",
       "      class_of_mutation_at_site  \n",
       "0                 Nonsynonymous  \n",
       "1                    Synonymous  \n",
       "2                    Synonymous  \n",
       "3                    Synonymous  \n",
       "4                    Synonymous  \n",
       "...                         ...  \n",
       "26896             Nonsynonymous  \n",
       "26897             Nonsynonymous  \n",
       "26898                Synonymous  \n",
       "26899                Synonymous  \n",
       "26900                Synonymous  \n",
       "\n",
       "[26901 rows x 77 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-146-42960671e8ff>:3: RuntimeWarning: All-NaN slice encountered\n",
      "  segmentPseudo = np.nanmin(segments[['piN_segment','piS_segment']].replace(0,np.nan).values)\n"
     ]
    }
   ],
   "source": [
    "#Calculate log(piN/piS)\n",
    "samplePseudo = np.nanmin(samples[['piN_sample','piS_sample']].replace(0,np.nan).values)\n",
    "segmentPseudo = np.nanmin(segments[['piN_segment','piS_segment']].replace(0,np.nan).values)\n",
    "genePseudo = np.nanmin(genes[['piN_gene','piS_gene']].replace(0,np.nan).values)\n",
    "\n",
    "samples['log(piN/piS)'] = np.log10((samples.piN_sample+samplePseudo)/(samples.piS_sample+samplePseudo))\n",
    "segments['log(piN/piS)_seg'] = np.log10((segments.piN_segment+segmentPseudo)/(segments.piS_segment+segmentPseudo))\n",
    "genes['log(piN/piS)_gene'] = np.log10((genes.piN_gene+genePseudo)/(genes.piS_gene+genePseudo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'gender', 'time_of_symptom_onset', 'recieved_flu_vaccine', 'school', 'jackson_score', 'antipyretic_use', 'flu_antiviral_treatment', 'school_absence', 'number_in_household', 'recent_travel', '%ofHHinfected', 'days_since_symptom_onset', 'symptom_severity', 'saw_a_doctor', 'snps_per_day_gene', 'nonsynon_snps_per_day_gene', 'synon_snps_per_day_gene', 'mutation_rate_gene', 'nonsynon_mutation_rate_gene', 'synon_mutation_rate_gene', 'divergence_per_day_gene', 'nonsynon_divergence_per_day_gene', 'synon_divergence_per_day_gene', 'divergence_rate_gene', 'nonsynon_divergence_rate_gene', 'synon_divergence_rate_gene']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_of_symptom_onset</th>\n",
       "      <th>num_of_snps_gene</th>\n",
       "      <th>snps_per_day_gene</th>\n",
       "      <th>nonsynon_snps_per_day_gene</th>\n",
       "      <th>synon_snps_per_day_gene</th>\n",
       "      <th>mutation_rate_gene</th>\n",
       "      <th>nonsynon_mutation_rate_gene</th>\n",
       "      <th>synon_mutation_rate_gene</th>\n",
       "      <th>divergence_per_day_gene</th>\n",
       "      <th>nonsynon_divergence_per_day_gene</th>\n",
       "      <th>synon_divergence_per_day_gene</th>\n",
       "      <th>divergence_rate_gene</th>\n",
       "      <th>nonsynon_divergence_rate_gene</th>\n",
       "      <th>synon_divergence_rate_gene</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HA_antigenic</th>\n",
       "      <td>298</td>\n",
       "      <td>304</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HA_nonantigenic</th>\n",
       "      <td>298</td>\n",
       "      <td>304</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time_of_symptom_onset  num_of_snps_gene  snps_per_day_gene  \\\n",
       "product                                                                       \n",
       "HA_antigenic                       298               304                295   \n",
       "HA_nonantigenic                    298               304                295   \n",
       "\n",
       "                 nonsynon_snps_per_day_gene  synon_snps_per_day_gene  \\\n",
       "product                                                                \n",
       "HA_antigenic                            295                      295   \n",
       "HA_nonantigenic                         295                      295   \n",
       "\n",
       "                 mutation_rate_gene  nonsynon_mutation_rate_gene  \\\n",
       "product                                                            \n",
       "HA_antigenic                    295                          295   \n",
       "HA_nonantigenic                 295                          295   \n",
       "\n",
       "                 synon_mutation_rate_gene  divergence_per_day_gene  \\\n",
       "product                                                              \n",
       "HA_antigenic                          295                      295   \n",
       "HA_nonantigenic                       295                      295   \n",
       "\n",
       "                 nonsynon_divergence_per_day_gene  \\\n",
       "product                                             \n",
       "HA_antigenic                                  295   \n",
       "HA_nonantigenic                               295   \n",
       "\n",
       "                 synon_divergence_per_day_gene  divergence_rate_gene  \\\n",
       "product                                                                \n",
       "HA_antigenic                               295                   295   \n",
       "HA_nonantigenic                            295                   295   \n",
       "\n",
       "                 nonsynon_divergence_rate_gene  synon_divergence_rate_gene  \n",
       "product                                                                     \n",
       "HA_antigenic                               295                         295  \n",
       "HA_nonantigenic                            295                         295  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = genes.loc[genes['product'].isin(['HA_antigenic','HA_nonantigenic'])].groupby(['product']).count()\n",
    "print ([column for column in x.columns if any(x[column].values != 304)])\n",
    "x[['time_of_symptom_onset','num_of_snps_gene','snps_per_day_gene', 'nonsynon_snps_per_day_gene', 'synon_snps_per_day_gene', 'mutation_rate_gene', 'nonsynon_mutation_rate_gene', 'synon_mutation_rate_gene', 'divergence_per_day_gene', 'nonsynon_divergence_per_day_gene', 'synon_divergence_per_day_gene', 'divergence_rate_gene', 'nonsynon_divergence_rate_gene', 'synon_divergence_rate_gene']]\n",
    "# missing_non=set(genes.loc[pd.isna(genes.snps_per_day_gene)&(genes['product']=='HA_nonantigenic'),'sampleID'].to_list())\n",
    "# missing_anti = set(genes.loc[pd.isna(genes.time_of_symptom_onset)&(genes['product']=='HA_nonantigenic'),'sampleID'].to_list())\n",
    "# tofix = list(missing_non.symmetric_difference(missing_anti))\n",
    "# tofix\n",
    "# genes.loc[genes.sampleID.isin(tofix)&genes.isin(['HA_antigenic','HA_nonantigenic'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N_sites_gene', 'S_sites_gene', 'num_N_sites_samp', 'num_S_sites_samp']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in genes.columns if 'site' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At various points while making these dfs I have used the term 'H1N1pdm', but I don't know that these are pandemic H1N1 sequences.\n",
    "#I will just blanket change the term in all dfs before saving them.\n",
    "\n",
    "subjects = subjects.replace('H1N1pdm', 'H1N1')\n",
    "# samples = samples.replace('H1N1pdm', 'H1N1')\n",
    "# segments = segments.replace('H1N1pdm', 'H1N1')\n",
    "# genes = genes.replace('H1N1pdm', 'H1N1')\n",
    "# codons = codons.replace('H1N1pdm', 'H1N1')\n",
    "# SNPs = SNPs.replace('H1N1pdm', 'H1N1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useSNPGenie2:\n",
    "    # Somehow this doesn't happen earlier, \n",
    "    # but the number of sites info isn't attached to SNPs. \n",
    "    # This info is handy for transmission SNPs calcs later. \n",
    "    # I'll add it here.\n",
    "    SNPs = SNPs.merge(genes[['sampleID','product','N_sites_gene', 'S_sites_gene', 'num_N_sites_samp', 'num_S_sites_samp']], on=['sampleID','product'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving subjects...\n",
      "saving metadata...\n",
      "saving segments...\n",
      "saving genes...\n",
      "saving codons...\n",
      "saving SNPs...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "basename = '/mnt/d/orchards/h1n1/figures'\n",
    "print ('saving subjects...')\n",
    "subjects.to_csv(basename+'/subjects.tsv', sep='\\t', index=False)\n",
    "print ('saving metadata...')\n",
    "samples.to_csv(basename+'/samples.tsv', sep='\\t', index=False)\n",
    "print ('saving segments...')\n",
    "segments.to_csv(basename+'/segments.tsv', sep='\\t', index=False)\n",
    "print ('saving genes...')\n",
    "genes.to_csv(basename+'/genes.tsv', sep='\\t', index=False)\n",
    "print ('saving codons...')\n",
    "codons.to_csv(basename+'/all_codons.gz', sep='\\t', index=False, compression='gzip')\n",
    "print ('saving SNPs...')\n",
    "SNPs.to_csv(basename+'/SNPs_lenient_filter.gz', sep='\\t', index=False, compression='gzip')\n",
    "print ('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = '/mnt/d/orchards/h1n1/figures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing SNPGenie 1 vs 2\n",
    "samples2 = samples.copy()\n",
    "genes2 = genes.copy()\n",
    "SNPs2 = SNPs.copy()\n",
    "SNPs1 = pd.read_csv(basename+'/SNPs_lenient_filter.gz', sep='\\t')\n",
    "genes1 = pd.read_csv(basename+'/genes.tsv', sep='\\t')\n",
    "samples1 = pd.read_csv(basename+'/samples.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(samples1.columns)\n",
    "sample_cols_of_interest=['sampleID',\n",
    " 'subtype',\n",
    " 'num_of_snps',\n",
    " 'num_of_mixed_muts',\n",
    " 'num_of_nonsynon_muts',\n",
    " 'num_of_oof_muts',\n",
    " 'num_of_synon_muts',\n",
    " 'pi_sample',\n",
    " 'piN_sample',\n",
    " 'piS_sample',\n",
    " 'num_N_sites_samp',\n",
    " 'num_S_sites_samp',\n",
    " 'subtypeLengths',\n",
    " 'mutation_rate_samp',\n",
    " 'log(piN/piS)']\n",
    "sample_comp = samples1[sample_cols_of_interest].merge(samples2[sample_cols_of_interest], on=['sampleID','subtype'], suffixes=('_1','_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlalli/.local/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtypeLengths_1</th>\n",
       "      <th>total_1</th>\n",
       "      <th>total_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13705</td>\n",
       "      <td>13249.000000</td>\n",
       "      <td>13249.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13705</td>\n",
       "      <td>13248.958724</td>\n",
       "      <td>13248.958724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13705</td>\n",
       "      <td>13249.000000</td>\n",
       "      <td>13249.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13705</td>\n",
       "      <td>13249.000000</td>\n",
       "      <td>13249.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13705</td>\n",
       "      <td>13249.000000</td>\n",
       "      <td>13249.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>13427</td>\n",
       "      <td>13050.000000</td>\n",
       "      <td>13050.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>13434</td>\n",
       "      <td>13047.000000</td>\n",
       "      <td>13047.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>13427</td>\n",
       "      <td>13050.000000</td>\n",
       "      <td>13050.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>13427</td>\n",
       "      <td>13050.000000</td>\n",
       "      <td>13050.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>13427</td>\n",
       "      <td>13049.898462</td>\n",
       "      <td>13049.898462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subtypeLengths_1       total_1       total_2\n",
       "0               13705  13249.000000  13249.000000\n",
       "1               13705  13248.958724  13248.958724\n",
       "2               13705  13249.000000  13249.000000\n",
       "3               13705  13249.000000  13249.000000\n",
       "4               13705  13249.000000  13249.000000\n",
       "..                ...           ...           ...\n",
       "332             13427  13050.000000  13050.000000\n",
       "333             13434  13047.000000  13047.000000\n",
       "334             13427  13050.000000  13050.000000\n",
       "335             13427  13050.000000  13050.000000\n",
       "336             13427  13049.898462  13049.898462\n",
       "\n",
       "[337 rows x 3 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot('num_S_sites_samp_1','num_S_sites_samp_2',hue='subtype',data=sample_comp, alpha=0.2)\n",
    "# plt.gca().set_yscale('log')\n",
    "# plt.gca().set_ylim(1e-6, 4e-4)\n",
    "# plt.gca().set_xscale('log')\n",
    "# plt.gca().set_xlim(1e-6, 4e-4)\n",
    "sample_comp[['num_N_sites_samp_1','num_N_sites_samp_2','num_S_sites_samp_1','num_S_sites_samp_2']]\n",
    "sample_comp['total_2'] = sample_comp.num_N_sites_samp_2 + sample_comp.num_S_sites_samp_2\n",
    "sample_comp['total_1'] = sample_comp.num_N_sites_samp_1 + sample_comp.num_S_sites_samp_1\n",
    "sample_comp[['subtypeLengths_1','total_1','total_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_gene_columns = ['sampleID',\n",
    " 'segment',\n",
    " 'product',\n",
    " 'piN_gene',\n",
    " 'piS_gene',\n",
    " 'N_sites_gene',\n",
    " 'S_sites_gene',\n",
    " 'subtype',\n",
    " 'num_of_snps',\n",
    " 'num_of_mixed_muts',\n",
    " 'num_of_nonsynon_muts',\n",
    " 'num_of_oof_muts',\n",
    " 'num_of_synon_muts',\n",
    " 'num_of_snps_gene',\n",
    " 'num_of_nonsynon_muts_gene',\n",
    " 'num_of_synon_muts_gene',\n",
    " 'genelength']\n",
    "\n",
    "gene_comp = genes1[testing_gene_columns].merge(genes2[testing_gene_columns], on=['sampleID','product'], suffixes=('_1','_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlalli/.local/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1e-05, 0.01)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot('piS_gene_1','piS_gene_2',hue='subtype_1',data=gene_comp, alpha=0.2)\n",
    "plt.gca().set_yscale('log')\n",
    "plt.gca().set_ylim(1e-5, 1e-2)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_xlim(1e-5, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_comp.columns\n",
    "# columns_in_presentation_order=['sampleID','product','piN_gene_1','piN_gene_2','piS_gene_1','piS_gene_2','N_sites_gene_1','N_sites_gene_2','S_sites_gene_1','S_sites_gene_2']\n",
    "# gene_comp.loc[(gene_comp['product']=='PB2')&(~np.isclose(gene_comp.N_sites_gene_1.values, gene_comp.N_sites_gene_2.values, atol=.1)), ['num_of_snps_gene_1']+columns_in_presentation_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(SNPGenie2)\n",
    "# from SNPGenie2 import *\n",
    "# del calcPi\n",
    "# vcfDF = df\n",
    "# gtf = gtfFiles[2]\n",
    "# maf = 0.01\n",
    "# rollingWindow=None\n",
    "# synon_nonsynon=False\n",
    "# # length = \n",
    "# refseq = consensusReferences[2]\n",
    "# # def calcPi(vcfDF, gtf=None, length = None, refseq = None, maf = 0.01, rollingWindow=None, synon_nonsynon=False): \n",
    "# # Main body of program\n",
    "# refseq = list(SeqIO.parse(refseq, 'fasta'))\n",
    "# refseqArray, segStarts, segCoords = combine_segments(refseq)\n",
    "\n",
    "# vcfDF = vcfDF.rename(columns={'sample':'sampleID'})\n",
    "# #Now that we have our concatenated sequence with dictionary of start of each segment,\n",
    "# #we can assemble a numpy representation of read counts aligned with our references\n",
    "# piSNPs = vcfDF[['sampleID','product','segment','inGenePos','ref_nuc','alt_nuc','RD','AD','AAtype','pos']]\n",
    "# piSNPs = piSNPs.sort_values(['sampleID','product','inGenePos'])\n",
    "\n",
    "# piSNPs['pos'] = piSNPs.pos-1 #Apparantly this is taken straight from vcf, which is 1-indexed\n",
    "\n",
    "# piSNPs = piSNPs.rename(columns={'pos':'inSegPos'})\n",
    "# piSNPs['refAlleleFreq'] = piSNPs.RD/(piSNPs.RD+piSNPs.AD)\n",
    "\n",
    "# # piSNPs = piSNPs.loc[(piSNPs.refAlleleFreq >= maf)&(piSNPs.refAlleleFreq <= (1-maf))]\n",
    "# # Adjust SNPs to remove reads that contribute to minor variants below maf cutoff:\n",
    "# piSNPs.loc[piSNPs.refAlleleFreq < maf, 'RD'] = 0\n",
    "# piSNPs.loc[piSNPs.refAlleleFreq > (1-maf), 'AD'] = 0\n",
    "# piSNPs['pos'] = piSNPs['product'] + ' '+ piSNPs['inGenePos'].astype(str)\n",
    "# piSNPs[['sampleID','segment','pos','ref_nuc','alt_nuc','RD','AD','AAtype','inSegPos','refAlleleFreq']].set_index(['sampleID','pos'])\n",
    "# piSNPs['inConcatPos'] = piSNPs.inSegPos\n",
    "\n",
    "# for seg, offset in segStarts.items():\n",
    "#     seg = seg.split('_')[-1]\n",
    "#     piSNPs.loc[piSNPs.segment == seg,'inConcatPos'] += offset\n",
    "\n",
    "# refSeqArrays, sampleKey = getRefSeqs(piSNPs, refseqArray)\n",
    "# readCts, samplelist, poslist = turnDFintoReadCts(piSNPs, len(refseqArray))\n",
    "\n",
    "# piMath = performPiCalc(readCts)\n",
    "\n",
    "# perSitePi = calcPerSitePi(piMath)\n",
    "\n",
    "# def calcPerSamplePi(sitePi, length=None, printit=None):\n",
    "#     '''given numpy array of pi values, returns average per sample pi'''\n",
    "#     if length is None:\n",
    "#         length = sitePi.shape[1]\n",
    "#     return np.nansum(sitePi,axis=1)/length\n",
    "\n",
    "# perSitePi = np.nan_to_num(perSitePi)\n",
    "# perSitePiDF = pd.DataFrame(perSitePi, index=sampleKey).dropna(how='all',axis=1)\n",
    "# # perSitePiDF.columns = poslist\n",
    "# perSitePiDF = perSitePiDF.stack().reset_index().rename(columns={'level_0':'sampleID','level_1':'inConcatPos',0:'pi'})\n",
    "# perSitePiDF = piSNPs.merge(perSitePiDF, on=['sampleID','inConcatPos'])\n",
    "\n",
    "# perSamplePi = calcPerSamplePi(perSitePi)\n",
    "# perSamplePiDF = pd.DataFrame(perSamplePi, index=sampleKey, columns=['pi_sample'])\n",
    "\n",
    "\n",
    "# #segment Pi\n",
    "# perSegmentPiDF = pd.DataFrame(index=sampleKey)\n",
    "\n",
    "# for seg, coords in segCoords.items():\n",
    "#     perSegmentPiDF[seg] = pd.Series(calcPerSamplePi(perSitePi[:,coords[0]:coords[1]], length = coords[1]-coords[0]), index=sampleKey)\n",
    "# perSegmentPiDF = perSegmentPiDF.stack().reset_index().rename(columns={'level_0':'sampleID','level_1':'segment',0:'pi_seg'})\n",
    "\n",
    "# #coding region Pi/PiN/PiS\n",
    "# codingCoords = parseGTF(gtf, segStarts)\n",
    "# codingMasks = masksFromCodingCoordinates(codingCoords, segStarts, refSeqArrays)\n",
    "# overlaps = calc_overlaps(codingCoords)\n",
    "\n",
    "# genePis = pd.DataFrame(index=sampleKey)\n",
    "# genePiN = pd.DataFrame(index=sampleKey)\n",
    "# genePiS = pd.DataFrame(index=sampleKey)\n",
    "# genePiN_sites = pd.DataFrame(index=sampleKey)\n",
    "# genePiS_sites = pd.DataFrame(index=sampleKey)\n",
    "# sample_piN_piS = pd.DataFrame(index=sampleKey, columns=['piN_sum','piN_sites','piS_sum','piS_sites']).fillna(0)\n",
    "\n",
    "\n",
    "# for gene, mask in codingMasks.items():\n",
    "#     # print ((readCts*mask[np.newaxis, :, np.newaxis]).sum())\n",
    "#     geneSeqMaskedArray = np.ma.array(refSeqArrays, mask=np.broadcast_to(abs(mask-1), refSeqArrays.shape))\n",
    "#     geneSeqArray = geneSeqMaskedArray.compressed().reshape(len(samplelist),-1)\n",
    "\n",
    "#     synonFilter, nonSynonFilter, synonSiteFilter, nonSynonSiteFilter = generateSynonFilters(geneSeqArray)\n",
    "\n",
    "#     gene_readCts = np.ma.array(readCts, mask=np.broadcast_to(abs(mask-1)[np.newaxis,:,np.newaxis], readCts.shape))\n",
    "#     gene_readCts = gene_readCts.compressed().reshape(synonSiteFilter.shape)\n",
    "\n",
    "#     SNP_freqs = read_cts_into_SNP_freqs(gene_readCts, geneSeqArray)\n",
    "#     # remove start codon from math\n",
    "#     if SNPGenie_1_rules:\n",
    "#         nonSynonSiteFilter[:,0:3,:] = 1\n",
    "#         synonSiteFilter[:,0:3,:] = 0\n",
    "\n",
    "#     nonsynon_sites = (SNP_freqs*nonSynonSiteFilter).sum(axis=2)\n",
    "#     synon_sites = (SNP_freqs*synonSiteFilter).sum(axis=2)\n",
    "#     print('\\n'+gene)\n",
    "#     print(synon_sites.sum(axis=1)-genes.loc[genes['sampleID'].isin(samplelist) & (genes['product']==gene), 'S_sites_gene'].values)\n",
    "#     print(np.sum(np.abs((apply_mask(refseqArray[np.newaxis,:], mask)==geneSeqArray)-1), axis=1))\n",
    "#     piMathMaskedArray = np.ma.array(piMath, mask=np.broadcast_to(abs(mask-1)[np.newaxis,:,np.newaxis], piMath.shape))\n",
    "#     genePiMathArray = piMathMaskedArray.compressed().reshape(synonFilter.shape)\n",
    "#     genePerSitePi = calcPerSitePi(genePiMathArray)\n",
    "#     nonSynonPerSitePi = calcPerSitePi(genePiMathArray*nonSynonFilter)\n",
    "#     synonPerSitePi = calcPerSitePi(genePiMathArray*synonFilter)\n",
    "#     genePis[gene] = calcPerSamplePi(genePerSitePi)\n",
    "#     genePiN[gene] = calcPerSamplePi(nonSynonPerSitePi, length=nonsynon_sites.sum(axis=1), printit='PiN')\n",
    "#     genePiS[gene] = calcPerSamplePi(synonPerSitePi, length=synon_sites.sum(axis=1),printit='PiS')\n",
    "#     genePiN_sites[gene] = nonsynon_sites.sum(axis=1)\n",
    "#     genePiS_sites[gene] = synon_sites.sum(axis=1)\n",
    "#     # print(gene, genePiN[gene])\n",
    "#     #And now do the same thing w/o overlapping regions to accurately determine whole-sample piN/piS\n",
    "#     if gene in overlaps.keys():\n",
    "#         synonFilter_no_overlap = remove_overlap_regions(overlaps, synonFilter)\n",
    "#         nonSynonFilter_no_overlap = remove_overlap_regions(overlaps, nonSynonFilter)\n",
    "#         seg = [seg for seg in codingCoords.keys() if gene in codingCoords[seg]][0]\n",
    "#         keepers = get_nonoverlapping_in_gene_locations(seg, gene, codingCoords)\n",
    "\n",
    "#         sample_piN_piS['piS_sites'] += synon_sites[:, keepers].sum(axis=1)\n",
    "#         sample_piN_piS['piN_sites'] += nonsynon_sites[:, keepers].sum(axis=1)\n",
    "# #             print(gene, 'nonsynon_sites', get_num_sites(nonSynonFilter_no_overlap))\n",
    "#         nonSynonPerSitePi_no_overlap = calcPerSitePi(genePiMathArray*nonSynonFilter_no_overlap)\n",
    "#         synonPerSitePi_no_overlap = calcPerSitePi(genePiMathArray*synonFilter_no_overlap)\n",
    "\n",
    "#         sample_piN_piS['piS_sum'] += calcPerSamplePi(synonPerSitePi_no_overlap, length=1, printit='PiS')\n",
    "#         sample_piN_piS['piN_sum'] += calcPerSamplePi(nonSynonPerSitePi_no_overlap, length=1, printit='PiN')\n",
    "#     else:\n",
    "#         if gene not in ['PA-X', 'PB1-F2', 'HA_antigenic', 'HA_nonantigenic']:\n",
    "# #             print(gene, get_num_sites(nonSynonFilter_no_overlap)[0])\n",
    "#             sample_piN_piS['piS_sites'] += synon_sites.sum(axis=1)\n",
    "#             sample_piN_piS['piN_sites'] += nonsynon_sites.sum(axis=1)\n",
    "#     #             print(gene, 'nonsynon_sites', num_nonsynon_sites)\n",
    "#             sample_piN_piS['piS_sum'] += calcPerSamplePi(synonPerSitePi, length=1, printit='PiS')\n",
    "#             sample_piN_piS['piN_sum'] += calcPerSamplePi(nonSynonPerSitePi, length=1, printit='PiN')\n",
    "\n",
    "\n",
    "# sample_piN_piS['piN_sample'] = sample_piN_piS['piN_sum']/sample_piN_piS['piN_sites']\n",
    "# sample_piN_piS['piS_sample'] = sample_piN_piS['piS_sum']/sample_piN_piS['piS_sites']\n",
    "# perSamplePiDF = perSamplePiDF.join(sample_piN_piS)#[['piN_sample','piS_sample']])\n",
    "\n",
    "# genePis = genePis.stack().reset_index().rename(columns={'level_0':'sampleID','level_1':'product',0:'pi'})\n",
    "# genePiN = genePiN.stack().reset_index().rename(columns={'level_0':'sampleID','level_1':'product',0:'pi'})\n",
    "# genePiS = genePiS.stack().reset_index().rename(columns={'level_0':'sampleID','level_1':'product',0:'pi'})\n",
    "# genePiN_sites = genePiN_sites.stack().reset_index().rename(columns={'level_0':'sampleID','level_1':'product',0:'sites'})\n",
    "# genePiS_sites = genePiS_sites.stack().reset_index().rename(columns={'level_0':'sampleID','level_1':'product',0:'sites'})\n",
    "# genePiN = genePiN.merge(genePiN_sites, on=['sampleID','product'], how='left')\n",
    "# genePiS = genePiS.merge(genePiS_sites, on=['sampleID','product'], how='left')\n",
    "\n",
    "# genePis['type'] = 'pi'\n",
    "# genePiN['type'] = 'piN'\n",
    "# genePiS['type'] = 'piS'\n",
    "\n",
    "# genePis = genePis.append(genePiN).append(genePiS)\n",
    "\n",
    "# # Finally, just because somehow it hasn't happened yet in this code,\n",
    "# # I'm going to make sure the segment is attached to the gene DF\n",
    "\n",
    "# segDict = {gene:seg.split('_')[-1] for seg in codingCoords.keys() for gene in codingCoords[seg]}\n",
    "# segDict['PB1-F2']='PB1'\n",
    "# segDict['PA-X']='PA'\n",
    "# segDict['HA_nonantigenic']='HA'\n",
    "# segDict['HA_antigenic']='HA'\n",
    "# segDict['NB'] = 'NA'\n",
    "# segDict['BM2'] = 'MP'\n",
    "# segDict['M2'] = 'MP'\n",
    "# segDict['M1'] = 'MP'\n",
    "# segDict['NEP'] = 'NS'\n",
    "# segDict['NS1'] = 'NS'\n",
    "# genePis['segment'] = genePis['product'].map(segDict)\n",
    "\n",
    "# #     return perSamplePiDF, perSegmentPiDF, genePis, perSitePiDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ref, df in SNPs.groupby('referenceFile'):\n",
    "#     df = df.rename(columns={'sampleID':'sample'})\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNP_freqs[np.where((SNP_freqs>0)&(SNP_freqs<1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_mask(array, mask):\n",
    "#     masked_array = np.ma.array(array, mask=np.broadcast_to(abs(mask-1), array.shape))\n",
    "#     return masked_array.compressed().reshape(array.shape[0],-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # len(refseqArray)\n",
    "# # SNPs.loc[(SNPs.sampleID=='18VR002051')&(SNPs.segment=='HA')&(SNPs.inGenePos==26)]\n",
    "# geneSeqMaskedArray = np.ma.array(refSeqArrays, mask=np.broadcast_to(abs(mask-1), refSeqArrays.shape))\n",
    "# geneSeqArray = geneSeqMaskedArray.compressed().reshape(len(samplelist),-1)\n",
    "# refSeqArrays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(((nonsynon_sites%1)!=0)&((nonsynon_sites%1)!=1/6)&((nonsynon_sites%1)!=2/6)&((nonsynon_sites%1)!=3/6)&((nonsynon_sites%1)!=4/6)&((nonsynon_sites%1)!=5/6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkForDuplicateColumnsPostMerge(df, suffixes=('_x','_y')):\n",
    "    '''if an index/contact or x/y column pairing are identical, unify them into one column.\n",
    "       Keeps np.nan values seperate.'''\n",
    "    columns = [column[:-len(suffixes[0])] for column in df.columns if column[-len(suffixes[0]):]==suffixes[0]]\n",
    "    \n",
    "    merged=[]\n",
    "    kept = []\n",
    "    for column in columns:\n",
    "        columna = column+suffixes[0]\n",
    "        columnb = column+suffixes[1]\n",
    "                \n",
    "        a=df[columna].values\n",
    "        b=df[columnb].values\n",
    "        \n",
    "        if (df[columna].dtype.kind in 'biufc') and (df[columnb].dtype.kind in 'biufc'):\n",
    "            theyAreEqual = ((a==b)|np.isclose(a,b,atol=1E-4)|np.isclose(b,a,atol=1E-4))\n",
    "        else:\n",
    "            theyAreEqual = ((a==b))\n",
    "        if theyAreEqual.all():\n",
    "            df = df.rename(columns={columna:column}).drop(columns=[columnb])\n",
    "            merged.append(column)\n",
    "        \n",
    "        else:\n",
    "            kept.append(column)\n",
    "    \n",
    "    print('merged:')\n",
    "    print (merged)\n",
    "    print('kept:')\n",
    "    print(kept)\n",
    "    return df\n",
    "\n",
    "def updateDuplicateColumnsPostMerge(df, exclude=[], suffixes=('_x','_y')):\n",
    "    '''if an index/contact or x/y column pairing are identical except for na values, unify them into one column.\n",
    "       Assumes np.nan values are artifacts, and fills in values if one column has them'''\n",
    "#     suffixes = ('_index','_contact')\n",
    "    columns = [column[:-len(suffixes[0])] for column in df.columns if column[-len(suffixes[0]):]==suffixes[0]]\n",
    "#     for column in df.columns:\n",
    "#         print (column)\n",
    "#         print (f'column:{column[:-len(suffixes[0])]}')\n",
    "#         print (f'matching end: {column[-len(suffixes[0]):]}')\n",
    "    merged=[]\n",
    "    kept = []\n",
    "    for column in columns:\n",
    "        columna = column+suffixes[0]\n",
    "        columnb = column+suffixes[1]\n",
    "                \n",
    "        a=df[columna].values\n",
    "        b=df[columnb].values\n",
    "        \n",
    "        if (df[columna].dtype.kind in 'biufc') and (df[columnb].dtype.kind in 'biufc'):\n",
    "            theyAreEqual = ((a==b)|pd.isna(a)|pd.isna(b)|np.isclose(a,b,atol=1E-4)|np.isclose(b,a,atol=1E-4))\n",
    "        else:\n",
    "            theyAreEqual = ((a==b)|pd.isna(a)|pd.isna(b))\n",
    "        \n",
    "        if 'AAstr' in column:\n",
    "            print (((a==b)|pd.isna(a)|pd.isna(b)).all())\n",
    "            print (df[((a!=b)&pd.notna(a)&pd.notna(b))])\n",
    "        \n",
    "        if theyAreEqual.all():\n",
    "            df[columna].update(df[columnb])\n",
    "            df = df.rename(columns={columna:column}).drop(columns=[columnb])\n",
    "            merged.append(column)\n",
    "        else:\n",
    "            kept.append(column)\n",
    "                \n",
    "    print('updated:')\n",
    "    print (merged)\n",
    "    print('untouched:')\n",
    "    print(kept)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pre-calculated sample distances...\n",
      "merged:\n",
      "['subtype', 'mapped', 'decimalDate', 'season', 'household', 'taken_on_hh_infection_day', 'clade', 'subclade', 'subtypeLengths']\n",
      "kept:\n",
      "['ptID', 'CT', 'sample_date', 'seasonalDate', 'week', 'day0_or_day7_sample', 'participant_ID', 'age', 'gender', 'time_of_symptom_onset', 'recieved_flu_vaccine', 'school', 'jackson_score', 'antipyretic_use', 'flu_antiviral_treatment', 'school_absence', 'number_in_household', 'recent_travel', 'isIndexCase', '%ofHHinfected', 'age_category', 'days_since_symptom_onset', 'symptom_severity', 'saw_a_doctor', 'num_of_snps', 'num_of_mixed_muts', 'num_of_nonsynon_muts', 'num_of_oof_muts', 'num_of_synon_muts', 'Xue_divergence', 'Xue_mixed_divergence', 'Xue_nonsynon_divergence', 'Xue_oof_divergence', 'Xue_synon_divergence', 'pi_sample', 'num_N_sites_samp', 'num_S_sites_samp', 'piN_sample', 'piS_sample', 'snps_per_day_samp', 'synon_snps_per_day_samp', 'nonsynon_snps_per_day_samp', 'oof_snps_per_day_samp', 'mutation_rate_samp', 'synon_mutation_rate_samp', 'nonsynon_mutation_rate_samp', 'Xue_divergence_per_day', 'Xue_synon_divergence_per_day', 'Xue_nonsynon_divergence_per_day', 'divergence_rate', 'synon_divergence_rate', 'nonsynon_divergence_rate', 'log(piN/piS)']\n"
     ]
    }
   ],
   "source": [
    "### Make transmission DF\n",
    "\n",
    "transmissionPairs = []\n",
    "indexsamples=[]\n",
    "problems = []\n",
    "tosequence = pd.DataFrame()\n",
    "\n",
    "print ('loading pre-calculated sample distances...')\n",
    "distanceDF = pd.DataFrame()\n",
    "for distancefile in distancefiles:\n",
    "    tmp = pd.read_csv(distancefile, **read_tsv_args).set_index('Unnamed: 0')\n",
    "    distanceDF = distanceDF.append(tmp, sort=True)\n",
    "\n",
    "for i, df in subjects.groupby('household'):\n",
    "    indexcase = df.loc[df.isIndexCase]\n",
    "    if len(indexcase) == 0:\n",
    "        continue\n",
    "   \n",
    "    indexSample = indexcase.day0_sample.values[0]\n",
    "    \n",
    "    for _, pt in df.iterrows():\n",
    "        if pd.notna(pt.day0_sample) & pd.notna(pt.day7_sample):    \n",
    "            transmissionPairs.append({'index':pt.day0_sample,'contact':pt.day7_sample, 'kind':'self','distance':distanceDF.at[pt.day0_sample,pt.day7_sample]})\n",
    "        if not pt.isIndexCase and pd.notna(indexSample):\n",
    "            if pd.notna(pt.day0_sample):\n",
    "                transmissionPairs.append({'index':indexSample,'contact':pt.day0_sample, 'kind':'transmission','distance':distanceDF.at[indexSample,pt.day0_sample]})\n",
    "            if pd.notna(pt.day7_sample):\n",
    "                transmissionPairs.append({'index':indexSample,'contact':pt.day7_sample, 'kind':'transmission','distance':distanceDF.at[indexSample,pt.day7_sample]})\n",
    "\n",
    "transmissionPairs = pd.DataFrame(transmissionPairs).dropna()\n",
    "transmissionPairs = transmissionPairs.merge(samples.set_index('sampleID'), how='left', left_on=['index'], right_index=True, suffixes=('','_index'))\n",
    "\n",
    "transmissionPairs = transmissionPairs.merge(samples.set_index('sampleID'), how='left', left_on=['contact'], right_index=True, suffixes=('_index','_contact'))\n",
    "transmissionPairs = checkForDuplicateColumnsPostMerge(transmissionPairs,suffixes=('_index','_contact'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indextp = transmissionPairs[['index','contact']].merge(SNPs.rename(columns={'sampleID':'index'}),on='index', how='left')\n",
    "# contacttp = transmissionPairs[['index','contact']].merge(SNPs.rename(columns={'sampleID':'contact'}),on='contact', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Empty DataFrame\n",
      "Columns: [index, contact, chrom, pos, ref_nuc, alt_nuc, qual, GT_index, GQ_index, SDP_index, depth_index, RD_index, AD_index, SNP_frequency_index, PVAL_index, RBQ_index, ABQ_index, RDF_index, RDR_index, ADF_index, ADR_index, product, inGenePos, refAA, codon, altAA, AAstr_index, AAtype_index, referenceFile_index, segment_index, subtype_index, CT_index, sample_date_index, decimalDate_index, seasonalDate_index, week_index, season_index, household_index, participant_ID_index, age_index, gender_index, time_of_symptom_onset_index, recieved_flu_vaccine_index, school_index, jackson_score_index, antipyretic_use_index, flu_antiviral_treatment_index, school_absence_index, number_in_household_index, recent_travel_index, isIndexCase_index, %ofHHinfected_index, age_category_index, days_since_symptom_onset_index, symptom_severity_index, saw_a_doctor_index, taken_on_hh_infection_day_index, clade_index, subclade_index, transformed_frequency_index, log_transformed_frequency_index, rolling_AA_mean_global_freq_index, rolling_AA_max_global_freq_index, rolling_DNA_mean_global_freq_index, rolling_DNA_max_global_freq_index, rolling_AA_mean_global_freq_following_year_index, rolling_AA_max_global_freq_following_year_index, rolling_DNA_mean_global_freq_following_year_index, rolling_DNA_max_global_freq_following_year_index, AAobservedGlobally_index, AAobservedGloballyFollowingSeason_index, DNAobservedGlobally_index, DNAobservedGloballyFollowingSeason_index, antigenic_site_index, antigenic_product_index, snpKey_index, pi_nucleotide_index, class_of_mutation_at_site_index, N_sites_gene_index, S_sites_gene_index, num_N_sites_samp_index, num_S_sites_samp_index, SNPkey, GT_contact, GQ_contact, SDP_contact, depth_contact, RD_contact, AD_contact, SNP_frequency_contact, PVAL_contact, RBQ_contact, ABQ_contact, RDF_contact, RDR_contact, ADF_contact, ADR_contact, AAstr_contact, AAtype_contact, referenceFile_contact, ...]\n",
      "Index: []\n",
      "updated:\n",
      "['index', 'contact', 'chrom', 'pos', 'ref_nuc', 'alt_nuc', 'qual', 'product', 'inGenePos', 'refAA', 'codon', 'altAA', 'AAstr', 'AAtype', 'referenceFile', 'segment', 'subtype', 'decimalDate', 'season', 'household', 'flu_antiviral_treatment', 'number_in_household', 'recent_travel', '%ofHHinfected', 'taken_on_hh_infection_day', 'clade', 'subclade', 'rolling_AA_mean_global_freq', 'rolling_AA_max_global_freq', 'rolling_AA_mean_global_freq_following_year', 'rolling_AA_max_global_freq_following_year', 'rolling_DNA_max_global_freq_following_year', 'AAobservedGlobally', 'AAobservedGloballyFollowingSeason', 'DNAobservedGlobally', 'DNAobservedGloballyFollowingSeason', 'antigenic_site', 'antigenic_product', 'class_of_mutation_at_site']\n",
      "untouched:\n",
      "['GT', 'GQ', 'SDP', 'depth', 'RD', 'AD', 'SNP_frequency', 'PVAL', 'RBQ', 'ABQ', 'RDF', 'RDR', 'ADF', 'ADR', 'CT', 'sample_date', 'seasonalDate', 'week', 'participant_ID', 'age', 'gender', 'time_of_symptom_onset', 'recieved_flu_vaccine', 'school', 'jackson_score', 'antipyretic_use', 'school_absence', 'isIndexCase', 'age_category', 'days_since_symptom_onset', 'symptom_severity', 'saw_a_doctor', 'transformed_frequency', 'log_transformed_frequency', 'rolling_DNA_mean_global_freq', 'rolling_DNA_max_global_freq', 'rolling_DNA_mean_global_freq_following_year', 'snpKey', 'pi_nucleotide', 'N_sites_gene', 'S_sites_gene', 'num_N_sites_samp', 'num_S_sites_samp']\n"
     ]
    }
   ],
   "source": [
    "indextp = transmissionPairs[['index','contact']].merge(SNPs.rename(columns={'sampleID':'index'}),on='index', how='left')\n",
    "contacttp = transmissionPairs[['index','contact']].merge(SNPs.rename(columns={'sampleID':'contact'}),on='contact', how='left')\n",
    "\n",
    "indextp['SNPkey'] = indextp['index'] + ':' + indextp['contact'] + ':'+indextp.segment+':'+indextp.pos.astype(str)+':'+indextp.alt_nuc+':'+indextp['product'].fillna('OORF')\n",
    "contacttp['SNPkey'] = contacttp['index'] + ':' + contacttp['contact'] + ':'+contacttp.segment+':'+contacttp.pos.astype(str)+':'+contacttp.alt_nuc+':'+contacttp['product'].fillna('OORF')\n",
    "\n",
    "assert len(indextp.SNPkey.unique())==len(indextp)\n",
    "assert len(contacttp.SNPkey.unique())==len(contacttp)\n",
    "\n",
    "tSNPs = indextp.merge(contacttp, on='SNPkey', how='outer', suffixes=('_index','_contact'))\n",
    "# tSNPs.loc[tSNPs.index_index != tSNPs.index_contact, ['index_index','index_contact','contact_index','contact_contact']]\n",
    "tSNPs = updateDuplicateColumnsPostMerge(tSNPs, suffixes=('_index','_contact'))\n",
    "tSNPs = tSNPs.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tSNPs.SNP_frequency_index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated:\n",
      "['subtype', 'CT_index', 'sample_date_index', 'decimalDate', 'seasonalDate_index', 'week_index', 'season', 'household', 'participant_ID_index', 'age_index', 'gender_index', 'time_of_symptom_onset_index', 'recieved_flu_vaccine_index', 'school_index', 'jackson_score_index', 'antipyretic_use_index', 'school_absence_index', 'isIndexCase_index', 'age_category_index', 'days_since_symptom_onset_index', 'symptom_severity_index', 'saw_a_doctor_index', 'taken_on_hh_infection_day', 'clade', 'subclade', 'num_N_sites_samp_index', 'num_S_sites_samp_index', 'CT_contact', 'sample_date_contact', 'seasonalDate_contact', 'week_contact', 'participant_ID_contact', 'age_contact', 'gender_contact', 'time_of_symptom_onset_contact', 'recieved_flu_vaccine_contact', 'school_contact', 'jackson_score_contact', 'antipyretic_use_contact', 'school_absence_contact', 'isIndexCase_contact', 'age_category_contact', 'days_since_symptom_onset_contact', 'symptom_severity_contact', 'saw_a_doctor_contact', 'num_N_sites_samp_contact', 'num_S_sites_samp_contact']\n",
      "untouched:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "transmissionSNPs = tSNPs.merge(transmissionPairs, on=['index','contact'])#.rename(columns={'subtype_y':'subtype'})\n",
    "\n",
    "transmissionSNPs = updateDuplicateColumnsPostMerge(transmissionSNPs, suffixes=('_x','_y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tSNPs.loc[np.abs(tSNPs.SNP_frequency_contact-tSNPs.SNP_frequency_index)>0.30])# transmissionSNPs.loc[transmissionSNPs.AAstr_index != transmissionSNPs.AAstr_contact,['AAstr_index','AAstr_contact']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transmissionSNPs.SNP_frequency_index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SNPs.loc[(SNPs.sampleID=='19VR004458')&(SNPs.segment=='HA')]\n",
    "# withMixedRemoved[['index','contact','refAA','codon','altAA_index','antigenic_product']].loc[withMixedRemoved.refAA!=withMixedRemoved.altAA_index].sort_values('codon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Figuring out where G145S and I230V went\n",
    "# hhs = list()\n",
    "# ptIDs = list()\n",
    "# for hh, df in subjects.groupby('household'):\n",
    "#     if len(df)>=3:\n",
    "#         hhs.append(hh)\n",
    "#         for id in df['participant_ID']:\n",
    "#             ptIDs.append(id)\n",
    "# sampleIDs = list(samples.loc[samples.participant_ID.isin(ptIDs), 'sampleID'])\n",
    "# SNPs.loc[(SNPs.codon==245)&(SNPs['segment']=='HA')&(SNPs['sampleID'].isin(sampleIDs))]\n",
    "# sampleIDs = ['19VR006682','19VR007474']\n",
    "# ptIDs = [1818.0,1817.2]\n",
    "# hh = 1817\n",
    "\n",
    "# # transmissionPairs.loc[(transmissionPairs['index'].isin(samps))|(transmissionPairs['contact'].isin(samps))]\n",
    "# contactsOfInterest = ['19VR006682','19VR007474']\n",
    "# indexOfInterest = ['19VR006679']\n",
    "# householdOfInterest = 1817\n",
    "# subjects.loc[subjects.household == hh, 'isIndexCase']\n",
    "# reffiles\n",
    "# len(_consensus_noambig.fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-175-7da69faea119>:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bottleneckpairingDF['bottleneck_size'] = 0\n",
      "<ipython-input-175-7da69faea119>:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bottleneckpairingDF['bottleneck_min'] = 0\n",
      "<ipython-input-175-7da69faea119>:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bottleneckpairingDF['bottleneck_max'] = 0\n",
      "<ipython-input-175-7da69faea119>:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bottleneckpairingDF['supportingSNPs'] = 0\n",
      "100%|██████████| 81/81 [01:28<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rscript /d/orchards/betaBinomial/Bottleneck_size_estimation_exact.r --file /mnt/d/orchards/h1n1/figures/bottleneck_figures/17VR005325_17VR005779.txt --plot_bool TRUE --var_calling_threshold 0.01 --Nb_min 1 --Nb_max 200 --confidence_level .95\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-7da69faea119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mpairingfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairingfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkoelleBottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairingfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpairingfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairingfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowerbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupperbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-7da69faea119>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mpairingfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairingfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkoelleBottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairingfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpairingfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairingfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowerbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupperbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-7da69faea119>\u001b[0m in \u001b[0;36mkoelleBottleneck\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'{pair} doesn\\'t have an exact plot svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mbottleneck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneckregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mlowerbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowerboundregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mupperbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupperboundregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "#Calculate bottleneck size:\n",
    "def getReadDepth(sample, segment, pos, alt):\n",
    "    reffile = SNPs.loc[SNPs['sampleID']==sample, 'referenceFile'].iloc[0]\n",
    "    ref = reffile.split('/')[5]\n",
    "    refbase = reffile.split('/')[-1].split('_')\n",
    "    if 'Hong_Kong' in reffile:\n",
    "        chrom = hongkongContigs[segment]\n",
    "    elif 'Michigan' in reffile:\n",
    "        chrom = '_'.join(refbase[:-4])+'_'+segment\n",
    "    elif refbase[-3] in ['17','18','19']:\n",
    "        chrom = '_'.join(refbase[:-3])+'_'+segment\n",
    "    else:\n",
    "        chrom = '_'.join(refbase[:-2])+'_'+segment\n",
    "\n",
    "    bamfile = '/'.join(reffile.split('/')[0:6])+'/'+'_'.join(reffile.split('/')[-1].split('_')[:-2])+'/map_to_consensus/'+sample+'.bam'\n",
    "    pos = int(pos)\n",
    "    sam = pysam.AlignmentFile(bamfile, \"rb\")\n",
    "    try:\n",
    "        pileup = sam.pileup(contig=chrom, start=pos, end=pos+1, truncate=True, stepper=\"nofilter\")\n",
    "    except:\n",
    "        print (sam.references)\n",
    "        print (chrom)\n",
    "        print (reffile)\n",
    "        print (ref)\n",
    "        raise\n",
    "        \n",
    "    column = next(pileup)\n",
    "    column.set_min_base_quality(30)\n",
    "    try:\n",
    "        bases = column.get_query_sequences(mark_matches=True)\n",
    "        altreads = bases.count(alt.lower()) + bases.count(alt.upper())\n",
    "    except:\n",
    "        altreads = 0\n",
    "    frequency = round(altreads/column.get_num_aligned(),4)\n",
    "    depth = column.get_num_aligned()\n",
    "    return frequency, altreads, depth\n",
    "\n",
    "def makeBottleneckInputFile(i, pairing):\n",
    "    index, contact = pairing\n",
    "    indexdata = SNPs.loc[SNPs['sampleID'] == index, ['segment', 'pos', 'SNP_frequency', 'AD', 'depth','alt_nuc']].groupby(['segment','pos']).first()\n",
    "    contactdata = SNPs.loc[SNPs['sampleID'] == contact, ['segment', 'pos', 'SNP_frequency', 'AD', 'depth','alt_nuc']].groupby(['segment','pos']).first()\n",
    "    export = indexdata.merge(contactdata, on=['segment','pos'], how='left', suffixes=('_index','_contact'))\n",
    "    for ix, row in export.iterrows():\n",
    "        if pd.isna(row.depth_contact):\n",
    "            export.loc[ix,['SNP_frequency_contact','AD_contact','depth_contact']] = getReadDepth(contact, ix[0],ix[1],row.alt_nuc_contact)\n",
    "    export.fillna(0)\n",
    "    filename = figures+'/bottleneck_figures/'+index+'_'+contact+'.txt'\n",
    "    export.to_csv(filename[:-4]+'.tsv', sep='\\t')\n",
    "    export = export[['SNP_frequency_index','SNP_frequency_contact','depth_contact','AD_contact']].round(4)\n",
    "    export = export.loc[(0.99 > export.SNP_frequency_index) & (export.SNP_frequency_index > 0.01)]\n",
    "    export = export.loc[~export.duplicated()]\n",
    "    bottleneckpairingDF.at[i, 'supportingSNPs'] = len(export)\n",
    "    export.to_csv(filename, sep='\\t', header=False, index=False)\n",
    "    return filename\n",
    "\n",
    "def koelleBottleneck(filename):\n",
    "    #print (filename)\n",
    "    bottleneckregex = r\"(?:size\\n)(\\d*)\"\n",
    "    lowerboundregex = r\"(?:left bound\\n)(\\d*)\"\n",
    "    upperboundregex = r\"(?:right bound\\n)(\\d*)\"\n",
    "    pair = filename.split('.')[0].split('/')[-1]\n",
    "    #print (f'Getting bottleneck of transmission pair {pair}')\n",
    "    with open(f\"{figures}/betabinomialResults_exact.log\", 'a+') as outputFile:\n",
    "        cmd = f'Rscript /d/orchards/betaBinomial/Bottleneck_size_estimation_exact.r --file {filename} --plot_bool TRUE --var_calling_threshold {SNP_frequency_cutoff} --Nb_min 1 --Nb_max 200 --confidence_level .95'\n",
    "        outputFile.write(f\"\\n\\n--------------------\\n\\n{pair}\\n\\n\")\n",
    "        print (cmd)\n",
    "        results = subprocess.run(cmd.split(\" \"), text=True, stdout=subprocess.PIPE)\n",
    "        try:\n",
    "            os.rename('/mnt/d/orchards/betaBinomial/exact_plot.svg', f'{figures}/{pair}_bottleneckplot_exact.svg')\n",
    "        except:\n",
    "            print (f'{pair} doesn\\'t have an exact plot svg')\n",
    "        bottleneck = int(re.search(bottleneckregex, results.stdout).group(1))\n",
    "        lowerbound = int(re.search(lowerboundregex, results.stdout).group(1))\n",
    "        upperbound = int(re.search(upperboundregex, results.stdout).group(1))\n",
    "        print (f'{pair}: {lowerbound} |-- {bottleneck} --| {upperbound}')\n",
    "    return pair, bottleneck, lowerbound, upperbound\n",
    "\n",
    "if recalc_bottlenecks: #calc bottleneck\n",
    "    bottleneckpairingDF = transmissionPairs[['index','contact']]\n",
    "    bottleneckpairingDF['bottleneck_size'] = 0\n",
    "    bottleneckpairingDF['bottleneck_min'] = 0\n",
    "    bottleneckpairingDF['bottleneck_max'] = 0\n",
    "    bottleneckpairingDF['supportingSNPs'] = 0\n",
    "    \n",
    "    pairingfiles = []\n",
    "    for i, row in tqdm(bottleneckpairingDF.iterrows(),total=len(bottleneckpairingDF)):\n",
    "        pairing = (row['index'],row.contact)\n",
    "        pairingfile = makeBottleneckInputFile(i, pairing)\n",
    "        pairingfiles.append(pairingfile)\n",
    "\n",
    "    for result in [koelleBottleneck(pairingfile) for pairingfile in pairingfiles]:\n",
    "        pair, bottleneck, lowerbound, upperbound = result\n",
    "        index, contact = tuple(pair.split('_'))\n",
    "        bottleneckpairingDF.loc[(bottleneckpairingDF['index'] == index) & (bottleneckpairingDF['contact'] == contact), ['bottleneck_size', 'bottleneck_min', 'bottleneck_max']] = bottleneck, lowerbound, upperbound\n",
    "\n",
    "#     with mp.Pool(6) as pool:\n",
    "#         for result in pool.imap_unordered(koelleBottleneck, pairingfiles):\n",
    "#             pair, bottleneck, lowerbound, upperbound = result\n",
    "#             print (result)\n",
    "#             index, contact = tuple(pair.split('_'))\n",
    "#             bottleneckpairingDF.loc[(bottleneckpairingDF['index'] == index) & (bottleneckpairingDF['contact'] == contact), ['bottleneck_size', 'bottleneck_min', 'bottleneck_max']] = bottleneck, lowerbound, upperbound\n",
    "\n",
    "\n",
    "    \n",
    "    bottleneckpairingDF.to_csv(figures+'/bottleneckpairingDF.tsv', sep='\\t', index=False)\n",
    "else:\n",
    "    bottleneckpairingDF = pd.read_csv(figures+'/bottleneckpairingDF.tsv', **read_tsv_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len([column for column in transmissionPairs.columns if '_x' in column])>0:\n",
    "    print (transmissionPairs.columns)\n",
    "    raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmissionSNPs.SNP_frequency_index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmissionPairs = transmissionPairs.merge(bottleneckpairingDF, on=['index','contact'])\n",
    "# print(transmissionPairs.columns)\n",
    "transmissionPairs = checkForDuplicateColumnsPostMerge(transmissionPairs,suffixes=('_x','_y'))\n",
    "# tocombine = ['household','number_in_household','week','clade','decimalDate','seasonalDate']\n",
    "# todelete = ['mapped_index','mapped_contact']\n",
    "# for column in tocombine:\n",
    "#     transmissionPairs[column+'_index'].update(transmissionPairs[column+'_contact'])\n",
    "#     transmissionPairs = transmissionPairs.drop(column+'_contact', axis=1).rename(columns={column+'_index':column})\n",
    "\n",
    "# transmissionPairs = transmissionPairs.drop(columns = todelete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-calculate change in variables across transmission\n",
    "# columnstodelta = set()\n",
    "# for column in transmissionPairs.columns:\n",
    "#     if '_index' in str(column):\n",
    "#         columnstodelta.add(column.replace('_index',''))\n",
    "#     if transmissionPairs[column].dtype != 'datetime64[ns]':\n",
    "#         try:\n",
    "#             transmissionPairs[column] = pd.to_numeric(transmissionPairs[column])\n",
    "#         except:\n",
    "#             pass\n",
    "# print ('merging columns')\n",
    "# for column in columnstodelta:\n",
    "#     if transmissionPairs[column+'_index'].dtype == 'datetime64[ns]':\n",
    "#         transmissionPairs[column+'_difference'] = (transmissionPairs[column+'_contact']-transmissionPairs[column+'_index']).dt.days\n",
    "#     else:\n",
    "#         try:\n",
    "#             transmissionPairs[column+'_difference'] = transmissionPairs[column+'_contact'] - transmissionPairs[column+'_index']\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "\n",
    "        \n",
    "# toDrop = ['isIndexCase_index','isIndexCase_contact','%ofHHinfected_contact','%ofHHinfected_difference','participant_ID_difference']\n",
    "# toRename = {'ofHHinfected_index':'ofHHinfected'}\n",
    "\n",
    "# transmissionPairs = transmissionPairs.drop(toDrop, axis=1).rename(columns=toRename)\n",
    "#(transmissionPairs.time_of_symptom_onset_contact - transmissionPairs.index_symptom_start_date_contact).dt.days\n",
    "# transmissionPairs = checkForDuplicateColumnsPostMerge(transmissionPairs,suffixes=('_index','_contact'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmissionSNPs.SNP_frequency_index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transmissionSegments = transmissionPairs.merge(segments, right_on=['sampleID'], left_on=['index'], how='left', suffixes=('','_index'))\n",
    "transmissionSegments = transmissionSegments.merge(segments,left_on=['contact','segment'], right_on=['sampleID', 'segment'], how='left', suffixes=('_index','_contact'))\n",
    "transmissionSegments = transmissionSegments.loc[:,~transmissionSegments.columns.duplicated()]\n",
    "todelete = ['mapped_index','mapped_contact','sampleID_index','sampleID_contact']\n",
    "\n",
    "\n",
    "transmissionSegments = transmissionSegments.drop(columns = todelete)\n",
    "transmissionSegments = checkForDuplicateColumnsPostMerge(transmissionSegments,suffixes=('_index','_contact'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sampleSpecificSNPcols = ['index','contact','ABQ','AD','ADF','ADR','depth','SNP_frequency','GQ','GT','PVAL','RBQ','RD','RDF','RDR','SDP','qual','transformed_frequency','log_transformed_frequency','pi_nucleotide']\n",
    "\n",
    "# tP_mergeOnCols = [column for column in transmissionPairs.columns if column not in SNPs.columns]\n",
    "# common_cols = [column for column in SNPs.columns if column in samples.columns]\n",
    "# SNPs_mergeOnCols = [column for column in SNPs.columns if column not in sampleSpecificSNPcols+common_cols]\n",
    "\n",
    "# a = transmissionPairs[['index','contact']].merge(SNPs,how='left',left_on='index',right_on='sampleID')\n",
    "# b = transmissionPairs[['index','contact']].merge(SNPs,how='left',left_on='contact',right_on='sampleID')\n",
    "\n",
    "# a = a.drop(columns=common_cols)\n",
    "# b = b.drop(columns=common_cols)\n",
    "\n",
    "# codingA = a.loc[~(a.AAtype=='Out of reading frame')]\n",
    "# codingB = b.loc[~(b.AAtype=='Out of reading frame')]\n",
    "# NC_A = a.loc[(a.AAtype=='Out of reading frame')]\n",
    "# NC_B = b.loc[(b.AAtype=='Out of reading frame')]\n",
    "\n",
    "\n",
    "# t = codingA.merge(codingB, on=['index','contact','product','inGenePos'], how='outer',suffixes=('_index','_contact'))\n",
    "# OORFtSNPs = NC_A.merge(NC_B, on=['index','contact','segment','pos'], how='outer',suffixes=('_index','_contact'))\n",
    "# transmissionSNPs = t.loc[~((t.alt_nuc_index != t.alt_nuc_contact)&pd.notna(t.alt_nuc_index)&pd.notna(t.alt_nuc_contact))]\n",
    "\n",
    "# transmissionSNPs = updateDuplicateColumnsPostMerge(transmissionSNPs,suffixes=('_index','_contact'))\n",
    "# OORFtSNPs = updateDuplicateColumnsPostMerge(OORFtSNPs,suffixes=('_index','_contact'))\n",
    "\n",
    "\n",
    "# # print([col for col in t.columns if 'pos' in col])\n",
    "\n",
    "\n",
    "\n",
    "# transmissionSNPs = transmissionSNPs.append(OORFtSNPs)\n",
    "\n",
    "# # print(transmissionSNPs['pos'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #The rolling columns are not always identical due to sampling taking place during different periods.\n",
    "# #The differences are too large to just ignore, but too small to actually matter. (on the order of 1E-2).\n",
    "# #So I'll merge all columns with rolling in them by taking the average\n",
    "# rollingcolumns = [column for column in transmissionSNPs.columns if (('rolling' in column) or ('Globally' in column)) and ('_index' in column)]\n",
    "# for column in rollingcolumns:\n",
    "#     iColumn = column\n",
    "#     cColumn = column[:-6]+'_contact'\n",
    "#     newcolumn = column[:-6]\n",
    "#     transmissionSNPs[iColumn].update(transmissionSNPs[cColumn])\n",
    "#     transmissionSNPs[newcolumn] = transmissionSNPs[[iColumn,cColumn]].mean(axis=1)\n",
    "#     transmissionSNPs = transmissionSNPs.drop(columns=[iColumn,cColumn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transmissionSNPs = transmissionSNPs.merge(transmissionPairs, on=['index', 'contact'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# c[['index','contact','segment','pos','SNP_frequency_index','SNP_frequency_contact','CT_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortIndexSnps = indexSNPs[['index','segment','pos','SNP_frequency']]\n",
    "# shortContactSnps = contactSNPs[['contact','segment','pos','SNP_frequency']]\n",
    "# pairing = transmissionPairs[['index','contact']]\n",
    "# tmp = shortIndexSnps.merge(pairing, on='index')\n",
    "# shortContactSnps.merge(tmp, on=['contact', 'segment','pos'],how='outer',suffixes=('_index','_contact'))\n",
    "\n",
    "# sharedColumns = [column for column in transmissionPairs.columns if column in SNPs]\n",
    "# snpSpecifyingColumns = ['index','contact','segment','antigenic_site','antigenic_product','chrom','referenceFile','codon', 'AAstr', 'AAtype', 'alt_nuc', 'altAA', 'product', 'inGenePos', 'pos', 'ref_nuc', 'refAA', 'rolling_AA_mean_global_freq','rolling_AA_max_global_freq','rolling_DNA_mean_global_freq','rolling_DNA_max_global_freq','rolling_AA_mean_global_freq_following_year','rolling_AA_max_global_freq_following_year','rolling_DNA_mean_global_freq_following_year','rolling_DNA_max_global_freq_following_year','AAobservedGlobally','AAobservedGloballyFollowingSeason','DNAobservedGlobally','DNAobservedGloballyFollowingSeason','class_of_mutation_at_site']\n",
    "\n",
    "# a = indexSNPs.merge(pairing)\n",
    "\n",
    "# b = contactSNPs.merge(pairing)\n",
    "# c = a.merge(b,on=snpSpecifyingColumns+sharedColumns,how='outer',suffixes=('_index','_contact'))\n",
    "# print(c.select_dtypes(include='object').columns)\n",
    "# c = c.apply(pd.to_numeric, c.select_dtypes(include='object').columns,errors='ignore')\n",
    "# transmissionPairs = transmissionPairs.apply(pd.to_numeric, transmissionPairs.select_dtypes(include='object').columns,errors='ignore')\n",
    "# sharedColumns = [column for column in transmissionPairs.columns if column in c.columns]\n",
    "# # print(sharedColumns[:3])\n",
    "# # c[sharedColumns[:3]].dtypes\n",
    "# # a.dtypes\n",
    "# c.merge(transmissionPairs, on=sharedColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# transmissionSNPs = transmissionSegments.merge(SNPs, left_on=['index','segment'], right_on=['sampleID', 'segment'], how='left')\n",
    "# transmissionSNPs = transmissionSNPs.merge(SNPs, left_on=['contact','segment']+snpSpecificColumns, right_on=['sampleID', 'segment']+snpSpecificColumns, how='left', suffixes=('_index','_contact'))\n",
    "\n",
    "# #If there's a segment with no SNPs, then this merge leaves a blank entry with only the segment information filled in.\n",
    "# #This line of code removes those entries.\n",
    "# # transmissionSNPs = transmissionSNPs.loc[pd.notna(transmissionSNPs.SNP_frequency_index) & pd.notna(transmissionSNPs.SNP_frequency_contact)]\n",
    "\n",
    "# transmissionSNPs = transmissionSNPs.drop(columns=['sampleID_index','sampleID_contact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = [column for column in transmissionColumns if column not in ['index','contact']]\n",
    "# # list(indexSNPs.merge(contactSNPs, on=['index','contact']+snpSpecificColumns+transmissionColumns[40:41], how='outer',suffixes=('_index','_contact')).columns)\n",
    "# transmissionColumns[:6] #6,13:16\n",
    "# tmpcol = [column for column in indexSNPs.columns if column in contactSNPs.columns]\n",
    "# indexSNPs.merge(contactSNPs, on=tmpcol, how='outer',suffixes=('_index','_contact'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transmissionSNPs.iloc[1388:1390][['index','contact']]\n",
    "# print(list(transmissionPairs.contact))\n",
    "# SNPs.loc[SNPs.sampleID=='19VR009188']\n",
    "transmissionSNPs.SNP_frequency_index.max()\n",
    "for sample in list(transmissionPairs.contact):\n",
    "    print(f'{sample}: {len(SNPs.loc[(SNPs.sampleID==sample)&(SNPs.SNP_frequency < 0.99)&(SNPs.SNP_frequency > 0.01)])}')\n",
    "    \n",
    "# sns.swarmplot(SNPs.loc[(SNPs.SNP_frequency < 0.98)&(SNPs.SNP_frequency > 0.02)].groupby('sampleID').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReadDepthWrapper(row):\n",
    "    if pd.isna(row.SNP_frequency_index):\n",
    "        try:\n",
    "            result = getReadDepth(row['index'], row.segment,row.pos,row.alt_nuc)+(row.SNP_frequency_contact,row.AD_contact,row.depth_contact)\n",
    "        except:\n",
    "            print (row[['index','contact','segment','pos','SNP_frequency_index','SNP_frequency_contact']])\n",
    "            raise\n",
    "    elif pd.isna(row.SNP_frequency_contact):\n",
    "        try:\n",
    "            result = (row.SNP_frequency_index,row.AD_index,row.depth_index)+getReadDepth(row.contact, row.segment,row.pos,row.alt_nuc)\n",
    "        except:\n",
    "            print (row)  \n",
    "            raise\n",
    "    else:\n",
    "        result = (row.SNP_frequency_index,row.AD_index,row.depth_index,row.SNP_frequency_contact,row.AD_contact,row.depth_contact)\n",
    "        \n",
    "    if len(result) != 6:\n",
    "#         print (row)\n",
    "        print (result)\n",
    "    return result\n",
    "\n",
    "# # transmissionSNPs[['SNP_frequency_index','AD_index','depth_index','SNP_frequency_contact','AD_contact','depth_contact']]\n",
    "# # getReadDepthWrapper(transmissionSNPs.iloc[0])\n",
    "# transmissionSNPs['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_coverage = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'd like to fill in my transmissionSNP depth data. It will make computing bottlenecks more straightforward, among other things.\n",
    "#I'm doing this by applying a function to all transmission SNPs that:\n",
    "#a) determines whether the index or contact frequency/AD/depth info contains nans\n",
    "#b) calls getReadDepth on the appropriate information to fill in the nans\n",
    "#c) returns the original data with getReadDepth's results filling in the nans\n",
    "columnsToUpdate = ['SNP_frequency_index','AD_index','depth_index','SNP_frequency_contact','AD_contact','depth_contact']\n",
    "# tmp = transmissionSNPs.apply(getReadDepthWrapper,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(transmissionSNPs.apply(getReadDepthWrapper,axis=1).to_list())\n",
    "\n",
    "#It makes me nervous that I'm applying a function to all my values which in theory could change all my SNP values.\n",
    "#So I'm going to do this carefully. I will apply the function and create a separate data frame, preserving my original data.\n",
    "#I then assert that the data that I am about to change is either a) identical to the new data, or b) nan\n",
    "a = transmissionSNPs[columnsToUpdate].to_numpy()\n",
    "b = tmp.to_numpy()\n",
    "\n",
    "assert ((a==b) | np.isnan(a)).all()\n",
    "\n",
    "#Once that's confirmed, I replace my original data with my updated data\n",
    "transmissionSNPs[columnsToUpdate] = b\n",
    "\n",
    "# To save time, I do not get reference SNP depth, just total depth and alt depth. \n",
    "# Both are calculated w/ quality minimums, so ref_depth is just total depth - alt depth\n",
    "# I'm only changing values that are nan, otherwise I will use the info previously gathered\n",
    "transmissionSNPs.loc[transmissionSNPs.RD_index.isna(), 'RD_index'] = transmissionSNPs.loc[transmissionSNPs.RD_index.isna(), 'depth_index']-transmissionSNPs.loc[transmissionSNPs.RD_index.isna(), 'AD_index']\n",
    "transmissionSNPs.loc[transmissionSNPs.RD_contact.isna(), 'RD_contact'] = transmissionSNPs.loc[transmissionSNPs.RD_contact.isna(), 'depth_contact']-transmissionSNPs.loc[transmissionSNPs.RD_contact.isna(), 'AD_contact']\n",
    "\n",
    "assert(len(transmissionSNPs.loc[transmissionSNPs.RD_index.isna()])==0)\n",
    "assert(len(transmissionSNPs.loc[transmissionSNPs.RD_contact.isna()])==0)\n",
    "assert(len(transmissionSNPs.loc[transmissionSNPs.AD_index.isna()])==0)\n",
    "assert(len(transmissionSNPs.loc[transmissionSNPs.AD_contact.isna()])==0)\n",
    "\n",
    "#Now that I have completely filled in depth data, make sure both index and contact meet minimum coverage requirements\n",
    "min_coverage = 100\n",
    "transmissionSNPs = transmissionSNPs.loc[(transmissionSNPs.depth_index >= min_coverage)&(transmissionSNPs.depth_contact >= min_coverage)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now that my nans are filled in, I calculated differences in snp frequency\n",
    "transmissionSNPs['abs_SNP_frequency_difference'] = np.abs(transmissionSNPs.SNP_frequency_contact-transmissionSNPs.SNP_frequency_index)\n",
    "transmissionSNPs['SNP_frequency_directional_change'] = transmissionSNPs.SNP_frequency_contact-transmissionSNPs.SNP_frequency_index\n",
    "transmissionSNPs['log_abs_SNP_frequency_difference'] = np.log10(transmissionSNPs.abs_SNP_frequency_difference).fillna(0).replace((np.inf), 0).replace((-np.inf),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transmissionSNPs.loc[transmissionSNPs.abs_SNP_frequency_difference>0.99])\n",
    "# transmissionSNPs.loc[(transmissionSNPs.SNP_frequency_contact>0.1)&(transmissionSNPs.SNP_frequency_index<0.1),['SNP_frequency_index','SNP_frequency_contact']]\n",
    "\n",
    "# transmissionSNPs.SNP_frequency_index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('saving transmission pairs...')\n",
    "transmissionPairs.to_csv(basename+'/transmissionPairs.tsv', sep='\\t', index=False)\n",
    "print ('saving transmission segments...')\n",
    "transmissionSegments.to_csv(basename+'/transmissionSegments.tsv', sep='\\t', index=False)\n",
    "print ('saving transmission SNPs...')\n",
    "transmissionSNPs.to_csv(basename+'/transmissionSNPs_lenient_filter.gz', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfilteredSNPs = pd.read_csv(basename+'/SNPs_unfiltered.gz',**read_tsv_args)\n",
    "# unfilteredtSNPs = pd.read_csv(basename+'/transmissionSNPs_unfiltered.gz',**read_tsv_args)\n",
    "# filteredSNPs = pd.read_csv(basename+'/SNPs_lenient_filter.gz',**read_tsv_args)\n",
    "# filteredtSNPs  = pd.read_csv(basename+'/transmissionSNPs_lenient_filter.gz',**read_tsv_args)\n",
    "\n",
    "# filteredSNPs2 = pd.read_csv(basename+'/SNPs_lenient_filter.gz',**read_tsv_args)\n",
    "# filteredtSNPs2  = pd.read_csv(basename+'/transmissionSNPs_lenient_filter.gz',**read_tsv_args)\n",
    "\n",
    "# unfilteredtSNPs['abs_SNP_frequency_difference'] = np.abs(unfilteredtSNPs.SNP_frequency_contact-unfilteredtSNPs.SNP_frequency_index)\n",
    "# unfilteredtSNPs['log_abs_SNP_frequency_difference'] = np.log10(unfilteredtSNPs.abs_SNP_frequency_difference)\n",
    "\n",
    "# filteredtSNPs['abs_SNP_frequency_difference'] = np.abs(filteredtSNPs.SNP_frequency_contact-filteredtSNPs.SNP_frequency_index)\n",
    "# filteredtSNPs['log_abs_SNP_frequency_difference'] = np.log10(filteredtSNPs.abs_SNP_frequency_difference).replace(-np.inf, -100)\n",
    "\n",
    "# filteredtSNPs2['abs_SNP_frequency_difference'] = np.abs(filteredtSNPs2.SNP_frequency_contact-filteredtSNPs2.SNP_frequency_index)\n",
    "# filteredtSNPs2['log_abs_SNP_frequency_difference'] = np.log10(filteredtSNPs2.abs_SNP_frequency_difference).replace(-np.inf, -100)\n",
    "# print (len(unfilteredtSNPs), len(filteredtSNPs),len(filteredtSNPs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Unfiltered analysis\n",
    "# from scipy import stats\n",
    "# unfilteredIntraHostResults={gene:{} for gene in unfilteredSNPs.antigenic_product.unique()}\n",
    "\n",
    "# for gene, df in unfilteredSNPs.loc[(unfilteredSNPs.subtype=='H3N2')&(unfilteredSNPs.transformed_frequency>0.01)].groupby('antigenic_product'):\n",
    "#     df['AAtype'] = df.AAtype.map({'Nonsynonymous':1, 'Synonymous':0})\n",
    "#     m, b, r_value, p_value, std_err = stats.linregress(df[['log_transformed_frequency','AAtype']].to_numpy())\n",
    "# #     print (f'gene:{gene} m: {m}, b: {b}, r^2: {r_value**2}, pvalue: {p_value}, ci: {std_err*1.96}')\n",
    "#     unfilteredIntraHostResults[gene] = {'slope':m,'ci':std_err*1.96,'pvalue':p_value}\n",
    "#     sns.lmplot('log_transformed_frequency','AAtype',data=df)\n",
    "#     plt.title(gene)\n",
    "    \n",
    "# pd.DataFrame(unfilteredIntraHostResults).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfilteredtSNPs['abs_SNP_frequency_difference'] = np.abs(unfilteredtSNPs.SNP_frequency_contact-unfilteredtSNPs.SNP_frequency_index)\n",
    "# unfilteredtSNPs['log_abs_SNP_frequency_difference'] = np.log10(unfilteredtSNPs.abs_SNP_frequency_difference)\n",
    "# unfilteredtSNPs.loc[(unfilteredtSNPs.kind=='transmission')&(unfilteredtSNPs.subtype=='H3N2')&(unfilteredtSNPs.abs_SNP_frequency_difference>0.01)]\n",
    "# unFilteredTransmissionResults={gene:{} for gene in unfilteredtSNPs.antigenic_product.unique()}\n",
    "# for gene, df in unfilteredtSNPs.loc[(unfilteredtSNPs.kind=='transmission')&(unfilteredtSNPs.subtype=='H3N2')&(unfilteredtSNPs.abs_SNP_frequency_difference > 0.01)].groupby('antigenic_product'):\n",
    "#     df['AAtype'] = df.AAtype.map({'Nonsynonymous':1, 'Synonymous':0})\n",
    "#     m, b, r_value, p_value, std_err = stats.linregress(df[['log_abs_SNP_frequency_difference','AAtype']].to_numpy())\n",
    "# #     print (f'gene:{gene} m: {m}, b: {b}, r^2: {r_value**2}, pvalue: {p_value}, ci: {std_err*1.96}')\n",
    "#     unFilteredTransmissionResults[gene] = {'slope':m,'ci':std_err*1.96,'pvalue':p_value}\n",
    "#     sns.lmplot('log_abs_SNP_frequency_difference','AAtype',data=df)\n",
    "#     plt.title(gene)\n",
    "\n",
    "# pd.DataFrame(unFilteredTransmissionResults).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #filtered analysis\n",
    "# from scipy import stats\n",
    "# intraHostResults={gene:{} for gene in filteredSNPs.antigenic_product.unique()}\n",
    "\n",
    "# for gene, df in filteredSNPs.loc[(filteredSNPs.subtype=='H3N2')&(filteredSNPs.transformed_frequency>0.01)].groupby('antigenic_product'):\n",
    "#     df['AAtype'] = df.AAtype.map({'Nonsynonymous':1, 'Synonymous':0})\n",
    "#     m, b, r_value, p_value, std_err = stats.linregress(df[['log_transformed_frequency','AAtype']].to_numpy())\n",
    "# #     print (f'gene:{gene} m: {m}, b: {b}, r^2: {r_value**2}, pvalue: {p_value}, ci: {std_err*1.96}')\n",
    "#     intraHostResults[gene] = {'slope':m,'ci':std_err*1.96,'pvalue':p_value}\n",
    "#     sns.lmplot('log_transformed_frequency','AAtype',data=df)\n",
    "#     plt.title(gene)\n",
    "    \n",
    "# pd.DataFrame(intraHostResults).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filteredtSNPs[['abs_SNP_frequency_difference','log_abs_SNP_frequency_difference']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_transmissionResults={gene:{} for gene in filteredtSNPs.antigenic_product.unique()}\n",
    "# for gene, df in filteredtSNPs.loc[(filteredtSNPs.kind=='transmission')&(filteredtSNPs.subtype=='H3N2')&(filteredtSNPs.abs_SNP_frequency_difference > 0.01)].groupby('antigenic_product'):\n",
    "#     print(gene)\n",
    "    \n",
    "#     df['AAtype'] = df.AAtype.map({'Nonsynonymous':1, 'Synonymous':0})\n",
    "#     m, b, r_value, p_value, std_err = stats.linregress(df[['log_abs_SNP_frequency_difference','AAtype']].to_numpy())\n",
    "# #     print (f'gene:{gene} m: {m}, b: {b}, r^2: {r_value**2}, pvalue: {p_value}, ci: {std_err*1.96}')\n",
    "#     filtered_transmissionResults[gene] = {'slope':m,'ci':std_err*1.96,'pvalue':p_value}\n",
    "#     sns.lmplot('log_abs_SNP_frequency_difference','AAtype',data=df)\n",
    "#     plt.title(gene)\n",
    "\n",
    "# pd.DataFrame(filtered_transmissionResults).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filteredintraHostResults = pd.DataFrame(intraHostResults).T\n",
    "# unfilteredIntraHostResults = pd.DataFrame(unfilteredIntraHostResults).T\n",
    "# transmissionResults = pd.DataFrame(filtered_transmissionResults).T\n",
    "# unfilteredTransmissionResults = pd.DataFrame(unFilteredTransmissionResults).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transmissionResults\n",
    "# allXResults = transmissionResults.merge(unfilteredTransmissionResults, left_index=True,right_index=True,suffixes=('_filtered','_unfiltered'))\n",
    "# allXResults[['slope_unfiltered','slope_filtered','pvalue_unfiltered','pvalue_filtered']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "# drop=['M2','NEP','PA-X','PB1-F2']\n",
    "# tmp = allXResults[['slope_filtered','ci_filtered']].dropna().drop(drop).reset_index()\n",
    "# sns.barplot('index','slope_filtered',data=tmp)\n",
    "\n",
    "# plt.errorbar(x=range(len(tmp)),y=tmp['slope_filtered'], yerr=tmp.ci_filtered, fmt='none')\n",
    "\n",
    "\n",
    "# filtered_transmissionResults={'HA_antigenic':{}, 'everything_else':{}}\n",
    "# tmp = filteredtSNPs.loc[(filteredtSNPs.kind=='transmission')&(filteredtSNPs.abs_SNP_frequency_difference > 0.01)]#&(filteredtSNPs.subtype!='Influenza B')]\n",
    "# anti = [('HA_antigenic',tmp.loc[tmp.antigenic_product == 'HA_antigenic']), ('everything_else',tmp.loc[tmp.antigenic_product != 'HA_antigenic'])]\n",
    "\n",
    "# for group, df in anti:\n",
    "#     plt.figure()\n",
    "#     df.loc[~df.antigenic_product.isin(drop)]\n",
    "#     df['AAtype'] = df.AAtype.map({'Nonsynonymous':1, 'Synonymous':0})\n",
    "#     ds = df[['log_abs_SNP_frequency_difference','AAtype']].dropna()\n",
    "#     X = ds.log_abs_SNP_frequency_difference\n",
    "#     X = sm.add_constant(X)\n",
    "#     Y = ds.AAtype\n",
    "#     model= sm.OLS(Y,X).fit()\n",
    "#     print(model.summary())\n",
    "    \n",
    "# tmp['broad'] = 0\n",
    "# tmp.loc[tmp.antigenic_product == 'HA_antigenic', 'broad'] = 1\n",
    "\n",
    "# tmp['AAtype'] = tmp.AAtype.map({'Nonsynonymous':1, 'Synonymous':0})\n",
    "# tmptmp = tmp[['broad','log_abs_SNP_frequency_difference', 'AAtype']].dropna()\n",
    "\n",
    "# X = tmptmp[['broad','log_abs_SNP_frequency_difference']]\n",
    "# X = sm.add_constant(X)\n",
    "# y = tmptmp.AAtype\n",
    "# model = sm.OLS(y,X).fit()\n",
    "# model.summary()\n",
    "\n",
    "# #     m, b, r_value, p_value, std_err = stats.linregress(df[['log_abs_SNP_frequency_difference','AAtype']].dropna().to_numpy())\n",
    "# #     print (f'gene:{gene} m: {m}, b: {b}, r^2: {r_value**2}, pvalue: {p_value}, ci: {std_err*1.96}')\n",
    "# #     filtered_transmissionResults[group] = {'slope':m,'ci':std_err*1.96,'pvalue':p_value}\n",
    "# #     plt.scatter(ds[:,0],ds[:,1])\n",
    "# #     plt.title(group)\n",
    "# # x = pd.DataFrame(filtered_transmissionResults).T\n",
    "# # x['range-']= x['slope']-x.ci\n",
    "# # x['range+'] = x.slope+x.ci\n",
    "# model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.formula.api as smf\n",
    "# tmp = tmp.dropna(subset=['AAtype','log_abs_SNP_frequency_difference','antigenic_product'])\n",
    "# mod = smf.ols(formula='AAtype ~ log_abs_SNP_frequency_difference', data=tmp).fit()\n",
    "# mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp = [[0,0,1],[0,1,0]]\n",
    "# print(model.f_test('broad=log_abs_SNP_frequency_difference'))\n",
    "# model.summary()\n",
    "# # infl = model.get_influence()\n",
    "# # cutoff = 2./len(X)**.5\n",
    "# # print (cutoff)\n",
    "# # inflparams = infl.summary_frame().filter(regex=\"dfb\")\n",
    "# # tmp.loc[inflparams.dfb_log_abs_SNP_frequency_difference>cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filteredLinearResults = filteredintraHostResults.merge(transmissionResults,left_index=True,right_index=True,suffixes=('_intrahost','_interhost'))#[['slope_intrahost','slope_interhost','pvalue_intrahost','pvalue_interhost']].dropna()\n",
    "# # unfilteredIntraHostResults.merge(unfilteredTransmissionResults,left_index=True,right_index=True,suffixes=('_intrahost','_interhost'))[['slope_intrahost','slope_interhost','pvalue_intrahost','pvalue_interhost']].dropna()\n",
    "\n",
    "# # unfilteredIntraHostResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfOfInterest = filteredtSNPs\n",
    "# categoryOfInterest = 'abs_SNP_frequency_difference'#'transformed_frequency'\n",
    "# tmp = dfOfInterest.loc[(dfOfInterest[categoryOfInterest] >0.01)&(dfOfInterest.kind=='transmission')]\n",
    "# categories=3\n",
    "# # tmp['qcut'] = pd.qcut(tmp.log_transformed_frequency,q=categories,labels=[float(x) for x in range(categories)])\n",
    "# tmp['qcut'] = pd.cut(tmp['log_'+categoryOfInterest],bins=categories,labels=[float(x) for x in range(categories)])\n",
    "# # tmp[['abs_SNP_frequency_difference','qcut']]\n",
    "# tmp=tmp.loc[tmp.AAtype!='Out of reading frame']\n",
    "\n",
    "# #     m, b, r_value, p_value, std_err = stats.linregress(df[['log_abs_SNP_frequency_difference','AAtype']].to_numpy())\n",
    "# # #     print (f'gene:{gene} m: {m}, b: {b}, r^2: {r_value**2}, pvalue: {p_value}, ci: {std_err*1.96}')\n",
    "# #     filtered_transmissionResults[gene] = {'slope':m,'ci':std_err*1.96,'pvalue':p_value}\n",
    "# #     sns.lmplot('log_abs_SNP_frequency_difference','AAtype',data=df)\n",
    "# #     plt.title(gene)\n",
    "\n",
    "# # tmp['log_abs_SNP_frequency_difference'] = tmp.log_abs_SNP_frequency_difference.replace(np.inf, 0).replace(-np.inf,-100)\n",
    "# # sns.distplot(1/(tmp.loc[tmp.log_abs_SNP_frequency_difference<0,'log_abs_SNP_frequency_difference']))tmp['rank#\n",
    "# # x = pd.qtmp.log_abs_SNP_frequency_difference\n",
    "# # sns.distplot()#, data=tmp)\n",
    "\n",
    "# # categories = 5\n",
    "\n",
    "# #Source: Matsuzaki 2014\n",
    "# H1N1_antigenic_sites = [87,88,90,91,92, 132,\n",
    "#           141,142,143,171,172,174,177,180,\n",
    "#           170,173,202,206,210,211,212,\n",
    "#           151,154,156,157,158,159,200,238,\n",
    "#           147]\n",
    "# H1N1_antigenic_sites = [site-1 for site in H1N1_antigenic_sites] #convert to zero-index\n",
    "# print(len(antigenic_sites))\n",
    "# print(len(antigenic_sites_glyc))\n",
    "# print(len(antigenic_sites_ridge_glyc))\n",
    "\n",
    "# tmp['antigenic_segment'] = tmp.segment\n",
    "# tmp.loc[(tmp.subtype=='H3N2')&(tmp.segment=='HA'), 'antigenic_segment'] = 'HA_nonantigenic'\n",
    "# tmp.loc[(tmp.subtype=='H3N2')&(tmp.segment=='HA')&tmp.codon.isin(antigenic_sites_ridge_glyc), 'antigenic_segment'] = 'HA_antigenic'\n",
    "# tmp.loc[(tmp.subtype=='H1N1pdm')&(tmp.segment=='HA'), 'antigenic_segment'] = 'HA_nonantigenic'\n",
    "# tmp.loc[(tmp.subtype=='H1N1pdm')&(tmp.segment=='HA')&tmp.codon.isin(H1N1_antigenic_sites), 'antigenic_segment'] = 'HA_antigenic'\n",
    "# tmp['numAAtype'] = tmp.AAtype.map({'Nonsynonymous':1, 'Synonymous':0})\n",
    "\n",
    "# tmp['exp'] = -1/(50*tmp[categoryOfInterest])\n",
    "# tmp['box'],lam = stats.boxcox(tmp[categoryOfInterest])\n",
    "\n",
    "# # print(len(tmp.loc[(tmp.subtype=='H1N1pdm')&(tmp.)]))\n",
    "# # tmp['qcut'] = pd.qcut(tmp.log_abs_SNP_frequency_difference,q=categories,labels=[float(x) for x in range(categories)])\n",
    "# # tmp['cut'] = pd.cut(tmp.log_abs_SNP_)\n",
    "# # tmp['cat_AAtype'] = tmp.AAtype.map({'Nonsynonymous':1, 'Synonymous':0})\n",
    "# for gene, df in tmp.loc[tmp.subtype!='Influenza B'].groupby('antigenic_segment'):\n",
    "# #     print(df[['qcut','cat_AAtype']])\n",
    "#     df['AAtype'] = df.AAtype.map({'Nonsynonymous':1, 'Synonymous':0})\n",
    "# #     m, b, r_value, p_value, std_err = stats.linregress(df[['log_abs_SNP_frequency_difference','AAtype']])\n",
    "#     m, b, r_value, p_value, std_err = stats.linregress(df[['box','AAtype']])\n",
    "#     print (f'gene:{gene} m: {m}, b: {b}, r^2: {r_value**2}, pvalue: {p_value}, ci: {std_err*1.96}')\n",
    "#     plt.figure()\n",
    "#     sns.lmplot('box','AAtype',data=df)\n",
    "#     plt.title(gene)\n",
    "# #     sns.residplot('abs_SNP_frequency_difference','AAtype',lowess=True,data=df)\n",
    "# #     break\n",
    "# #     filtered_transmissionResults[gene] = {'slope':m,'ci':std_err*1.96,'pvalue':p_value}\n",
    "# #     sns.lmplot('log_abs_SNP_frequency_difference','AAtype',data=df)\n",
    "# #     plt.title(gene)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ax = plt.figure().add_subplot(111)\n",
    "# prob = stats.boxcox_normplot(test.abs_SNP_frequency_difference,-20,20,N=200,plot=ax)\n",
    "# x, maxlog = stats.boxcox(test.abs_SNP_frequency_difference)\n",
    "# # ax.axvline(maxlog, color='r')\n",
    "# # maxlog\n",
    "\n",
    "# sns.distplot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test = tmp.loc[tmp.antigenic_segment=='PB2']\n",
    "# test['AAtype'] = test.AAtype.map({'Synonymous':0,'Nonsynonymous':1})\n",
    "# test['exp'] = 1/(test.abs_SNP_frequency_difference)\n",
    "# test['log_log_abs_SNP_frequency_difference'] = np.log(test.abs_SNP_frequency_difference).replace(-np.nan, 0)\n",
    "# # print(test.log_log_abs_SNP_frequency_difference.describe())\n",
    "# sns.residplot('abs_SNP_frequency_difference','AAtype',lowess=True,data=test)\n",
    "# plt.title('abs')\n",
    "# plt.figure()\n",
    "# sns.lmplot('abs_SNP_frequency_difference','AAtype',data=test)\n",
    "# print('abs')\n",
    "# m, b, r_value, p_value, std_err = stats.linregress(test[['abs_SNP_frequency_difference','AAtype']])\n",
    "# print (f'gene:PB2 m: {m}, b: {b}, r^2: {r_value**2}, pvalue: {p_value}, ci: {std_err*1.96}')\n",
    "\n",
    "# plt.figure()\n",
    "# sns.residplot('log_abs_SNP_frequency_difference','AAtype',lowess=True,data=test)\n",
    "# plt.title('log_abs')\n",
    "# plt.figure()\n",
    "# sns.lmplot('log_abs_SNP_frequency_difference','AAtype',data=test)\n",
    "# print('log_abs')\n",
    "# m, b, r_value, p_value, std_err = stats.linregress(test[['log_abs_SNP_frequency_difference','AAtype']])\n",
    "# print (f'gene:PB2 m: {m}, b: {b}, r^2: {r_value**2}, pvalue: {p_value}, ci: {std_err*1.96}')\n",
    "\n",
    "# plt.figure()\n",
    "# sns.residplot('log_log_abs_SNP_frequency_difference','AAtype',lowess=True,data=test)\n",
    "# plt.title('log_log_abs')\n",
    "# plt.figure()\n",
    "# sns.lmplot('log_log_abs_SNP_frequency_difference','AAtype',data=test)\n",
    "# print('log_log_abs')\n",
    "# m, b, r_value, p_value, std_err = stats.linregress(test[['log_log_abs_SNP_frequency_difference','AAtype']])\n",
    "# print (f'gene:PB2 m: {m}, b: {b}, r^2: {r_value**2}, pvalue: {p_value}, ci: {std_err*1.96}')\n",
    "\n",
    "# plt.figure()\n",
    "# sns.residplot('exp','AAtype',lowess=True,data=test)\n",
    "# plt.title('exp')\n",
    "# plt.figure()\n",
    "# sns.lmplot('exp','AAtype',data=test)\n",
    "# print('exp')\n",
    "# m, b, r_value, p_value, std_err = stats.linregress(test[['exp','AAtype']])\n",
    "# print (f'gene:PB2 m: {m}, b: {b}, r^2: {r_value**2}, pvalue: {p_value}, ci: {std_err*1.96}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(-tmp.exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleted = tmp\n",
    "# sns.lmplot('log_abs_SNP_frequency_difference','numAAtype',data=deleted.loc[(deleted.subtype=='H3N2')&(deleted.antigenic_segment=='HA_nonantigenic')])\n",
    "# deleted['H3N2_codon'] = deleted.codon-14\n",
    "# # print(antigenic_sites)\n",
    "# print([site-14 for site in antigenic_sites])\n",
    "# deleted.loc[(deleted.antigenic_segment=='HA_nonantigenic')&(deleted.log_abs_SNP_frequency_difference>-1.2),['index','contact','segment','pos','AAstr','codon','H3N2_codon','abs_SNP_frequency_difference','log_abs_SNP_frequency_difference','AAtype']]\n",
    "# # tmp.loc[(tmp.subtype=='H3N2')&(tmp.antigenic_segment=='HA_nonantigenic')&(tmp.log_abs_SNP_frequency_difference>-1),['index','contact','segment','pos','AAstr','codon','abs_SNP_frequency_difference','log_abs_SNP_frequency_difference','AAtype']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #antigenic sites alone:\n",
    "# gene:HA m: -0.39676518465977434, b: 0.6407688380218459, r^2: 0.01898256898473917, pvalue: 0.05476259266094406, ci: 0.4024128517326837\n",
    "# gene:HA_antigenic m: 0.20151640257687498, b: 0.7608758644754745, r^2: 0.015471304647140404, pvalue: 0.36557823279193413, ci: 0.43279203406413286\n",
    "# gene:HA_nonantigenic m: 0.15031152947673096, b: 0.6875388144713909, r^2: 0.0041608717131222835, pvalue: 0.4456635633359718, ci: 0.3852004428239943\n",
    "# #add glyc sites:\n",
    "# gene:HA m: -0.39676518465977434, b: 0.6407688380218459, r^2: 0.01898256898473917, pvalue: 0.05476259266094406, ci: 0.4024128517326837\n",
    "# gene:HA_antigenic m: 0.20864653475125594, b: 0.7653081324929474, r^2: 0.023497228448378416, pvalue: 0.2506389126082541, ci: 0.3522910861635799\n",
    "# gene:HA_nonantigenic m: 0.07139396755402376, b: 0.6870006370616757, r^2: 0.0006427480600744835, pvalue: 0.7670393455282324, ci: 0.47140838401876767\n",
    "# #add ridge sites:\n",
    "# gene:HA m: -0.39676518465977434, b: 0.6407688380218459, r^2: 0.01898256898473917, pvalue: 0.05476259266094406, ci: 0.4024128517326837\n",
    "# gene:HA_antigenic m: 0.19314447758636405, b: 0.7786999455457064, r^2: 0.020042484187416006, pvalue: 0.27645094739234033, ci: 0.34461978727402776\n",
    "# gene:HA_nonantigenic m: 0.0813942433702434, b: 0.6795956969557642, r^2: 0.000842702140099572, pvalue: 0.7372606087384561, ci: 0.47454483029544187\n",
    "\n",
    "\n",
    "# gene:HA m: 0.18383588955740468, b: 0.706976152450306, r^2: 0.007806827612243418, pvalue: 0.2169558120933586, ci: 0.29089073496096907\n",
    "# gene:MP m: -0.5965578083984763, b: 0.6047818344209833, r^2: 0.04311013955024761, pvalue: 0.1024989216320991, ci: 0.7053179725178346\n",
    "# gene:NA m: -0.5859867266158809, b: 0.677997386701978, r^2: 0.0537661898793504, pvalue: 0.0010431770778698175, ci: 0.34504135024911464\n",
    "# gene:NP m: -0.8948263664149644, b: 0.7518448659781097, r^2: 0.04655673452623357, pvalue: 0.003264865835931893, ci: 0.5883223153120624\n",
    "# gene:NS m: 0.1016784607519183, b: 0.6943386476209003, r^2: 0.00036913723758104307, pvalue: 0.8223741084893332, ci: 0.8860345355261467\n",
    "# gene:PA m: -0.6192436543469743, b: 0.7062408242905469, r^2: 0.028909674427365698, pvalue: 0.0036825137489776235, ci: 0.41450483973667357\n",
    "# gene:PB1 m: -0.2702087592219644, b: 0.6929854355581092, r^2: 0.013330895174339585, pvalue: 0.052352210868886015, ci: 0.27180562655905466\n",
    "# gene:PB2 m: -0.426146594598844, b: 0.6337060779353555, r^2: 0.013851130110175433, pvalue: 0.07879962935220033, ci: 0.47300664581253976\n",
    "\n",
    "# tmp.loc[tmp['segment'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dfOfInterest = filteredtSNPs\n",
    "# # categoryOfInterest = 'abs_SNP_frequency_difference'#transformed_frequency\n",
    "# # tmp = dfOfInterest.loc[(dfOfInterest[categoryOfInterest] >0.01)&(dfOfInterest.subtype!='Influenza B')&(dfOfInterest.kind=='transmission')]\n",
    "# # categories=5\n",
    "# # tmp['qcut'] = pd.qcut(tmp.log_transformed_frequency,q=categories,labels=[float(x) for x in range(categories)])\n",
    "# # tmp['qcut'] = pd.cut(tmp['log_'+categoryOfInterest],bins=categories,labels=[float(x) for x in range(categories)])\n",
    "# tmp['qcut'] = pd.cut(tmp[categoryOfInterest],bins=[0,.05,.1,1],labels=[1,2,3])\n",
    "# # tmp[['abs_SNP_frequency_difference','qcut']]\n",
    "# tmp=tmp.loc[tmp.AAtype!='Out of reading frame']\n",
    "# # tmp.groupby(['qcut','AAtype','antigenic_product']).count()['index'].fillna(0).unstack().T\n",
    "# # # sns.distplot(np.log(np.log(tmp.abs_SNP_frequency_difference)))\n",
    "# # sns.distplot(np.log(-np.log(tmp.abs_SNP_frequency_difference).replace(np.inf, 0)).replace(-np.inf, 0))\n",
    "# for gene, df in tmp.loc[tmp.subtype!='Influenza B'].groupby('antigenic_segment'):\n",
    "#     table = sm.stats.Table(pd.crosstab(df.AAtype,tmp.qcut))\n",
    "#     print (f'{gene}: {table.test_ordinal_association().pvalue}')\n",
    "#     print (table)\n",
    "# #     print(table.test_ordinal_association())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = filteredtSNPs.set_index('SNPkey')\n",
    "# y = unfilteredtSNPs.set_index('SNPkey')\n",
    "# droppedkeys = x.index.symmetric_difference(y.index)\n",
    "\n",
    "# dropped = unfilteredtSNPs.loc[unfilteredtSNPs.SNPkey.isin(droppedkeys)]\n",
    "# droppedSNPs = dropped.loc[(dropped.subtype=='H3N2')&(dropped.kind=='transmission')&(dropped.abs_SNP_frequency_difference>0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppedSNPs['log_abs_SNP_frequency_difference'] = np.log10(droppedSNPs.abs_SNP_frequency_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(droppedSNPs['log_abs_SNP_frequency_difference'])\n",
    "# sns.distplot(unfilteredtSNPs.loc[unfilteredtSNPs.abs_SNP_frequency_difference>0.01,'log_abs_SNP_frequency_difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppedSNPs.loc[droppedSNPs.antigenic_product == 'PB2',['SNPkey','index','contact','pos','AAtype','SNP_frequency_index','SNP_frequency_contact']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# from statsmodels.formula.api import logit,probit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNPsofInterest = filteredSNPs\n",
    "# logitSNPs = SNPsofInterest.loc[(SNPsofInterest.subtype=='H3N2')&pd.notna(SNPsofInterest.antigenic_product)&(SNPsofInterest.transformed_frequency>0.01)]\n",
    "# logitSNPs['numAAtype'] = logitSNPs.AAtype.map({'Nonsynonymous':1.0, 'Synonymous':0.0})\n",
    "\n",
    "# tSNPsofInterest = filteredtSNPs\n",
    "# logitTransSNPs = tSNPsofInterest.loc[(tSNPsofInterest.subtype=='H3N2')&pd.notna(tSNPsofInterest.antigenic_product)&(tSNPsofInterest.abs_SNP_frequency_difference>0.01)&(tSNPsofInterest.kind=='transmission')]\n",
    "# logitTransSNPs['numAAtype'] = logitTransSNPs.AAtype.map({'Nonsynonymous':1.0, 'Synonymous':0.0})\n",
    "# results_df = pd.DataFrame()\n",
    "# for gene, df in logitTransSNPs.groupby('antigenic_product'):\n",
    "#     print('\\n\\n')\n",
    "#     print(gene)\n",
    "    \n",
    "#     logit_mod = logit('numAAtype ~ log_abs_SNP_frequency_difference', df).fit()#sm.Logit(df.numAAtype, df.log_transformed_frequency)\n",
    "# #     logit_res = logit_mod\n",
    "#     margfx = logit_mod.get_margeff()\n",
    "#     print (margfx.summary())\n",
    "#     probit_mod = probit('numAAtype ~ log_abs_SNP_frequency_difference', df).fit()#sm.Logit(df.numAAtype, df.log_transformed_frequency)\n",
    "# #     logit_res = logit_mod\n",
    "#     margfx = probit_mod.get_margeff()\n",
    "#     results_df = results_df.append(margfx.summary_frame().rename({'log_abs_SNP_frequency_difference':gene}))\n",
    "    \n",
    "    \n",
    "# # for gene, df in logitTransSNPs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([filteredLinearResults.dropna()['slope_interhost'], results_df['dy/dx']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnsOfInterest=['index','contact','AAstr','AAtype','abs_SNP_frequency_difference']\n",
    "# filteredtSNPs.loc[(filteredtSNPs['product'] == 'PB2')&(filteredtSNPs.subtype=='H3N2')&(filteredtSNPs.kind=='transmission')&(filteredtSNPs.abs_SNP_frequency_difference > 0.1),columnsOfInterest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (len(unfilteredtSNPs), len(filteredtSNPs),len(filteredtSNPs2))\n",
    "# SNPsofInterest = filteredSNPs2\n",
    "# logitSNPs = SNPsofInterest.loc[(SNPsofInterest.transformed_frequency>0.01)]\n",
    "# logitSNPs['numAAtype'] = logitSNPs.AAtype.map({'Nonsynonymous':1.0, 'Synonymous':0.0})\n",
    "\n",
    "# tSNPsofInterest = filteredtSNPs2\n",
    "# logitTransSNPs = tSNPsofInterest.loc[(tSNPsofInterest.abs_SNP_frequency_difference>0.01)&(tSNPsofInterest.kind=='transmission')]\n",
    "# logitTransSNPs['numAAtype'] = logitTransSNPs.AAtype.map({'Nonsynonymous':1.0, 'Synonymous':0.0})\n",
    "# results_df = pd.DataFrame()\n",
    "# for gene, df in logitTransSNPs.groupby('antigenic_product'):\n",
    "# #     print('\\n\\n')\n",
    "# #     print(gene)\n",
    "    \n",
    "#     logit_mod = logit('numAAtype ~ log_abs_SNP_frequency_difference', df).fit()#sm.Logit(df.numAAtype, df.log_transformed_frequency)\n",
    "# #     logit_res = logit_mod\n",
    "#     margfx = logit_mod.get_margeff()\n",
    "# #     print (margfx.summary())\n",
    "#     probit_mod = probit('numAAtype ~ log_abs_SNP_frequency_difference', df).fit()#sm.Logit(df.numAAtype, df.log_transformed_frequency)\n",
    "# #     logit_res = logit_mod\n",
    "#     margfx = probit_mod.get_margeff()\n",
    "#     results_df = results_df.append(margfx.summary_frame().rename({'log_abs_SNP_frequency_difference':gene}))\n",
    "    \n",
    "# results_df\n",
    "# # for gene, df in logitTransSNPs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df\n",
    "# # logitTransSNPs.loc[logitTransSNPs.antigenic_product=='HA_nonantigenic', ('SNP_frequency_index','SNP_frequency_contact','codon','pos','ref_nuc','alt_nuc','segment','product','antigenic_product','antigenic_site')]\n",
    "# # transmissionSNPs.loc[transmissionSNPs.abs_SNP_frequency_difference>0.01]#.loc[logitTransSNPs.subtype!='H3N2',['SNP_frequency_index','SNP_frequency_contact','codon','pos','ref_nuc','alt_nuc','segment','product','antigenic_product','antigenic_site']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Compare to global freqs\n",
    "# def getVariableMutationSites(jsonFile,cutoff=0.02, cutoffyear = 2009):\n",
    "#     with open(jsonFile, 'r') as freqFile:\n",
    "#         frequencies = json.load(freqFile)\n",
    "#     dates = frequencies.pop('pivots')\n",
    "#     offKeys = [key for key in frequencies.keys() if len(frequencies[key]) != len(dates)]\n",
    "#     if len(offKeys) > 0:\n",
    "#         print (f'offKey length is {len(offKeys)}')\n",
    "#         offVals = [frequencies.pop(key) for key in offKeys]\n",
    "#     freqDF = pd.DataFrame(frequencies, index=dates)\n",
    "    \n",
    "#     freqDF = freqDF.loc[freqDF.index > cutoffyear]\n",
    "#     freqDF = freqDF.unstack().reset_index().rename(columns={'level_0':'mutkey','level_1':'month',0:'freq'})\n",
    "#     return freqDF.loc[(freqDF.freq>cutoff)&(freqDF.freq<(1-cutoff)),'mutkey'].str[:-1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNPs.loc[SNPs.codon<0, ['chrom','pos','codon','product']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawSNPs.loc[rawSNPs.sampleID=='17VR014370', ['pos','chrom','product','SNP_frequency', 'AAtype']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
