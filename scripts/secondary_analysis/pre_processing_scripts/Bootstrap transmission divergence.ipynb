{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-7cd3a621f227>:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import pysam\n",
    "from tqdm.notebook import trange\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading subjects...\n",
      "loading samples...\n",
      "loading segments...\n",
      "loading genes...\n",
      "loading SNPs...\n",
      "loading transmission pairs...\n",
      "loading transmission segments...\n",
      "loading transmission SNPs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3337: DtypeWarning: Columns (58) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# Import data, custom figure-making functions\n",
    "sys.path.append('/mnt/d/orchards')\n",
    "from figure_constants import *\n",
    "from figure_functions import *\n",
    "sys.path.append(installDir+'scripts')\n",
    "from chartannotator import add_stat_annotation as si\n",
    "multiple_annotation_method=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_transmissionPairs = transmissionPairs.loc[transmissionPairs.kind=='transmission']\n",
    "NUMBER_OF_H3N2_PAIRS = len(real_transmissionPairs.loc[real_transmissionPairs.subtype=='H3N2'])\n",
    "NUMBER_OF_H1N1_PAIRS = len(real_transmissionPairs.loc[real_transmissionPairs.subtype=='H1N1'])\n",
    "NUMBER_OF_FLUB_PAIRS = len(real_transmissionPairs.loc[real_transmissionPairs.subtype=='Influenza B'])\n",
    "\n",
    "\n",
    "subtype = 'H3N2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def memoize(obj):\n",
    "    cache = obj.cache = {}\n",
    "    \n",
    "    @functools.wraps(obj)\n",
    "    def memoizer(*args, **kwargs):\n",
    "        key = str(args) + str(kwargs)\n",
    "        if key not in cache:\n",
    "            cache[key] = obj(*args, **kwargs)\n",
    "        return cache[key]\n",
    "    return memoizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize\n",
    "def getReadDepth(sample, segment, pos, alt):\n",
    "    reffile = SNPs.loc[SNPs['sampleID']==sample, 'referenceFile'].iloc[0]\n",
    "    ref = reffile.split('/')[5]\n",
    "    refbase = reffile.split('/')[-1].split('_')\n",
    "    if 'Hong_Kong' in reffile:\n",
    "        chrom = hongkongContigs[segment]\n",
    "    elif 'Michigan' in reffile:\n",
    "        chrom = '_'.join(refbase[:-4])+'_'+segment\n",
    "    elif refbase[-3] in ['17','18','19']:\n",
    "        chrom = '_'.join(refbase[:-3])+'_'+segment\n",
    "    else:\n",
    "        chrom = '_'.join(refbase[:-2])+'_'+segment\n",
    "\n",
    "    bamfile = '/'.join(reffile.split('/')[0:6])+'/'+'_'.join(reffile.split('/')[-1].split('_')[:-2])+'/map_to_consensus/'+sample+'.bam'\n",
    "    pos = int(pos)\n",
    "    sam = pysam.AlignmentFile(bamfile, \"rb\")\n",
    "    try:\n",
    "        pileup = sam.pileup(contig=chrom, start=pos-1, end=pos, truncate=True, stepper=\"nofilter\")\n",
    "        column = next(pileup)\n",
    "    except StopIteration:\n",
    "        print (chrom, pos)\n",
    "        print (pileup)\n",
    "        print (bamfile)\n",
    "        return (0,0,0)        \n",
    "    except:\n",
    "        print (sam.references)\n",
    "        print (chrom)\n",
    "        print (reffile)\n",
    "        print (ref)\n",
    "        raise\n",
    "    \n",
    "    column.set_min_base_quality(30)\n",
    "    try:\n",
    "        bases = column.get_query_sequences(mark_matches=True)\n",
    "        altreads = bases.count(alt.lower()) + bases.count(alt.upper())\n",
    "    except:\n",
    "        altreads = 0\n",
    "    \n",
    "    depth = column.get_num_aligned()\n",
    "    if depth > 0:\n",
    "        frequency = round(altreads/column.get_num_aligned(),4)\n",
    "    else:\n",
    "        frequency = 0\n",
    "    return frequency, altreads, depth\n",
    "\n",
    "def checkForDuplicateColumnsPostMerge(df, suffixes=('_x','_y'), verbose=False):\n",
    "    '''if an index/contact or x/y column pairing are identical, unify them into one column.\n",
    "       Keeps np.nan values seperate.'''\n",
    "    columns = [column[:-len(suffixes[0])] for column in df.columns if column[-len(suffixes[0]):]==suffixes[0]]\n",
    "    \n",
    "    merged=[]\n",
    "    kept = []\n",
    "    for column in columns:\n",
    "        columna = column+suffixes[0]\n",
    "        columnb = column+suffixes[1]\n",
    "                \n",
    "        a=df[columna].values\n",
    "        b=df[columnb].values\n",
    "        \n",
    "        if (df[columna].dtype.kind in 'biufc') and (df[columnb].dtype.kind in 'biufc'):\n",
    "            theyAreEqual = ((a==b)|np.isclose(a,b,atol=1E-4)|np.isclose(b,a,atol=1E-4))\n",
    "        else:\n",
    "            theyAreEqual = ((a==b))\n",
    "        if theyAreEqual.all():\n",
    "            df = df.rename(columns={columna:column}).drop(columns=[columnb])\n",
    "            merged.append(column)\n",
    "        \n",
    "        else:\n",
    "            kept.append(column)\n",
    "    if verbose:\n",
    "        print('merged:')\n",
    "        print (merged)\n",
    "        print('kept:')\n",
    "        print(kept)\n",
    "    return df\n",
    "\n",
    "def updateDuplicateColumnsPostMerge(df, exclude=[], suffixes=('_x','_y'), verbose=False):\n",
    "    '''if an index/contact or x/y column pairing are identical except for na values, unify them into one column.\n",
    "       Assumes np.nan values are artifacts, and fills in values if one column has them'''\n",
    "\n",
    "    columns = [column[:-len(suffixes[0])] for column in df.columns if column[-len(suffixes[0]):]==suffixes[0]]\n",
    "\n",
    "    merged=[]\n",
    "    kept = []\n",
    "    for column in columns:\n",
    "        columna = column+suffixes[0]\n",
    "        columnb = column+suffixes[1]\n",
    "                \n",
    "        a=df[columna].values\n",
    "        b=df[columnb].values\n",
    "        \n",
    "        if (df[columna].dtype.kind in 'biufc') and (df[columnb].dtype.kind in 'biufc'):\n",
    "            theyAreEqual = ((a==b)|pd.isna(a)|pd.isna(b)|np.isclose(a,b,atol=1E-4)|np.isclose(b,a,atol=1E-4))\n",
    "        else:\n",
    "            theyAreEqual = ((a==b)|pd.isna(a)|pd.isna(b))\n",
    "        \n",
    "        if 'AAstr' in column:\n",
    "            if verbose:\n",
    "                print (((a==b)|pd.isna(a)|pd.isna(b)).all())\n",
    "                print (df[((a!=b)&pd.notna(a)&pd.notna(b))])\n",
    "        \n",
    "        if theyAreEqual.all():\n",
    "            df[columna].update(df[columnb])\n",
    "            df = df.rename(columns={columna:column}).drop(columns=[columnb])\n",
    "            merged.append(column)\n",
    "        else:\n",
    "            kept.append(column)\n",
    "    \n",
    "    if verbose:\n",
    "        print('updated:')\n",
    "        print (merged)\n",
    "        print('untouched:')\n",
    "        print(kept)\n",
    "    return df\n",
    "\n",
    "@memoize\n",
    "def getReadDepthWrapper(row):\n",
    "    if pd.isna(row.SNP_frequency_index):\n",
    "        try:\n",
    "            result = getReadDepth(row['index'], row.segment,row.pos,row.alt_nuc)+(row.SNP_frequency_contact,row.AD_contact,row.depth_contact)\n",
    "        except:\n",
    "            print (row[['index','contact','segment','pos','SNP_frequency_index','SNP_frequency_contact']])\n",
    "            raise\n",
    "    elif pd.isna(row.SNP_frequency_contact):\n",
    "        try:\n",
    "            result = (row.SNP_frequency_index,row.AD_index,row.depth_index)+getReadDepth(row.contact, row.segment,row.pos,row.alt_nuc)\n",
    "        except:\n",
    "            print (row)  \n",
    "            raise\n",
    "    else:\n",
    "        result = (row.SNP_frequency_index,row.AD_index,row.depth_index,row.SNP_frequency_contact,row.AD_contact,row.depth_contact)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_rand_pairing(n):\n",
    "    int1 = np.random.randint(0, n)\n",
    "    int2 = np.random.randint(0, n)\n",
    "    while int1 == int2:\n",
    "        int2 = np.random.randint(0, n)\n",
    "    return (int1, int2)\n",
    "\n",
    "def draw_rand_pairings(n, p_candidates):\n",
    "    rand_pairings = list()\n",
    "    while len(rand_pairings) < n+1:\n",
    "        index, contact = draw_rand_pairing(len(p_candidates))\n",
    "        if abs(p_candidates.iloc[index].time_of_symptom_onset - p_candidates.iloc[contact].time_of_symptom_onset) <= pd.Timedelta(7, 'd'):\n",
    "            if p_candidates.iloc[index].subclade == p_candidates.iloc[contact].subclade:\n",
    "                if pd.notna(p_candidates.iloc[index].day0_sample):\n",
    "                    index = p_candidates.iloc[index].day0_sample\n",
    "                else:\n",
    "                    index = p_candidates.iloc[index].day7_sample\n",
    "                if pd.notna(p_candidates.iloc[contact].day0_sample):\n",
    "                    contact = p_candidates.iloc[contact].day0_sample\n",
    "                else:\n",
    "                    contact = p_candidates.iloc[contact].day7_sample\n",
    "                rand_pairings.append((index, contact))\n",
    "    return rand_pairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_antigenic_product(transmissionSNPs):\n",
    "    HA_add_on = transmissionSNPs.loc[transmissionSNPs['product'].isin(['HA_antigenic','HA_nonantigenic'])]\n",
    "    HA_add_on.loc[:, 'product'] = 'HA'\n",
    "    transmissionSNPs = transmissionSNPs.append(HA_add_on)\n",
    "    return transmissionSNPs\n",
    "\n",
    "def make_all_changes_minor_to_major(transmissionSNPs):\n",
    "    #Adjust SNP frequencies so that I'm always looking at the change that happens to the *minor* allele\n",
    "    transmissionSNPs['minorAlleleFreq_index']= transmissionSNPs.SNP_frequency_index\n",
    "    transmissionSNPs['minorAlleleFreq_contact']= transmissionSNPs.SNP_frequency_contact\n",
    "    transmissionSNPs['minor_alt_nuc']= transmissionSNPs.alt_nuc\n",
    "    transmissionSNPs['minor_ref_nuc']= transmissionSNPs.ref_nuc\n",
    "    print (transmissionSNPs.SNP_frequency_index.max())\n",
    "    tmpSNPs = transmissionSNPs.copy()\n",
    "\n",
    "    majorityMinoritySNPs=tmpSNPs.SNP_frequency_index > 0.5\n",
    "    alt_nucs = tmpSNPs.loc[majorityMinoritySNPs,'alt_nuc']\n",
    "    tmpSNPs.loc[majorityMinoritySNPs,'minor_alt_nuc'] = tmpSNPs.loc[majorityMinoritySNPs,'ref_nuc']\n",
    "    tmpSNPs.loc[majorityMinoritySNPs,'minor_ref_nuc'] = alt_nucs\n",
    "\n",
    "    tmpSNPs.loc[majorityMinoritySNPs, 'minorAlleleFreq_index'] = np.abs(1-tmpSNPs.loc[majorityMinoritySNPs, 'SNP_frequency_index'].values)\n",
    "    tmpSNPs.loc[majorityMinoritySNPs, 'minorAlleleFreq_contact'] = np.abs(1-tmpSNPs.loc[majorityMinoritySNPs, 'SNP_frequency_contact'].values)\n",
    "\n",
    "    tmpSNPs['SNP_frequency_directional_change'] = tmpSNPs.SNP_frequency_contact - tmpSNPs.SNP_frequency_index\n",
    "    tmpSNPs['abs_SNP_frequency_difference'] = np.abs(tmpSNPs.SNP_frequency_directional_change)\n",
    "    return tmpSNPs\n",
    "\n",
    "def calc_changes_in_SNP_frequency(df):\n",
    "    df['abs_SNP_frequency_difference'] = np.abs(df.SNP_frequency_contact-df.SNP_frequency_index)\n",
    "    df['SNP_frequency_directional_change'] = df.SNP_frequency_contact-df.SNP_frequency_index\n",
    "    df['log_abs_SNP_frequency_difference'] = np.log10(df.abs_SNP_frequency_difference).fillna(0).replace((np.inf), 0).replace((-np.inf),0)\n",
    "    return df\n",
    "\n",
    "def apply_depth_filter(transmissionSNPs, min_depth=100):\n",
    "    return transmissionSNPs.loc[~((transmissionSNPs.depth_contact < min_depth)|(transmissionSNPs.depth_index < min_depth))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum allele frequency is used as a pseudocount here when synon/nonsynon divergence in a gene pairing is 0\n",
    "def calc_pairing_divergences(transmittedSNPs, pairings, subtype='H3N2', freq_cutoff=0.01):\n",
    "    # First, calculate divergences only using SNPs that are above the freq cutoff\n",
    "    transmittedSNPs = transmittedSNPs.loc[transmittedSNPs.abs_SNP_frequency_difference >= freq_cutoff]\n",
    "    \n",
    "    # Next, in order to account for gene pairings with no differences between them\n",
    "    # (and thus aren't reprsented in the transmittedSNPs dataframe), \n",
    "    # I will make a separate dataframe of id columns that contain all possible pairing/gene/AAtype combinations.\n",
    "    all_possible_products = pd.DataFrame(transmittedSNPs.loc[transmittedSNPs.subtype == subtype,'product'].dropna().unique())\n",
    "    AAtypes = pd.DataFrame(['Nonsynonymous', 'Synonymous'])\n",
    "\n",
    "    # in case a pairing has no differences at all, I will use a separate list of pairings\n",
    "    transmittedSNPs['pairing_id'] = transmittedSNPs['index'] + '|' + transmittedSNPs.contact\n",
    "    pairing_ids = pairings['index']+ '|' + pairings.contact \n",
    "\n",
    "    # actually create the dataframe of possible combinations\n",
    "    pairing_divergence = all_possible_products.merge(AAtypes, how='cross').merge(pd.DataFrame(pairing_ids).reset_index(drop=True), how='cross')\n",
    "    pairing_divergence = pairing_divergence.rename(columns={'0_x':'product','0_y':'AAtype',0:'pairing_id'})\n",
    "    \n",
    "    # calc the sum of absolute SNP frequency changes (aka divergence) \n",
    "    # and merge with all possible combinations of pairings/genes/AAtypes. \n",
    "    # Combinations of id variables that have no changes will be nan, and so I will set those at 0.\n",
    "    between_pairing_sum_of_frequencies = transmittedSNPs.groupby(['pairing_id', 'product', 'AAtype']).sum()['abs_SNP_frequency_difference'].reset_index().rename(columns={'abs_SNP_frequency_difference':'sum_divergence'})\n",
    "    pairing_divergence = pairing_divergence.merge(between_pairing_sum_of_frequencies, on=['pairing_id','product', 'AAtype'], how='left')\n",
    "    pairing_divergence.sum_divergence = pairing_divergence.sum_divergence.fillna(0)\n",
    "    \n",
    "    # To finish off calculating the total divergence, because I will be taking the log of these data, which are counts, \n",
    "    # I have to deal with missing data (0s that are due to under-counting. Presumably 0s just represent regions that\n",
    "    # rarely aquire mutations. It is difficult to observe rate of mutation below a hard cutoff of 1/len of gene.\n",
    "    # One common way to deal with this is to add a pseudocount of the smallest possible observation to all observations.\n",
    "    # For this experiment, the smallest possible observation is one mutation between pairs of frequency \"freq_cutoff\".\n",
    "    # So I add that pseudocount here.\n",
    "    pseudocount = freq_cutoff\n",
    "    pairing_divergence.sum_divergence += pseudocount \n",
    "    \n",
    "    \n",
    "    # I will normalize divergences to the number of synon/nonsynon sites in the *index* case. So first, identify index:\n",
    "    pairing_divergence['sampleID'] = pairing_divergence.pairing_id.str.split('|').str[0]\n",
    "\n",
    "    # And merge with the genes dataframe to add the number of synon/nonsynon sites per gene in the index cases.\n",
    "    # There should not be any missing data here; I should previously have calculated this for all samples collected.\n",
    "    pairing_divergence = pairing_divergence.merge(genes[['sampleID','product','N_sites_gene','S_sites_gene']], on=['sampleID','product'], how='left').rename(columns={'sampleID':'index'})\n",
    "\n",
    "    # Now reorganize the synon/nonsynon sites data so that each row is either synon or nonsynon only\n",
    "    pairing_divergence['sites'] = pairing_divergence.N_sites_gene\n",
    "    pairing_divergence.loc[pairing_divergence.AAtype == 'Synonymous', 'sites'] = pairing_divergence.loc[pairing_divergence.AAtype == 'Synonymous', 'S_sites_gene'].values\n",
    "    \n",
    "    # Finally, I can now normalize total divergence by the number of possible sites\n",
    "    pairing_divergence['normalized_divergence'] = pairing_divergence.sum_divergence/pairing_divergence.sites\n",
    "    \n",
    "    # And I can calculate the log of the normalized divergece.\n",
    "    pairing_divergence['log_divergence'] = np.log10(pairing_divergence.normalized_divergence)\n",
    "    \n",
    "    return pairing_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def get_all_pairing_divergences(pairings):\n",
    "    # It is simpler to calculate the per-gene, per-AAtype normalized divergence for all possible pairings \n",
    "    # (there are ~2600 of them) than to do this for 10000 random pairs. This function does that.\n",
    "    print('making pairings')\n",
    "    transmissionPairs = pd.DataFrame(pairings, columns=['index','contact']).dropna()\n",
    "    transmissionPairs['kind'] = 'transmission'\n",
    "    \n",
    "    divergence_relevent_columns =  ['sampleID',\n",
    "                                    'SNP_frequency',\n",
    "                                    'alt_nuc',\n",
    "                                    'ref_nuc',\n",
    "                                    'depth',\n",
    "                                    'RD',\n",
    "                                    'AD',\n",
    "                                    'subtype',\n",
    "                                    'AAtype',\n",
    "                                    'segment',\n",
    "                                    'pos',\n",
    "                                    'product']\n",
    "    \n",
    "    print('obtaining transmitted SNPs for all pairings')\n",
    "    index_SNPs = transmissionPairs.merge(SNPs[divergence_relevent_columns].rename(columns={'sampleID':'index'}),on='index', how='left')\n",
    "    contact_SNPs = transmissionPairs.merge(SNPs[divergence_relevent_columns].rename(columns={'sampleID':'contact'}),on='contact', how='left')\n",
    "\n",
    "    print ('making SNP keys')\n",
    "    index_SNPs['SNPkey'] = index_SNPs['index'] + ':' + index_SNPs['contact'] + ':'+index_SNPs.segment+':'+index_SNPs.pos.astype(str)+':'+index_SNPs.alt_nuc+':'+index_SNPs['product'].fillna('OORF')\n",
    "    contact_SNPs['SNPkey'] = contact_SNPs['index'] + ':' + contact_SNPs['contact'] + ':'+contact_SNPs.segment+':'+contact_SNPs.pos.astype(str)+':'+contact_SNPs.alt_nuc+':'+contact_SNPs['product'].fillna('OORF')\n",
    "\n",
    "    print ('merging index and contact SNPs')\n",
    "    transmissionSNPs = index_SNPs.merge(contact_SNPs, on='SNPkey', how='outer', suffixes=('_index','_contact'))\n",
    "    transmissionSNPs = updateDuplicateColumnsPostMerge(transmissionSNPs, suffixes=('_index','_contact'))\n",
    "    transmissionSNPs = transmissionSNPs.drop_duplicates()\n",
    "\n",
    "    # Its important to have depths for both the index and contact for all single nucleotide differences between the two. \n",
    "    # I don't have this information for SNPs which arose de novo in the contact or reverted to refernece in the contact,\n",
    "    # Because I don't have depth data for sites that are 100% reference.\n",
    "    # So I will applying a function to all transmission SNPs that:\n",
    "       # a) determines whether the index or contact frequency/AD/depth info contains nans\n",
    "       # b) calls getReadDepth on the appropriate information to fill in the nans\n",
    "       # c) returns the original data with getReadDepth's results filling in the nans\n",
    "    columnsToUpdate = ['SNP_frequency_index','AD_index','depth_index','SNP_frequency_contact','AD_contact','depth_contact']\n",
    "\n",
    "    print ('getting depths')\n",
    "    tqdm.pandas()\n",
    "    if os.path.exists('tSNPs_w_depth.csv'):\n",
    "        tmp=pd.read_csv('tSNPs_w_depth.csv').drop('Unnamed: 0', axis=1)\n",
    "    else:\n",
    "        tmp = pd.DataFrame(transmissionSNPs.progress_apply(getReadDepthWrapper,axis=1).to_list())\n",
    "        tmp.to_csv('tSNPs_w_depth.csv')\n",
    "    #It makes me nervous that I'm applying a function to all my values which in theory could change all my SNP values.\n",
    "    #So I'm going to do this carefully. I will apply the function and create a separate data frame, preserving my original data.\n",
    "    #I then assert that the data that I am about to change is either a) identical to the new data, or b) nan\n",
    "    print('checking results')\n",
    "    a = transmissionSNPs[columnsToUpdate].to_numpy()\n",
    "    b = tmp.to_numpy()\n",
    "\n",
    "#     assert ((a==b) | np.isnan(a)).all()\n",
    "\n",
    "    #Once that's confirmed, I replace my original data with my updated data\n",
    "    transmissionSNPs[columnsToUpdate] = b\n",
    "\n",
    "    # To save time, I do not get reference SNP depth, just total depth and alt depth. \n",
    "    # Both are calculated w/ quality minimums, so ref_depth is just total depth - alt depth\n",
    "    # I'm only changing values that are nan, otherwise I will use the info previously gathered\n",
    "    transmissionSNPs.loc[transmissionSNPs.RD_index.isna(), 'RD_index'] = transmissionSNPs.loc[transmissionSNPs.RD_index.isna(), 'depth_index']-transmissionSNPs.loc[transmissionSNPs.RD_index.isna(), 'AD_index']\n",
    "    transmissionSNPs.loc[transmissionSNPs.RD_contact.isna(), 'RD_contact'] = transmissionSNPs.loc[transmissionSNPs.RD_contact.isna(), 'depth_contact']-transmissionSNPs.loc[transmissionSNPs.RD_contact.isna(), 'AD_contact']\n",
    "\n",
    "    assert(len(transmissionSNPs.loc[transmissionSNPs.RD_index.isna()])==0)\n",
    "    assert(len(transmissionSNPs.loc[transmissionSNPs.RD_contact.isna()])==0)\n",
    "    assert(len(transmissionSNPs.loc[transmissionSNPs.AD_index.isna()])==0)\n",
    "    assert(len(transmissionSNPs.loc[transmissionSNPs.AD_contact.isna()])==0)\n",
    "\n",
    "    # And now that my nans are filled in, I calculate the differences in snp frequency\n",
    "    print('calculating divergences')\n",
    "    \n",
    "    # First filter out any transmission SNPs where the index or contact was not sequenced at \n",
    "    # sufficient depth to be confident of the within-host frequency of that site\n",
    "    transmissionSNPs = apply_depth_filter(transmissionSNPs, min_depth=100)\n",
    "    transmissionSNPs = calc_changes_in_SNP_frequency(transmissionSNPs)\n",
    "\n",
    "    if len(transmissionSNPs['product'] == 'HA') == 0:\n",
    "        print ('acutally using this clause')\n",
    "        transmissionSNPs = add_antigenic_product(transmissionSNPs)\n",
    "        \n",
    "    divergences = calc_pairing_divergences(transmissionSNPs, pairings=pairings)\n",
    "    divergences = divergences[['pairing_id','product','AAtype','normalized_divergence']]\n",
    "    return divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making pairings\n",
      "obtaining transmitted SNPs for all pairings\n",
      "making SNP keys\n",
      "merging index and contact SNPs\n",
      "getting depths\n",
      "checking results\n",
      "calculating divergences\n"
     ]
    }
   ],
   "source": [
    "# First, get the normalized divergences of all combinations of samples that could plausibly be the result of \n",
    "# a transmission (ie., sx onset in contact occured within 10 days after sx onset of index)\n",
    "\n",
    "a = allvsall.loc[allvsall.subtype_index==subtype]\n",
    "\n",
    "# All vs all is a df of all potential pairings w/ distances pre-calculated.\n",
    "# It should already be limited to plausible transmissions.\n",
    "assert len(a.loc[np.abs(pd.to_datetime(a.time_of_symptom_onset_index)-pd.to_datetime(a.time_of_symptom_onset_index))<=(pd.to_timedelta('10D'))]) == len(a)\n",
    "\n",
    "all_plausible_pairing_divergences = get_all_pairing_divergences(a[['index','contact']])\n",
    "\n",
    "# Now that I have a df with the divergences of each AA type of each product in every plausible sample combination,\n",
    "# I need to calculate the stat I'm actually interested in: the log of the ratio of Nonsynon to Synon divergences \n",
    "# for each gene product in each plausible sample combo.\n",
    "# First, take the log of the normalized divergence\n",
    "all_plausible_pairing_divergences['log_normalized_divergence'] = np.log(all_plausible_pairing_divergences.normalized_divergence)\n",
    "\n",
    "# Then, do this sort of odd code that is very fast. Sort by pairing_id, product, and AA type.\n",
    "all_plausible_pairing_divergences = all_plausible_pairing_divergences.sort_values(['pairing_id','product', 'AAtype']).reset_index(drop=True)\n",
    "\n",
    "# Because I sort by AA type last, every even row is Nonsynonymous, and every odd row is Synonymous.\n",
    "# So make a data frame that is just id values (ie, pairings and products):\n",
    "pairing_divergences = all_plausible_pairing_divergences.groupby(['pairing_id','product']).first().reset_index()[['pairing_id','product']]\n",
    "# and our log divergence ratio will be log evens = log odds. Having previously sorted by pairing_id and product, \n",
    "# the resulting numbers should be in the right order.\n",
    "pairing_divergences['log_divergence_ratio'] = all_plausible_pairing_divergences.loc[all_plausible_pairing_divergences.index%2==0, 'log_normalized_divergence'].values - all_plausible_pairing_divergences.loc[all_plausible_pairing_divergences.index%2==1, 'log_normalized_divergence'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "###Create random pairs with distances that are in *same distribution* as household pairs\n",
    "\n",
    "# add information about pairing genetic distances back to the all_plausible_pairing_divergences df\n",
    "distances = allvsall[['index','contact','distance']]\n",
    "distances['pairing_id'] = allvsall['index']+'|'+allvsall.contact\n",
    "\n",
    "all_plausible_pairing_divergences = all_plausible_pairing_divergences.merge(distances[['pairing_id','index','contact','distance']], on='pairing_id',how='left')\n",
    "\n",
    "# bin those pairings by distance\n",
    "subtype_ava = all_plausible_pairing_divergences.groupby(['pairing_id','distance']).first().reset_index()[['pairing_id','distance']]\n",
    "subtype_ava['quantiles'] = pd.cut(subtype_ava.distance,bins=np.linspace(0, 500, 50001), labels=np.linspace(0.01, 500, 50000))\n",
    "\n",
    "subtype_transPairs = transmissionPairs.loc[(transmissionPairs.subtype==subtype) & (transmissionPairs.kind=='transmission')].reset_index(drop=True)\n",
    "\n",
    "bootstrap_size = 10000\n",
    "bootstrap_stat = 'distance'\n",
    "\n",
    "# then fit a log-norm distribution to our actual transmissionPairs to match distances to\n",
    "mu, sigma = np.log(subtype_transPairs.distance).mean(), np.log(subtype_transPairs.distance).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_n_random_pairings_from_lognormal_distribution(potential_pairings, n, mu, sigma):\n",
    "    random_pairings = list()\n",
    "    print('taking distance distribution')\n",
    "    samples = np.round(np.random.lognormal(mean=mu, sigma=sigma, size=n), 2)\n",
    "    print('finding samples w/ distances nearest to drawn distances')\n",
    "    potentials = potential_pairings.quantiles.cat.categories[potential_pairings.quantiles.cat.codes].values\n",
    "    \n",
    "    adjusted_samples = potentials[np.abs(samples[:,np.newaxis]-potentials[np.newaxis,:]).argmin(axis=1)]\n",
    "    print('darwing bootstrapped sample pairs')\n",
    "    random_pairings = [potential_pairings.loc[potential_pairings.quantiles==x].sample(1)['pairing_id'].values[0] for x in adjusted_samples]\n",
    "\n",
    "    return random_pairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking distance distribution\n",
      "finding nearest samples\n",
      "getting pairings\n"
     ]
    }
   ],
   "source": [
    "# its faster to draw boostrapSize*numOfBootstraps random pairs from the lognormal distribution \n",
    "# and later reshape into a [boostrapSize, numOfBootstraps] shaped dataframe/array\n",
    "drawings = draw_n_random_pairings_from_lognormal_distribution(potential_pairings = subtype_ava,\n",
    "                                                              n = NUMBER_OF_H3N2_PAIRS*bootstrap_size,\n",
    "                                                              mu=mu,\n",
    "                                                              sigma=sigma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_drawings_df = pd.DataFrame(drawings, columns=['pairing_id'])\n",
    "\n",
    "random_drawings_df['bootstrap_id'] = random_drawings_df.index % 10000\n",
    "\n",
    "# Now that I have 10000 bootstraps of n pairs, \n",
    "# I can merge with pairing_divergences to get the log divergence ratios of each product for all my randomly drawn pairs\n",
    "random_drawings_df = random_drawings_df.merge(pairing_divergences, on='pairing_id', how='left')\n",
    "\n",
    "# Calc the average log divergence ratio for each product per bootstrap\n",
    "random_drawings_df = random_drawings_df.groupby(['bootstrap_id', 'product']).mean().reset_index()\n",
    "\n",
    "# And save the product. The rest of the work will be done in the notebook that actually makes the figure.\n",
    "random_drawings_df.to_csv('/mnt/d/orchards/10000_random_H3N2_log_ratios.tsv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
